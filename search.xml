<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HTTP权威指南笔记之HTTP报文]]></title>
    <url>%2F2017%2F12%2F28%2FHTTP%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E7%AC%94%E8%AE%B0%E4%B9%8BHTTP%E6%8A%A5%E6%96%87%2F</url>
    <content type="text"><![CDATA[一些概念：资源，资源定位符URI/URL，媒体类型MIME,HTTP事务(一次请求响应的过程)方法(GET/PUT/POST/HEAD/TRACE/OPTIONS/DELETE)，状态码(200/302/404)，报文(请求、响应消息:起始行，首部字段，主体)，连接(TCP/IP)Telnet、netcat工具协议版本:0.9,1.0,1.1,2.0Web的结构组件: 代理:请求转发，可能会进行一些过滤操作。场景的有Apache Http Server,Nginx 缓存:对响应的资源进行缓存，下次请求即可直接从缓存返回，一般用于静态资源如图片，css，js，常见的有CDN,Nginx缓存。 网关:通常用于将HTTP流量转换成其他的协议。网关接受请求时就好像自己是资源的源端服务器一样。 隧道(tunnel)在一条或多条连接上对原始数据进行盲转发，转发时不会窥探数据。如TLS/SSL隧道。 Agent代理：代表用户发起HTTP请求的客户端程序，如浏览器，爬虫 规范信息：http://www.w3.org/Protocolshttp://www.ietf.org/rfc/rfc2616.txt http1.1规范http://www.ietf.org/rfc/rfc2046.txt MIME规范 URL与资源url语法相对与绝对url：支持的schema:http,https,mailto,ftp,file,news,telnet,rtsp/rtspu(音/视频,Real-Time Streaming Protocol) HTTP报文术语：流入(inbound)、流出(outbound)、上游、下游、HTTP事务报文组成：起始行(方法，url，版本，状态码,原因短语)、首部(冒号分隔的多行header)、主体(可文本也可二进制数据)报文分类：请求报文，响应报文报文构成： 状态码：详解请看：http://tool.oschina.net/commons?type=51、100-199：信息性状态码100 Continue:主要针对的场景是：HTTP客户端希望在发送实体之前确认下服务器是否会接受(避免因为服务器无法处理或请求实体过大时拒绝而浪费带宽)。客户端发送时，加个头部Expect: 100 Continue,服务器返回100 Continue响应。101 Switching Protocols: 将协议切换成Update头部所列的协议 2、200-299：成功状态码200 OK201 Created 用于PUT请求。202 Accepted 告诉客户端请求正在被执行，但还没有处理完。203 (Non-Authoritative Information/非官方信息):服务器已成功处理了请求，但返回的实体头部元信息不是在原始服务器上有效的确定集合，而是来自本地或者第三方的拷贝。当前的信息可能是原始版本的子集或者超集。 204 No Content: 服务器成功处理了请求，但不需要返回任何实体内容，并且希望返回更新了的元信息。如果客户端是浏览器的话，那么用户浏览器应保留发送了该请求的页面，而不产生任何文档视图上的变化，即使按照规范新的或更新后的元信息应当被应用到用户浏览器活动视图中的文档。由于204响应被禁止包含任何消息体，因此它始终以消息头后的第一个空行结尾。 205 Reset Content:虽然没有新文档但浏览器要重置文档显示。这个状态码用于强迫浏览器清除表单域。 206 Partial Content:在服务器完成了一个包含Range头信息的局部请求时被发送的.场景如断点续传下载. 请求包含Range来指示客户端希望得到的内容范围，并且可能包含 If-Range 来作为请求条件。 响应必须包含如下的头部域：Content-Range 用以指示本次响应中返回的内容的范围. 3、300-399:重定向状态码，响应带Location头300 Multiple Choices: 表示被请求的文档可以在多个地方找到，并将在返回的文档中列出来301 Moved Permanently:被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个 URI 之一.302 Found：请求的资源现在临时从不同的 URI 响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。303 See Other：对应当前请求的响应可以在另一个 URI 上被找到，而且客户端应当采用 GET 的方式访问那个资源。这个方法的存在主要是为了允许由脚本激活的POST请求输出重定向GET到一个新的资源。304 Not Modified:当客户端有一个缓存的文档，通过提供一个 If-Modified-Since 头信息可指出客户端只希望文档在指定日期之后有所修改时才会重载此文档，用这种方式可以进行有条件的请求。305 Use Proxy307 Temporary Redirect:浏览器处理307状态的规则与302相同。307状态被加入到 HTTP 1.1中是由于许多浏览器在收到302响应时即使是原始消息为POST的情况下仍然执行了错误的转向。只有在收到303响应时才假定浏览器会在POST请求时重定向。添加这个新的状态码的目的很明确：在响应为303时按照POST请求转向GET；而在307响应时则按照GET请求转向。 302、303、307的区别：RFC 1945(http1.0规范)指明：如果客户端发出POST请求后，收到服务端的302状态码，那么不能自动的向新的URI发送重复请求，必须跟用户确认是否该重发，因为第二次POST时，环境可能已经发生变化（POST方法不是幂等的），POST操作会不符合用户预期。但是，很多客户端/浏览器在这种情况下都会把POST请求变为GET请求。RFC2616(http1.1规范)：对302的描述与http1.0一样，不过添加了303，307吗。很多浏览器都把302当作303处理了，它们获取到HTTP响应报文头部的Location字段信息，并发起一个GET请求。这种不符合文档规范的问题依然存在,所以HTTP1.1为细化，并明确含义，引入了303，307状态码:303状态码的响应，也就是上边提到的现在浏览器对302状态码的处理：POST重定向为GET.307状态码则相当于HTTP1.0文档中的302状态码:POST重定向时，必须跟用户进行确认，是否继续使用POST重发请求到Location指出的地址。即307是不会把POST转为GET的文档也说到，为兼容很多HTTP1.1之前的浏览器，服务端在需要发出303状态码时，会选择用302状态码替代；而对于307的处理，则需要在响应实体中包含信息，以便不能处理307状态码的用户有能力在新URI中发起重复请求，也就是说，把重定向的页面展示给用户，让用户去点重定向URI链接。 文档规定：浏览器对303状态码的处理跟原来浏览器对HTTP1.0的302状态码的处理方法一样；浏览器对307状态码处理则跟原来HTTP1.0文档里对302的描述一样。303和307的存在，归根结底是由于POST方法的非幂等属性引起的。在HTTP1.1中，302理论上是要被放弃掉的，它被细化为303和307，但为了兼容，它目前还在业界中大量使用。其实由于普遍是对GET请求进行重定向，而对POST请求重定向的场景很少见(做过爬虫的会遇到哦，这里要注意了。)，所以一般而言307状态码很少见，而浏览器普遍又采用不规范的302行为，故而303码也很少见。此段参考：https://www.cnblogs.com/cswuyg/p/3871976.html 4、400-499:客户端错误码400 Bad Request：在写前端ajax调试时，你可能碰到过。401 Unauthorized 当前请求需要用户验证。该响应必须包含一个适用于被请求资源的 WWW-Authenticate 信息头用以询问用户信息。客户端可以重复提交一个包含恰当的 Authorization 头信息的请求。403 Forbidden 服务器已经理解请求，但是拒绝执行它。404 Not Found405 Method Not Allowed 指出请求方法(GET, POST, HEAD, PUT, DELETE等)对某些特定的资源不允许使用406 Not Acceptable 表示请求资源的MIME类型与客户端中Accept头信息中指定的类型不一致407 Proxy Authentication Required 与401状态有些相似，只是这个状态用于代理服务器408 Request Timeout 指服务端等待客户端发送请求的时间过长413 Request Entity Too Large415 Unsupported Media Type 5、500-599：服务器错误码500 Internal Server Error 内部服务器错误，该状态经常由CGI程序引起也可能由无法正常运行的或返回头信息格式不正确的servlet引起。501 Not Implemented502 Bad Gateway 作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应。503 Service Unavailable 由于临时的服务器维护或者过载，服务器当前无法处理请求。这个状况是临时的，并且将在一段时间以后恢复。如果能够预计延迟时间，那么响应中可以包含一个 Retry-After 头用以标明这个延迟时间。如果没有给出这个 Retry-After 信息，那么客户端应当以处理500响应的方式处理它。 注意：503状态码的存在并不意味着服务器在过载的时候必须使用它。某些服务器只不过是希望拒绝客户端的连接。 504 Gateway Timeout 作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器（URI标识出的服务器，例如HTTP、FTP、LDAP）或者辅助服务器（例如DNS）收到响应。 头部详情：http://www.cnblogs.com/Joans/p/3956490.html通用头部：Connection 是否持久连接(Http1.1默认持久连接)，如Connection: close;Connection: Keep-Alive.DateTrailerTransfer-Encoding 如Transfer-Encoding:chunkedUpdateVia 通过的中间网关或代理服务器地址，通信协议Cache-Control 如 Cache-Control: no-cachePragma 如Pragma: no-cache 注意:对于缓存Cache-Control优先于Pragma 请求头部Host 指定请求的服务器的域名和端口号From 发出请求的用户的Email From: user@email.comRefererContent-Type 如Content-Type: application/x-www-form-urlencodedUser-AgentAccept,Accept-Charset,Accept-Encoding,Accept-LanguageAccept-Ranges:可以请求网页实体的一个或者多个子范围字段 Accept-Ranges: bytesExpect 请求的特定的服务器行为 Expect: 100-continueIf-Match,If-None-Match(是否匹配通过ETag来判断,头的值为ETag值，如If-None-Match: “737060cd8c284d8af7ad3082f209582d”)，If-Modified-Since,If-Unmodified-SinceIf-Range 如果实体未改变，服务器发送客户端丢失的部分，否则发送整个实体。参数也为Etag，如If-Range: “737060cd8c284d8af7ad3082f209582d”Range 只请求实体的一部分，指定范围Range: bytes=500-999AuthorizationCookie,Cookie2 如Cookie: $Version=1; Skin=new;Max-ForwardProxy-AuthorizationProxy-Connection 响应头部AgeRetry-After 如果实体暂时不可取，通知客户端在指定时间之后再次尝试ServerAccept-Ranges 如Accept-Ranges: bytesVary 告诉下游代理是使用缓存响应还是从原始服务器请求Proxy-AuthenticateSet-Cookie 如Set-Cookie: UserID=JohnDoe; Max-Age=3600;Set-Cookie2WWW-Authenticate 表明客户端请求实体应该使用的授权方案 WWW-Authenticate: BasicAllowLocationContent-Encoding,Content-Length,Content-LocationContent-Range 如 Content-Range: bytes 21010-4702Content-TypeETagExpiresLast-Modifiedrefresh 应用于重定向或一个新的资源被创造，在5秒之后重定向，如Refresh: 5; url=http://www.zcmhi.com/archives/94.html 问题一：如何理解Trailer参考：https://stackoverflow.com/questions/5590791/http-chunked-encoding-need-an-example-of-trailer-mentioned-in-spec/11313254Chunked body引入的原因是发送数据的时候并不确定会发送多少，因此利用chunked编码在HTTP层进行分段。Trailer引入是在chunk发送完毕后，尾部添加用户自定义的Header字段。如果使用Trailer头部，需要添加”Trailer:header_name” header,然后在chunked body 的尾部添加trailer指定的header。 trailer header可以添加0或多个。(除了Content-Encoding, Content-Type, Content-Range, and Trailer). 问题二：如何理解Vary建议参考：http://luchuan.iteye.com/blog/1058563http://mark.koli.ch/understanding-the-http-vary-header-and-caching-proxies-squid-etcVary = “Vary” “:” ( ““ | 1#field-name ) 要么是“\”，要么是header的key名称组合vary的意义在于告诉代理服务器/缓存/CDN，如何判断请求是否一样，vary中的组合就是服务器/缓存/CDN判断的依据，比如Vary中有User-Agent，那么即使相同的请求，如果用户使用IE打开了一个页面，再用Firefox打开这个页面的时候，CDN/代理会认为是不同的页面，如果Vary中没有User-Agent，那么CDN/代理会认为是相同的页面，直接给用户返回缓存的页面，而不会再去web服务器请求相应的页面。1234Vary: Accept-Encoding Vary: Accept-Encoding,User-Agent Vary: X-Some-Custom-Header,Host Vary: * 问题三：断点续传涉及的头Range:(unit=first byte pos)-[last byte pos]Content-Range: bytes (unit first byte pos) - [last byte pos]/[entity legth] 请求下载整个文件:GET /test.rar HTTP/1.1Host: 116.1.219.219Range: bytes=0-80 //一般请求下载整个文件是bytes=0- 或不用这个头 一般正常回应HTTP/1.1 206 Partial ContentAccept-Ranges: bytesContent-Length: 81Content-Type: application/octet-streamContent-Range: bytes 0-80/801 //801:文件总大小Connection: Keep-Alive 例如：1$curl -v -i --range 0-9 http://www.baidu.com/img/bdlogo.gif If-Range = “If-Range” “:” ( entity-tag | HTTP-date )IF-Range头部需配合Range，如果没有Range参数，则If-Range会被视为无效。如果If-Range匹配上，那么客户端已经存在的部分是有效的，服务器将返回缺失部分，也就是Range里指定的，然后返回206（Partial content)，否则证明客户端的部分已无效（可能已经更改），那么服务器将整个实体内容全部返回给客户端，同时返回200OK]]></content>
      <categories>
        <category>读书笔记</category>
        <category>HTTP权威指南</category>
      </categories>
      <tags>
        <tag>tcp</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis缓存及命中率与过期移除策略]]></title>
    <url>%2F2017%2F12%2F28%2FRedis%E7%BC%93%E5%AD%98%E5%91%BD%E4%B8%AD%E7%8E%87%E5%8F%8A%E8%BF%87%E6%9C%9F%E7%A7%BB%E9%99%A4%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[缓存特征命中率命中率=返回正确结果数/请求缓存次数，命中率问题是缓存中的一个非常重要的问题，它是衡量缓存有效性的重要指标。 最大元素（或最大空间）缓存中可以存放的最大元素的数量，一旦缓存中元素数量超过这个值（或者缓存数据所占空间超过其最大支持空间），那么将会触发缓存启动清空策略。根据不同的场景合理的设置最大元素值往往可以一定程度上提高缓存的命中率，从而更有效的时候缓存。 清空策略当缓存空间被用满时，如何保证在稳定服务的同时有效提升命中率？这就由缓存清空策略来处理，设计适合自身数据特征的清空策略能有效提升命中率。常见的一般策略有： FIFO(first in first out)先进先出策略，最先进入缓存的数据在缓存空间不够的情况下（超出最大元素限制）会被优先被清除掉，以腾出新的空间接受新的数据。策略算法主要比较缓存元素的创建时间。在数据实效性要求场景下可选择该类策略，优先保障最新数据可用。 LFU(less frequently used)最少使用策略，无论是否过期，根据元素的被使用次数判断，清除使用次数较少的元素释放空间。策略算法主要比较元素的hitCount（命中次数）。在保证高频数据有效性场景下，可选择这类策略。 LRU(least recently used)最近最少使用策略，无论是否过期，根据元素最后一次被使用的时间戳，清除最远使用时间戳的元素释放空间。策略算法主要比较元素最近一次被get使用时间。在热点数据场景下较适用，优先保证热点数据的有效性。 除此之外，还有一些简单策略比如： 根据过期时间判断，清理过期时间最长的元素； 根据过期时间判断，清理最近要过期的元素； 随机清理； 根据关键字（或元素内容）长短清理等。 缓存介质可以分成内存、硬盘文件、数据库。 缓存分类：分为local cache（本地缓存，如普通的map，ehcache，guava cache）和remote cache（分布式缓存,如memcache,redis） 缓存存活期 TTL(Time To Live):存活期即从缓存中创建时间点开始直到它到期的一个时间段（不管在这个时间段内有没有访问都将过期）缓存空闲期 TTI(Time To Idle):空闲期即一个数据多久没被访问将从缓存中移除的时间。 将 Redis 作为缓存的时候应该注意： 设置最大可用的内存maxmemory 设置 Key 的清除策略（也就是缓存策略） maxmemory-policy Redis缓存命中率缓存命中率是关于缓存性能一个很关键的指标，你产品用了缓存，但缓存能达到多大的效果呢，这得看命中率。Redis缓存命中率怎么看呢？其实很简单。两种方式:1、telnet localhost 6379 ;2、redis-cli -h localhost -p 6379之后便进入交互界面，输入info命令即可看到如下情况：12345678127.0.0.1:6379&gt;info...keyspace_hits:14414110 keyspace_misses:3228654 used_memory:433264648 expired_keys:1333536 evicted_keys:1547380... keyspace_hits 代表命中次数keyspace_misses 代表没命中次数命中：可以直接通过缓存获取到需要的数据。不命中：无法直接通过缓存获取到想要的数据，需要再次查询数据库或者执行其它的操作。原因可能是由于缓存中根本不存在，或者缓存已经过期。 通过计算hits和miss，我们可以得到缓存的命中率：14414110 / (14414110 + 3228654) = 81% .一个缓存失效机制，和过期时间设计良好的系统，命中率可以做到95%以上 同时有个redis-stat开源项目，它利用INFO命令展现出更直观的信息报表.提供终端界面和浏览器界面两种方式。1234gem install redis-statredis-statredis-stat --serverredis-stat --verbose --server=8080 5 具体情况其github。 info命令输出的信息有很多，具体代表些什么。可以参考：http://blog.csdn.net/mysqldba23/article/details/68066322 Redis缓存失效移除策略Redis key的三种过期策略 惰性删除：当读/写一个已经过期的key时，会触发惰性删除策略，直接删除掉这个过期key，这是被动的！ 定期删除：由于惰性删除策略无法保证冷数据被及时删掉，所以 redis 会定期主动淘汰一批已过期的key。 主动删除：当前已用内存超过maxmemory限定时，触发主动清理策略，该策略由启动参数的配置决定，可配置数据淘汰策略： Redis支持6种的数据淘汰策略： volatile-lru：从已设置过期时间的数据集中挑选最近最少使用的数据淘汰； volatile-ttl：从已设置过期时间的数据集中挑选将要过期的数据淘汰； volatile-random：从已设置过期时间的数据集中任意选择数据淘汰 ； allkeys-lru：从数据集中挑选最近最少使用的数据淘汰； allkeys-random：从数据集中任意选择数据淘汰； no-enviction（驱逐）：禁止驱逐数据。 影响缓存命中率的几个因素1、业务场景和业务需求：缓存适合“读多写少”的业务场景，反之，使用缓存的意义其实并不大，命中率会很低。互联网应用的大多数业务场景下都是很适合使用缓存的。 2、缓存的设计（粒度和策略）:通常情况下，缓存的粒度越小，命中率会越高。例如：当缓存单个对象的时候（例如：单个用户信息），只有当该对象对应的数据发生变化时，我们才需要更新缓存或者让移除缓存。而当缓存一个集合的时候（例如：所有用户数据），其中任何一个对象对应的数据发生变化时，都需要更新或移除缓存。此外，缓存的更新/过期策略也直接影响到缓存的命中率。当数据发生变化时，直接更新缓存的值会比移除缓存（或者让缓存过期）的命中率更高，当然，系统复杂度也会更高。 3、缓存容量和基础设施缓存的容量有限，则容易引起缓存失效和被淘汰（目前多数的缓存框架或中间件都采用了LRU算法）。同时，缓存的技术选型也是至关重要的，比如采用应用内置的本地缓存就比较容易出现单机瓶颈，而采用分布式缓存则毕竟容易扩展。所以需要做好系统容量规划，并考虑是否可扩展。通常，两种也可以结合使用，不过实现复杂度也会相应提升，具体看并发量。 4、其他因素当缓存节点发生故障时，需要避免缓存失效并最大程度降低影响，这种特殊情况也是架构师需要考虑的。业内比较典型的做法就是通过一致性Hash算法，或者通过节点冗余的方式。 提高缓存命中率的方法通常来讲，在相同缓存时间和key的情况下，并发越高，缓存的收益会越高，即便缓存时间很短。需要在业务需求，缓存粒度，缓存策略，技术选型等各个方面去通盘考虑并做权衡。尽可能的聚焦在高频访问且时效性要求不高的热点业务上，通过缓存预加载（预热）、增加存储容量、调整缓存粒度、更新缓存等手段来提高命中率。 对于时效性很高（或缓存空间有限），内容跨度很大（或访问很随机），并且访问量不高的应用来说缓存命中率可能长期很低，可能预热后的缓存还没来得被访问就已经过期了。 附-Spring CacheSpring 3.1之后，引入了注解缓存技术。它只是一个缓存抽象。Spring的缓存可以使用SpEL来定义缓存的key和各种condition，还提供开箱即用的缓存临时存储方案如 JDK 实现的缓存和 Guava cache，也支持和主流的专业缓存例如 EHCache 和 Redis 集成。Spring Cache的关键原理就是Spring AOP，通过Spring AOP实现了在方法调用前、调用后获取方法的入参和返回值，进而实现了缓存的逻辑。Spring Cache主要注解： @Cacheable 声明使用 Cache，方式是 put if not found @CacheEvict 触发缓存清除操作 @CachePut 在方法执行的时候更新特定缓存，不影响方法执行结果 @Caching 如果一个方法有多个缓存操作可以使用 @CacheConfig 配置当前类中使用的缓存配置 123456public @interface CachePut &#123; String[] value(); //缓存的名字,可看做命名空间，可以把数据写到多个缓存 String key() default ""; //缓存key，命名空间下缓存唯一key，如果不指定将使用默认的KeyGenerator生成. String condition() default ""; //满足缓存条件的数据才会放入缓存，condition在调用方法之前和之后都会判断 String unless() default ""; //用于否决缓存更新的，不像condition，该表达只在方法执行之后判断，此时可以拿到返回值result进行判断了 &#125; 提供的SpEL上下文数据自定义缓存扩展的三步骤，首先需要提供一个CacheManager接口的实现（继承至AbstractCacheManager），管理自身的cache实例；其次，实现自己的cache实例MyCache(继承至Cache)，在这里面引入我们需要的第三方cache或自定义cache；最后就是对配置项进行声明，将MyCache实例注入CacheManager进行统一管理。 Spring cahce整合Redis:Spring-Data-Redis1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;1.8.9.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt; &lt;/dependency&gt; redis.properties123456789redis.host=192.168.0.43redis.port=6379redis.pass=2015redis.maxIdle=50redis.maxActive=50redis.maxWait=50redis.testOnBorrow=trueredis.timeout=1000redis.dbIndex=0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@Configuration@EnableCachingpublic class CacheConfig extends CachingConfigurerSupport &#123; private static final Logger log = Logger.getLogger(CacheConfig.class); @Bean public JedisConnectionFactory redisConnectionFactory() &#123; JedisConnectionFactory factory = new JedisConnectionFactory(); factory.afterPropertiesSet(); factory.setHostName(properties.getHost()); factory.setPort(properties.getPort()); factory.setUsePool(true); factory.setDatabase(properties.getDatabase()); return factory; &#125; @Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory cf) &#123; RedisTemplate&lt;String, String&gt; redisTemplate = new RedisTemplate&lt;String, String&gt;(); redisTemplate.setConnectionFactory(cf); return redisTemplate; &#125; @Bean public CacheManager cacheManager(RedisTemplate redisTemplate) &#123; RedisCacheManager cacheManager = new RedisCacheManager(redisTemplate); cacheManager.setDefaultExpiration(300); return cacheManager; &#125;//默认提供了DefaultKeyGenerator生成器（Spring 4之后使用SimpleKeyGenerator） @Bean public KeyGenerator customKeyGenerator() &#123; return new KeyGenerator() &#123; @Override public Object generate(Object o, Method method, Object... params) &#123; StringBuilder sb = new StringBuilder(); sb.append(o.getClass().getName()); sb.append(method.getName()); for (Object obj : params) &#123; sb.append(obj.toString()); &#125; return sb.toString(); &#125; &#125;; &#125;&#125; 使用123456789101112131415@Controllerpublic class ExampleController &#123; private static final Logger log = Logger.getLogger(ExampleController.class); @RequestMapping(value = "/", method = RequestMethod.GET) @ResponseBody @Cacheable(value = "calculateResult", keyGenerator = "customKeyGenerator") public String calculateResult() &#123; log.debug("Performing expensive calculation..."); // perform computationally expensive calculation return "result"; &#125;&#125; 自定义缓存注解:略。 缓存在高并发场景下的常见问题缓存一致性问题当数据时效性要求很高时，需要保证缓存中的数据与数据库中的保持一致，而且需要保证缓存节点和副本中的数据也保持一致，不能出现差异现象。这就比较依赖缓存的过期和更新策略。一般会在数据发生更改的时，主动更新缓存中的数据或者移除对应的缓存。 缓存并发问题缓存过期后将尝试从后端数据库获取数据，这是一个看似合理的流程。但是，在高并发场景下，有可能多个请求并发的去从数据库获取数据，对后端数据库造成极大的冲击，甚至导致 “雪崩”现象。此外，当某个缓存key在被更新时，同时也可能被大量请求在获取，这也会导致一致性的问题。那如何避免类似问题呢？我们会想到类似“锁”的机制，在缓存更新或者过期的情况下，先尝试获取到锁，当更新或者从数据库获取完成后再释放锁，其他的请求只需要牺牲一定的等待时间，即可直接从缓存中继续获取数据。 缓存穿透问题:结果为空的缓存颠簸问题:一般是由于缓存节点故障导致。业内推荐的做法是通过一致性Hash算法来解决。 缓存的雪崩现象缓存雪崩就是指由于缓存的原因，导致大量请求到达后端数据库，从而导致数据库崩溃，整个系统崩溃，发生灾难。导致这种现象的原因有很多种，上面提到的“缓存并发”，“缓存穿透”，“缓存颠簸”等问题，其实都可能会导致缓存雪崩现象发生。还有一种情况，例如某个时间点内，系统预加载的缓存周期性集中失效了，也可能会导致雪崩。为了避免这种周期性失效，可以通过设置不同的过期时间，来错开缓存过期，从而避免缓存集中失效。从应用架构角度，我们可以通过限流、降级、熔断等手段来降低影响，也可以通过多级缓存来避免这种灾难。 更多参考：https://tech.meituan.com/cache_about.htmlhttps://www.tuicool.com/articles/INRZFrhttps://www.jianshu.com/p/f01ac0873c23https://www.cnblogs.com/dinglang/p/6117309.htmlhttps://github.com/caseyscarborough/spring-redis-caching-examplehttp://jinnianshilongnian.iteye.com/blog/2001040http://blog.csdn.net/l1028386804/article/details/70946410?locationNum=12&amp;fps=1https://www.cnblogs.com/imyijie/p/6518547.htmlhttp://www.cnblogs.com/dinglang/p/6133501.html]]></content>
      <categories>
        <category>中间件</category>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tomcat性能优化]]></title>
    <url>%2F2017%2F12%2F27%2Ftomcat%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[JVM参数优化catalina.sh首先通过添加-server选项来开启server mode，默认情况下是client mode。然后垃圾收集器选择ParNew/CMS收集器，以实现较低的STW时间。然后根据自己的系统测试来评估实际的-Xms -Xmx值。1CATALINA_OPTS=&quot;-server -Xms3G -Xmx3G $CATALINA_OPTS -XX:+UseConcMarkSweepGC -XX:NewSize=1G -XX:+UseParNewGC&quot; 那么如何看到是否合理呢12java -XX:+PrintFlagsFinal tomcat | grep HeapSize java -XX:+PrintFlagsFinal -Xms512m -Xmx1024m -Xss512k -XX:PermSize=64m -XX:MaxPermSize=128m -XX:+PrintFlagsInitial表示打印出所有XX选项的默认值，-XX:+PrintFlagsFinal表示打印出XX选项在运行程序时生效的值。-Xms 设置初始堆的大小，也是最小堆的大小，它等价于：-XX:InitialHeapSize-Xmx 设置最大堆的大小，它等价于-XX:MaxHeapSize。 配置连接器 there are several connectors available for Tomcat: BIO – blocking I/O -&gt; protocol=”HTTP/1.1” (good for low concurrency, or high concurrency without keep-alive from clients) NIO – Non-blocking I/O -&gt; protocol=”org.apache.coyote.http11.Http11NioProtocol” (good for high currency with no keep-alive parameter from clients) Apache Portable Runtime (APR) -&gt; protocol=”org.apache.coyote.http11.Http11AprProtocol”比较：Stability BIO NIO or APRSSL APR NIO BIOLow concurrency BIO APR NIOHigh concurrency No Keep-Alive BIO APR NIOHigh concurrency Keep-Alive APR NIO BIO 注意：Tomcat 8.5 and 9.0 已经移除了BIO连接器。Tomcat 8.5以后，仅支持 NIO, NIO2, and the APR-based connectors. 配置示例：启用nio连接器，禁用dns查询，连接池、请求数调优、压缩123456789101112131415&lt;Connector port="8080" protocol="org.apache.coyote.http11.Http11NioProtocol" connectionTimeout="20000" redirectPort="8443" maxThreads="500" minSpareThreads="100" maxSpareThreads="400" acceptCount="2048" maxConnections="1024" compression="on" compressionMinSize="500" compressableMimeType="text/html,text/xml,text/javascript,text/css,text/plain,application/json" disableUploadTimeout="true" enableLookups="false" URIEncoding="UTF-8" /&gt; maxThreads的值应该根据流量的大小，如果值过低，将有没有足够的线程来处理所有的请求，请求将进入等待状态，只有当一个的处理线程释放后才被处理；如果设置的太大，Tomcat的启动将花费更多时间，同时也会浪费系统资源。compression压缩,默认是off.如果使用nginx代理，此项也没必要设置。acceptCount=”1024”//指定当所有可以使用的处理请求的线程数都被使用时，可以放到处理队列中的请求数，超过这个数的请求将不予处理，直接拒绝。 公式：服务器端最佳线程数量=((线程等待时间+线程cpu时间)/线程cpu时间) * cpu数量 我们可以把tomcat比做一个电影院，流程是取号、买票、观影，acceptCount比作前厅(容纳取到号的人)、maxConnections比作大厅（容纳买到票的人）、maxThreads比作影厅（可以理解一个影厅只容纳一个人，因为一个线程同时只处理一个请求），以下场景是针对已达到maxConnections最大值来讨论的1）取号：如果前厅人数已达到acceptCount，则拿号失败，会得到Connection refused connect的回复信息。反之则会进入前厅，等待买票。2）买票：当大厅人数小于maxConnections时，前厅的人就可以进入大厅3）观影：当影厅的人离开时，大厅的部分人能进入影厅，一般来讲大厅的容量要远大于影厅的数量。 详细参数说明，可以参考：http://blog.csdn.net/lifetragedy/article/details/7708724 注意：这里是http connector的优化，如果使用apache和tomcat做集群的负载均衡，并且使用ajp协议做apache和tomcat的协议转发，那么还需要优化ajp connector。 采用集群 采用Nginx来缓存静态资源，并对内容进行压缩 具体测试可以用ab,jmeter之类的工具。 寻找最佳线程数最佳线程数:性能压测的情况下，起初随着用户数的增加，QPS会上升，当到了一定的阀值之后，用户数量增加QPS并不会增加，或者增加不明显，同时请求的响应时间却大幅增加。这个阀值我们认为是最佳线程数。 为什么要找最佳线程数1.过多的线程只会造成，更多的内存开销，更多的CPU开销，但是对提升QPS确毫无帮助2.找到最佳线程数后通过简单的设置，可以让web系统更加稳定，得到最高，最稳定的QPS输出 最佳线程数的获取：1、通过用户慢慢递增来进行性能压测，观察QPS，响应时间2、根据公式计算:服务器端最佳线程数量=((线程等待时间+线程cpu时间)/线程cpu时间) * cpu数量 配置依据：（1）、部署的程序偏计算型，主要利用cpu资源，应该将该参数设置小一点，减小同一时间抢占cpu资源的线程个数。（2）、部署的程序对io、数据库占用时间较长，线程处于等待的时间较长，应该将该参数调大一点，增加处理个数。应该注意：如果线程数过大，那么cpu用在线程切换的时间占的比重会增大，系统性能会降低。 最佳线程数和jvm堆内存得关系：对于java应用还有一个因素是FULL GC，我们要保证在最佳线程数量下，不会发生频繁FULL GC 参考：http://www.tecbar.net/optimize-tomcat-performance/https://www.itworld.com/article/2764170/networking/tomcat-performance-tuning-tips.htmlhttp://blog.csdn.net/flyliuweisky547/article/details/25161239https://stackoverflow.com/questions/11032739/what-is-the-difference-between-tomcats-bio-connector-and-nio-connectorhttps://tomcat.apache.org/tomcat-8.5-doc/changelog.htmlhttp://tomcat.apache.org/tomcat-7.0-doc/config/http.html#NIO_specific_configurationhttp://blog.csdn.net/architect0719/article/details/50252361http://blog.csdn.net/jewes/article/details/42174893https://stackoverflow.com/questions/26707508/tomcat-performance-tuninghttp://blog.csdn.net/kaka20099527/article/details/53285348https://www.jianshu.com/p/8cfaff5d4bd6http://blog.csdn.net/hzzhoushaoyu/article/details/48769805http://blog.csdn.net/u013802160/article/details/51701644]]></content>
      <categories>
        <category>中间件</category>
        <category>tomcat</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http1.1与1.0及2.0的区别]]></title>
    <url>%2F2017%2F12%2F27%2FHttp1-1%E4%B8%8E1-0%E4%B8%8E2-0%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[HTTP概念HTTP是基于TCP协议之上的。在TCP/IP协议参考模型的各层对应的协议如下图,其中HTTP是应用层的协议。 HTTP1.1与1.0的区别筛选 Persistent Connection持久连接在HTTP1.0中，默认的是短连接，没有正式规定 Connection:Keep-alive 操作；在HTTP1.1中所有连接都是Keep-alive的，也就是默认都是持续连接的（Persistent Connection）。持续连接在同一个tcp的连接中可以传送多个HTTP请求和响应. 多个请求和响应可以同时进行. HTTP1.0 持久连接是通过请求头Connection:keep-alive实现的。即使用着HTTP1.1,但现在很多客户端和服务器仍然在使用这些早期的keep-live连接。实现HTTP/1.0 keep-live连接的客户端可以通过包含Connection:Keep-Alive 首部请求将一条连接保持在打开状态。如果服务器愿意为下一条请求将连接保持在打开状态，就在响应中包含相同的首部，如果响应中没有Connection:Keep-Alive 首部，客户端就认为服务器不支持keep-live，会在发回响应报文后关闭连接。123Connection:Keep-AliveKeep-Alive:max=5,timeout=120#服务器最多还会为另外5个事务保持连接的打开状态，或者将打开状态保持到连接空闲了2分钟以后。 在HTTP/1.0中，keep-alive并不是默认使用的。客户端必需发送一个Connection：Keep-Alive 请求首部来激活keep-alive连接。 HTTP/1.1 逐渐停止了对keep-alive连接的支持，用一种名为持久连接的改进型设计取代了它。持久连接的目的与keep-alive连接的目的相同，但是工作机制更优些。HTTP/1.1持久连接在默认情况下是激活的，除非特别指明，否则HTTP/1.1假定所有的连接都是持久的，要在事务处理结束之后将连接关闭，HTTP/1.1应用程序必须向报文中显示地添加一个Connection：close首部。HTTP1.1客户端加载在收到响应后，除非响应中包含了Connection：close首部，不然HTTP/1.1连接就仍然维持在打开状态。但是，客户端和服务器仍然可以随时关闭空闲的连接。不发送Connection：close并不意味这服务器承诺永远将连接保持在打开状态。注意：1、只有当实体主体部分的长度都和相应的Content-Length一致，或者用分块传输编码方式编码的，连接才能持久保持。2、如果客户端不想在连接上发送其他请求了，就应该在最后一条请求中发送一个Connection：close请求首部3、当服务端返回头包含Connection:close时代表客户端可以关闭该持久连接。 例子：HTTP 1.1支持长连接（PersistentConnection），在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。例如：一个包含有许多图像的网页文件的多个请求和应答可以在一个连接中传输，但每个单独的网页文件的请求和应答仍然需要使用各自的连接。 持久连接会在不同事务之间保持打开状态，直到客户端或服务器决定其关闭为之。重用已对目标服务器打开的空闲持久连接，就可以避开缓慢的连接建立阶段。而且，已经打开的连接还可以避免慢启动的拥塞适应阶段，以便更快速地进行数据传输。所以，持久连接降低了时延和连接建立的开销，将连接保持在已调谐状态，而且减少了打开连接的潜在数量。 HTTP 1.1请求的流水线（Pipelining）处理，还允许客户端不用等待上一次请求结果返回，就可以发出下一次请求，但服务器端必须按照接收到客户端请求的先后顺序依次回送响应结果，以保证客户端能够区分出每次请求的响应内容，这样也显著地减少了整个下载过程所需要的时间。 Host域在HTTP1.0中认为每台服务器都绑定一个唯一的IP地址，因此，请求消息中的URL并没有传递主机名（hostname）。但随着虚拟主机技术的发展，在一台物理服务器上可以存在多个虚拟主机（Multi-homed Web Servers），并且它们共享一个IP地址。HTTP1.1的请求消息和响应消息都应支持Host头域，且请求消息中如果没有Host头域会报告一个错误（400 Bad Request）。 Transfer CodingsHTTP1.1支持chunked transfer，所以可以有Transfer-Encoding头部域:Transfer-Encoding:chunked HTTP1.0则没有。HTTP消息中可以包含任意长度的实体，通常它们使用Content-Length来给出消息结束标志。但是，对于很多动态产生的响应，只能通过缓冲完整的消息来判断消息的大小，但这样做会加大延迟。如果不使用长连接，还可以通过连接关闭的信号来判定一个消息的结束。HTTP/1.1中引入了Chunked transfer-coding(Transfer-Encoding是逐段式（hop-by-hop）的编码，如Chunked编码。)来解决上面这个问题，发送方将消息分割成若干个任意大小的数据块，每个数据块在发送时都会附上块的长度，最后用一个零长度的块作为消息结束的标志。这种方法允许发送方只缓冲消息的一个片段，避免缓冲整个消息带来的过载。 Range 和 Content-Range(节约优化)HTTP1.1支持传送内容的一部分。比方说，当客户端已经有内容的一部分，为了节省带宽，可以只向服务器请求一部分。HTTP/1.0中，存在一些浪费带宽的现象，例如客户端只是需要某个对象的一部分，而服务器却将整个对象送过来了。例如，客户端只需要显示一个文档的部分内容，又比如下载大文件时需要支持断点续传功能，而不是在发生断连后不得不重新下载完整的包。HTTP/1.1中在请求消息中引入了range头域，它允许只请求资源的某个部分。在响应消息中Content-Range头域声明了返回的这部分对象的偏移值和长度。如果服务器相应地返回了对象所请求范围的内容，则响应码为206（Partial Content），它可以防止Cache将响应误以为是完整的一个对象。 Request methodHTTP1.1增加了OPTIONS,PUT, DELETE, TRACE, CONNECT这些Request方法. Status codeHTTP1.1 增加了一些新的status code。如：HTTP/1.1加入了一个新的状态码100（Continue）。客户端事先发送一个只带头域的请求，如果服务器因为权限拒绝了请求，就回送响应码401（Unauthorized）；如果服务器接收此请求就回送响应码100，客户端就可以继续发送带实体的完整请求了。 HTTP 1.1状态代码及其含义状态代码有三位数字组成，第一个数字定义了响应的类别，且有五种可能取值： 1xx：指示信息–表示请求已接收，继续处理 2xx：成功–表示请求已被成功接收、理解、接受 3xx：重定向–要完成请求必须进行更进一步的操作 4xx：客户端错误–请求有语法错误或请求无法实现 5xx：服务器端错误–服务器未能实现合法的请求 Cache (缓存)在HTTP/1.0中，使用Expire头域来判断资源的fresh或stale，并使用条件请求（conditional request）来判断资源是否仍有效。例如，cache服务器通过If-Modified-Since头域向服务器验证资源的Last-Modefied头域是否有更新，源服务器可能返回304（Not Modified），则表明该对象仍有效；也可能返回200（OK）替换请求的Cache对象。此外，HTTP/1.0中还定义了Pragma:no-cache头域，客户端使用该头域说明请求资源不能从cache中获取，而必须回源获取。HTTP/1.1在1.0的基础上加入了一些cache的新特性，当缓存对象的Age超过Expire时变为stale对象，cache不需要直接抛弃stale对象，而是与源服务器进行重新激活（revalidation）。HTTP/1.0中，If-Modified-Since头域使用的是绝对时间戳，精确到秒，但使用绝对时间会带来不同机器上的时钟同步问题。而HTTP/1.1中引入了一个ETag头域用于重激活机制，它的值entity tag可以用来唯一的描述一个资源。请求消息中可以使用If-None-Match头域来匹配资源的entitytag是否有变化。HTTP/1.1增加了Cache-Control头域（请求消息和响应消息都可使用），它支持一个可扩展的指令子集：例如max-age指令支持相对时间戳；private和no-store指令禁止对象被缓存；no-transform阻止Proxy进行任何改变响应的行为。 HTTP 1.1持久连接的好处例如，一个包含有许多图像的网页文件中并没有包含真正的图像数据内容，而只是指明了这些图像的URL地址，当WEB浏览器访问这个网页文件时，浏览器首先要发出针对该网页文件的请求，当浏览器解析WEB服务器返回的该网页文档中的HTML内容时，发现其中的&lt; img&gt;图像标签后，浏览器将根据src属性所指定的URL地址再次向服务器发出下载图像数据的请求，如图所示:当一个网页文件中包含Applet，JavaScript文件，CSS文件等内容时，也会出现类似上述的情况。 为了克服HTTP 1.0的这个缺陷，HTTP1.1支持持久连接，在一个TCP连接上可以传送多个HTTP请求和响应，减少了建立和关闭连接的消耗和延迟。一个包含有许多图像的网页文件的多个请求和应答可以在一个连接中传输，但每个单独的网页文件的请求和应答仍然需要使用各自的连接。HTTP 1.1还允许客户端不用等待上一次请求结果返回，就可以发出下一次请求，但服务器端必须按照接收到客户端请求的先后顺序依次回送响应结果，以保证客户端能够区分出每次请求的响应内容，这样也显著地减少了整个下载过程所需要的时间。基于HTTP 1.1协议的客户机与服务器的信息交换过程，如图所示: 总结：HTTP 1.1通过增加更多的请求头和响应头来改进和扩充HTTP1.0的功能。例如，由于HTTP 1.0不支持Host请求头字段，WEB浏览器无法使用主机头名来明确表示要访问服务器上的哪个WEB站点，这样就无法使用WEB服务器在同一个IP地址和端口号上配置多个虚拟WEB站点。在HTTP 1.1中增加Host请求头字段后，WEB浏览器可以使用主机头名来明确表示要访问服务器上的哪个WEB站点，这才实现了在一台WEB服务器上可以在同一个IP地址和端口号上使用不同的主机名来创建多个虚拟WEB站点。HTTP 1.1的持续连接，也需要增加新的请求头来帮助实现，例如，Connection请求头的值为Keep-Alive时，客户端通知服务器返回本次请求结果后保持连接；Connection请求头的值为close时，客户端通知服务器返回本次请求结果后关闭连接。HTTP 1.1还提供了与身份认证、状态管理和Cache缓存等机制相关的请求头和响应头。 HTTP2.0多路复用 (Multiplexing)多路复用允许同时通过单一的 HTTP/2 连接发起多重的请求-响应消息。在 HTTP/1.1 协议中浏览器客户端在同一时间，针对同一域名下的请求有一定数量限制。超过限制数目的请求会被阻塞。这也是为何一些站点会有多个静态资源 CDN 域名的原因之一.而 HTTP/2 的多路复用(Multiplexing) 则允许同时通过单一的 HTTP/2 连接发起多重的请求-响应消息。因此 HTTP/2 可以很容易的去实现多流并行而不用依赖建立多个 TCP 连接，HTTP/2 把 HTTP 协议通信的基本单位缩小为一个一个的帧，这些帧对应着逻辑流中的消息。并行地在同一个 TCP 连接上双向交换消息。 二进制分帧HTTP/2在 应用层(HTTP/2)和传输层(TCP or UDP)之间增加一个二进制分帧层。解决了HTTP1.1 的性能限制，改进传输性能，实现低延迟和高吞吐量。在二进制分帧层中， HTTP/2 会将所有传输的信息分割为更小的消息和帧（frame）,并对它们采用二进制格式的编码 ，其中 HTTP1.x 的首部信息会被封装到 HEADER frame，而相应的 Request Body 则封装到 DATA frame 里面。HTTP/2 通信都在一个连接上完成，这个连接可以承载任意数量的双向数据流。在过去， HTTP 性能优化的关键并不在于高带宽，而是低延迟。TCP 连接会随着时间进行自我调谐，起初会限制连接的最大速度，如果数据成功传输，会随着时间的推移提高传输的速度。这种调谐则被称为[TCP 慢启动TCP Slow-start](http://en.wikipedia.org/wiki/Slow-start)。由于这种原因，让原本就具有突发性和短时性的 HTTP 连接变的十分低效。HTTP/2 通过让所有数据流共用同一个连接，可以更有效地使用 TCP 连接，让高带宽也能真正的服务于 HTTP 的性能提升。这种单连接多资源的方式，减少服务端的链接压力,内存占用更少,连接吞吐量更大；而且由于 TCP 连接的减少而使网络拥塞状况得以改善，同时慢启动时间的减少,使拥塞和丢包恢复速度更快。 首部压缩（Header Compression）HTTP/1.1并不支持 HTTP 首部压缩，为此 SPDY 和 HTTP/2 应运而生， SPDY 使用的是通用的DEFLATE 算法，而 HTTP/2 则使用了专门为首部压缩而设计的 HPACK 算法。 服务端推送（Server Push）服务端推送是一种在客户端请求之前发送数据的机制。在 HTTP/2 中，服务器可以对客户端的一个请求发送多个响应。 参考:https://www.greenbytes.de/tech/webdav/rfc2616.html#rfc.section.19.6.1http://www8.org/w8-papers/5c-protocols/key/key.htmlhttps://stackoverflow.com/questions/246859/http-1-0-vs-1-1http://www.ra.ethz.ch/cdstore/www8/data/2136/pdf/pd1.pdfhttp://blog.csdn.net/hguisu/article/details/8608888https://www.cnblogs.com/shijingxiang/articles/4434643.htmlhttps://www.jianshu.com/p/52d86558ca57http://blog.csdn.net/hguisu/article/details/8680808]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker之入门笔记]]></title>
    <url>%2F2017%2F12%2F26%2FDocker%E4%B9%8B%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[书籍为：https://yeasy.gitbooks.io/docker_practice/content/最新Docker版本：17.09.1-ce 15年的时候就接触过docker，不过当时docker生态并没有现在这么完善，而且跨主机间通信什么的过于复杂，就放弃了。都忘得差不多了，现在拾起来重新学习。加入macroservice+docker的潮流当中。 Docker 是个划时代的开源项目，它彻底释放了计算虚拟化的威力，极大提高了应用的运行效率，降低了云计算资源供应的成本！使用 Docker，可以让应用的部署、测试和分发都变得前所未有的高效和轻松！ 历史：Docker 最初是 dotCloud 公司创始人 Solomon Hykes 在法国期间发起的一个公司内部项目，它是基于 dotCloud 公司多年云服务技术的一次革新，并于 2013 年 3 月以 Apache 2.0 授权协议开源，主要项目代码在 GitHub 上进行维护。Docker 项目后来还加入了 Linux 基金会，并成立推动 开放容器联盟（OCI）。Docker 最初是在 Ubuntu 12.04 上开发实现的，不过现在已经支持多个平台了。Docker 使用 Google 公司推出的 Go 语言 进行开发实现，基于 Linux 内核的 cgroup，namespace，以及 AUFS 类的 Union FS 等技术，对进程进行封装隔离，属于 操作系统层面的虚拟化技术。由于隔离的进程独立于宿主和其它的隔离的进程，因此也称其为容器。最初实现是基于 LXC，从 0.7 版本以后转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。 Docker与传统虚拟化的区别：下面的图片比较了 Docker 和传统虚拟化方式的不同之处。传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程；而容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核，而且也没有进行硬件虚拟。因此容器要比传统虚拟机更为轻便。 为什么要使用 Docker？ 更高效的利用系统资源由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。 更快速的启动时间传统的虚拟机技术启动应用服务往往需要数分钟，而 Docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。 一致的运行环境开发过程中一个常见的问题是环境一致性问题。 持续交付和部署对开发和运维（DevOps）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。而且使用 Dockerfile 使镜像构建透明化. 更轻松的迁移由于 Docker 确保了执行环境的一致性，使得应用的迁移更加容易。 更轻松的维护和扩展 Docker 包括三个基本概念 镜像（Image） 容器（Container） 仓库（Repository） Docker 镜像是一个特殊的文件系统，除了提供容器运行时所需的程序、库、资源、配置等文件外，还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。镜像不包含任何动态数据，其内容在构建之后也不会被改变。 分层存储由于ISO镜像包含操作系统完整的 root 文件系统，其体积往往是庞大的，因此在 Docker 设计时，就充分利用 Union FS 的技术，将其设计为分层存储的架构。Docker镜像只是一个虚拟的概念，其实际体现并非由一个文件组成，而是由一组文件系统组成，或者说，由多层文件系统联合组成。 镜像构建时，会一层层构建，前一层是后一层的基础。每一层构建完就不会再发生改变，后一层上的任何改变只发生在自己这一层。比如，删除前一层文件的操作，实际不是真的删除前一层的文件，而是仅在当前层标记为该文件已删除。在最终容器运行的时候，虽然不会看到这个文件，但是实际上该文件会一直跟随镜像。因此，在构建镜像的时候，需要额外小心，每一层尽量只包含该层需要添加的东西，任何额外的东西应该在该层构建结束前清理掉。分层存储的特征还使得镜像的复用、定制变的更为容易。甚至可以用之前构建好的镜像作为基础层，然后进一步添加新的层，以定制自己所需的内容，构建新的镜像。注意：目前最多支持127层。 镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的 类 和 实例 一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 容器的实质是进程，但与直接在宿主执行的进程不同，容器进程运行于属于自己的独立的 命名空间。因此容器可以拥有自己的 root 文件系统、自己的网络配置、自己的进程空间，甚至自己的用户 ID 空间。容器内的进程是运行在一个隔离的环境里，使用起来，就好像是在一个独立于宿主的系统下操作一样。这种特性使得容器封装的应用比直接在宿主运行更加安全。也因为这种隔离的特性，很多人初学 Docker 时常常会混淆容器和虚拟机。 前面讲过镜像使用的是分层存储，容器也是如此。每一个容器运行时，是以镜像为基础层，在其上创建一个当前容器的存储层，我们可以称这个为容器运行时读写而准备的存储层为容器存储层。 容器存储层的生存周期和容器一样，容器消亡时，容器存储层也随之消亡。因此，任何保存于容器存储层的信息都会随容器删除而丢失。按照 Docker 最佳实践的要求，容器不应该向其存储层内写入任何数据，容器存储层要保持无状态化。所有的文件写入操作，都应该使用 数据卷（Volume）、或者绑定宿主目录。 数据卷的生存周期独立于容器，容器消亡，数据卷不会消亡。因此，使用数据卷后，容器删除或者重新运行之后，数据却不会丢失。 Docker Registry(Docker镜像仓库)一个 Docker Registry 中可以包含多个仓库（Repository）；每个仓库可以包含多个标签（Tag）；每个标签对应一个镜像。通过 &lt;仓库名&gt;:&lt;标签&gt; 的格式来指定具体是这个软件哪个版本的镜像。如果不给出标签，将以 latest 作为默认标签。如ubuntu:16.04最常使用的 Registry 公开服务： 官方的 Docker Hub CoreOS 的 Quay.io 阿里云加速器 DaoCloud 加速器 时速云镜像仓库 网易云镜像服务 私有 Docker Registry: docker-registry VMWare Harbor Sonatype Nexus 安装以ubuntu为例方式一：1234567891011121314151617181920212223242526272829303132#旧版叫docker或docker-engine$ sudo apt-get remove docker \ docker-engine \ docker.io$ sudo apt-get update#安装AUFS内核驱动$ sudo apt-get install \ linux-image-extra-$(uname -r) \ linux-image-extra-virtual$ sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ software-properties-common$ curl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# 官方源# $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -$ sudo add-apt-repository \ "deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu \ $(lsb_release -cs) \ stable"# 官方源# $ sudo add-apt-repository \# "deb [arch=amd64] https://download.docker.com/linux/ubuntu \# $(lsb_release -cs) \# stable"$ sudo apt-get update$ sudo apt-get install docker-ce 方式二：12$ curl -fsSL get.docker.com -o get-docker.sh$ sudo sh get-docker.sh --mirror Aliyun 启动docker服务：12$ sudo systemctl enable docker$ sudo systemctl start docker 建立 docker 用户组:默认情况下，docker 命令会使用 Unix socket 与 Docker 引擎通讯。而只有 root 用户和 docker 组的用户才可以访问 Docker 引擎的 Unix socket。12$ sudo groupadd docker$ sudo usermod -aG docker $USER 测试 Docker 是否安装正确1$ docker run hello-world 配置镜像加速器： Docker 官方提供的中国 registry mirror 阿里云加速器 DaoCloud 加速器 Ubuntu 16.04+、Debian 8+、CentOS 7对于使用 systemd 的系统，请在 /etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）12345&#123; "registry-mirrors": [ "https://registry.docker-cn.com" ]&#125; 注意，一定要保证该文件符合 json 规范，否则 Docker 将不能启动。之后重新启动服务。12$ sudo systemctl daemon-reload$ sudo systemctl restart docker 镜像基本操作获取镜像:docker pull [选项] [Docker Registry 地址[:端口号]/]仓库名[:标签]1$ docker pull ubuntu:16.04 运行：1234$ docker run -it --rm \ ubuntu:16.04 \ bashroot@e7009c6ce357:/# cat /etc/os-release -it：这是两个参数，一个是 -i：交互式操作，一个是 -t 终端。--rm：这个参数是说容器退出后随之将其删除。默认情况下，为了排障需求，退出的容器并不会立即删除，除非手动 docker rm。 列出镜像:1234$ docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEredis latest 5f515359c7f8 5 days ago 183 MBnginx latest 05a60462f8ba 5 days ago 181 MB 镜像 ID 则是镜像的唯一标识，一个镜像可以对应多个标签。 镜像体积:1、Docker Hub 中显示的体积是压缩后的体积2、docker image ls 列表中的镜像体积总和并非是所有镜像实际硬盘消耗。由于 Docker 镜像是多层存储结构，并且可以继承、复用，因此不同镜像可能会因为使用相同的基础镜像，从而拥有共同的层。由于 Docker 使用 Union FS，相同的层只需要保存一份即可，因此实际镜像硬盘占用空间很可能要比这个列表镜像大小的总和要小的多。 你可以通过以下命令来便捷的查看镜像、容器、数据卷所占用的空间。1234567$ docker system dfTYPE TOTAL ACTIVE SIZE RECLAIMABLEImages 24 0 1.992GB 1.992GB (100%)Containers 1 0 62.82MB 62.82MB (100%)Local Volumes 9 0 652.2MB 652.2MB (100%)Build Cache 0B 0B 虚悬镜像:这个镜像既没有仓库名，也没有标签，均为 &lt; none&gt;。1&lt;none&gt; &lt;none&gt; 00285df0df87 5 days ago 342 MB 由于新旧镜像同名，旧镜像名称被取消，从而出现仓库名、标签均为 &lt; none&gt; 的镜像。这类无标签镜像也被称为 虚悬镜像(dangling image) ，可以用下面的命令专门显示这类镜像：1$ docker image ls -f dangling=true 一般来说，虚悬镜像已经失去了存在的价值，是可以随意删除的，可以用下面的命令删除。1$ docker image prune #对于stopped容器可以docker container prune 中间层镜像:为了加速镜像构建、重复利用资源，Docker 会利用 中间层镜像。默认的 docker image ls 列表中只会显示顶层镜像，如果希望显示包括中间层镜像在内的所有镜像的话，需要加 -a 参数。1$ docker image ls -a 这样会看到很多无标签的镜像，与之前的虚悬镜像不同，这些无标签的镜像很多都是中间层镜像，是其它镜像所依赖的镜像。这些无标签镜像不应该删除，否则会导致上层镜像因为依赖丢失而出错。 列出部分镜像:1234$ docker image ls ubuntu$ docker image ls ubuntu:16.04$ docker image ls -f since=mongo:3.2 #--filter -f before=xxx$ docker image ls -f label=com.example.version=0.1 以特定格式显示:123456789101112$ docker image ls -q5f515359c7f805a60462f8ba$ docker image ls --format &quot;&#123; &#123;.ID&#125; &#125;: &#123; &#123;.Repository&#125; &#125;&quot;5f515359c7f8: redis05a60462f8ba: nginx$ docker image ls --format &quot;table &#123; &#123;.ID&#125; &#125;\t&#123; &#123;.Repository&#125; &#125;\t&#123; &#123;.Tag&#125; &#125;&quot;IMAGE ID REPOSITORY TAG5f515359c7f8 redis latest05a60462f8ba nginx latest 删除本地镜像:1$ docker image rm [选项] &lt;镜像1&gt; [&lt;镜像2&gt; ...] 其中，&lt;镜像&gt; 可以是 镜像短 ID、镜像长 ID、镜像名 或者 镜像摘要。 12345678$ docker image rm 501$ docker image rm centos$ docker image rm ubuntu:16.04比如，我们需要删除所有仓库名为 redis 的镜像：$ docker image rm $(docker image ls -q redis)或者删除所有在 mongo:3.2 之前的镜像：$ docker image rm $(docker image ls -q -f before=mongo:3.2) CentOS/RHEL 的用户需要注意的事项在 Ubuntu/Debian 上有 UnionFS 可以使用，如 aufs 或者 overlay2，而 CentOS 和 RHEL 的内核中没有相关驱动。因此对于这类系统，一般使用 devicemapper 驱动利用 LVM 的一些机制来模拟分层存储。这样的做法除了性能比较差外，稳定性一般也不好，而且配置相对复杂。 对于 CentOS/RHEL 的用户来说，在没有办法使用 UnionFS 的情况下，一定要配置 direct-lvm 给 devicemapper，无论是为了性能、稳定性还是空间利用率。 或许有人注意到了 CentOS 7 中存在被 backports 回来的 overlay 驱动，不过 CentOS 里的这个驱动达不到生产环境使用的稳定程度，所以不推荐使用。 利用 commit 理解镜像构成docker commit 命令除了学习之外，还有一些特殊的应用场合，比如被入侵后保存现场等。但是，不要使用 docker commit 定制镜像，定制镜像应该使用 Dockerfile 来完成。镜像是多层存储，每一层是在前一层的基础上进行的修改；而容器同样也是多层存储，是在以镜像为基础层，在其基础上加一层作为容器运行时的存储层。docker commit [选项] &lt;容器ID或容器名&gt; [&lt;仓库名&gt;[:&lt;标签&gt;]] 现在让我们以定制一个 Web 服务器为例子，来讲解镜像是如何构建的。1234567891011121314$ docker run --name webserver -d -p 80:80 nginx$ docker exec -it webserver bashroot@3729b97e8226:/# echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.htmlroot@3729b97e8226:/# exitexit$ docker diff webserver$ docker commit \ --author &quot;Tao Wang &lt;twang2218@gmail.com&gt;&quot; \ --message &quot;修改了默认网页&quot; \ webserver \ nginx:v2$ docker history nginx:v2 使用 docker commit 命令虽然可以比较直观的帮助理解镜像分层存储的概念，但是实际环境中并不会这样使用。使用 docker commit 意味着所有对镜像的操作都是黑箱操作，生成的镜像也被称为黑箱镜像，换句话说，就是除了制作镜像的人知道执行过什么命令、怎么生成的镜像，别人根本无从得知。虽然 docker diff 或许可以告诉得到一些线索，但是远远不到可以确保生成一致镜像的地步。这种黑箱镜像的维护工作是非常痛苦的。 而且，回顾之前提及的镜像所使用的分层存储的概念，除当前层外，之前的每一层都是不会发生改变的，换句话说，任何修改的结果仅仅是在当前层进行标记、添加、修改，而不会改动上一层。如果使用 docker commit 制作镜像，以及后期修改的话，每一次修改都会让镜像更加臃肿一次，所删除的上一层的东西并不会丢失，会一直如影随形的跟着这个镜像. 使用 Dockerfile 定制镜像https://docs.docker.com/engine/reference/builder/Dockerfile 是一个文本文件，其内包含了一条条的指令(Instruction)，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。注意：Dockerfile中每一个指令都会建立一层。Union FS 是有最大层数限制的，比如 AUFS，曾经是最大不得超过 42 层，现在是不得超过 127 层。 12FROM nginxRUN echo &apos;&lt;h1&gt;Hello, Docker!&lt;/h1&gt;&apos; &gt; /usr/share/nginx/html/index.html FROM指定基础镜像RUN 指令是用来执行命令行命令的 除了选择现有镜像为基础镜像外，Docker 还存在一个特殊的镜像，名为 scratch。这个镜像是虚拟的概念，并不实际存在，它表示一个空白的镜像。 Dockerfile中每一个指令都会建立一层，RUN 也不例外。每一个 RUN 的行为，就和刚才我们手工建立镜像的过程一样：新建立一层，在其上执行这些命令，执行结束后，commit 这一层的修改，构成新的镜像。不合理写法：(建立了7层镜像)123456789FROM debian:jessieRUN apt-get updateRUN apt-get install -y gcc libc6-dev makeRUN wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-3.2.5.tar.gz&quot;RUN mkdir -p /usr/src/redisRUN tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1RUN make -C /usr/src/redisRUN make -C /usr/src/redis install 合理写法：(一层镜像)1234567891011121314FROM debian:jessieRUN buildDeps=&apos;gcc libc6-dev make&apos; \ &amp;&amp; apt-get update \ &amp;&amp; apt-get install -y $buildDeps \ &amp;&amp; wget -O redis.tar.gz &quot;http://download.redis.io/releases/redis-3.2.5.tar.gz&quot; \ &amp;&amp; mkdir -p /usr/src/redis \ &amp;&amp; tar -xzf redis.tar.gz -C /usr/src/redis --strip-components=1 \ &amp;&amp; make -C /usr/src/redis \ &amp;&amp; make -C /usr/src/redis install \ &amp;&amp; rm -rf /var/lib/apt/lists/* \ &amp;&amp; rm redis.tar.gz \ &amp;&amp; rm -r /usr/src/redis \ &amp;&amp; apt-get purge -y --auto-remove $buildDeps 在撰写 Dockerfile 的时候，要经常提醒自己，这并不是在写 Shell 脚本，而是在定义每一层该如何构建。 此外，还可以看到这一组命令的最后添加了清理工作的命令，删除了为了编译构建所需要的软件，清理了所有下载、展开的文件，并且还清理了 apt 缓存文件。这是很重要的一步，我们之前说过，镜像是多层存储，每一层的东西并不会在下一层被删除，会一直跟随着镜像。因此镜像构建时，一定要确保每一层只添加真正需要添加的东西，任何无关的东西都应该清理掉。 很多人初学 Docker 制作出了很臃肿的镜像的原因之一，就是忘记了每一层构建的最后一定要清理掉无关文件。 构建镜像:docker build [选项] &lt;上下文路径/URL/-&gt;1$ docker build -t nginx:v3 . 镜像构建上下文（Context）:如果注意，会看到 docker build 命令最后有一个 .。. 表示当前目录，而 Dockerfile 就在当前目录，因此不少初学者以为这个路径是在指定 Dockerfile 所在路径，这么理解其实是不准确的。那么什么是上下文呢？首先我们要理解 docker build 的工作原理。Docker 在运行时分为 Docker 引擎（也就是服务端守护进程）和客户端工具。Docker 的引擎提供了一组 REST API，被称为 Docker Remote API，而如 docker 命令这样的客户端工具，则是通过这组 API 与 Docker 引擎交互，从而完成各种功能。因此，虽然表面上我们好像是在本机执行各种 docker 功能，但实际上，一切都是使用的远程调用形式在服务端（Docker 引擎）完成。也因为这种 C/S 设计，让我们操作远程服务器的 Docker 引擎变得轻而易举。 当我们进行镜像构建的时候，并非所有定制都会通过 RUN 指令完成，经常会需要将一些本地文件复制进镜像，比如通过 COPY 指令、ADD 指令等。而 docker build 命令构建镜像，其实并非在本地构建，而是在服务端，也就是 Docker 引擎中构建的。那么在这种客户端/服务端的架构中，如何才能让服务端获得本地文件呢？这就引入了上下文的概念。当构建的时候，用户会指定构建镜像上下文的路径，docker build 命令得知这个路径后，会将路径下的所有内容打包，然后上传给 Docker 引擎。这样 Docker 引擎收到这个上下文包后，展开就会获得构建镜像所需的一切文件。 如果在 Dockerfile 中这么写：COPY ./package.json /app/这并不是要复制执行 docker build 命令所在的目录下的 package.json，也不是复制 Dockerfile 所在目录下的 package.json，而是复制 上下文（context） 目录下的 package.json。 因此，COPY 这类指令中的源文件的路径都是相对路径。这也是初学者经常会问的为什么 COPY ../package.json /app 或者 COPY /opt/xxxx /app 无法工作的原因，因为这些路径已经超出了上下文的范围，Docker 引擎无法获得这些位置的文件。如果真的需要那些文件，应该将它们复制到上下文目录中去。现在就可以理解刚才的命令 docker build -t nginx:v3 . 中的这个 .，实际上是在指定上下文的目录，docker build 命令会将该目录下的内容打包交给 Docker 引擎以帮助构建镜像。 一般来说，应该会将 Dockerfile 置于一个空目录下，或者项目根目录下。如果目录下有些东西确实不希望构建时传给 Docker 引擎，那么可以用 .gitignore 一样的语法写一个 .dockerignore，该文件是用于剔除不需要作为上下文传递给 Docker 引擎的。 那么为什么会有人误以为 . 是指定 Dockerfile 所在目录呢？这是因为在默认情况下，如果不额外指定 Dockerfile 的话，会将上下文目录下的名为 Dockerfile 的文件作为 Dockerfile。你也可以用 -f ../Dockerfile 参数指定某个文件作为 Dockerfile 直接用 Git repo 进行构建:1$ docker build https://github.com/twang2218/gitlab-ce-zh.git#:8.14 这行命令指定了构建所需的 Git repo，并且指定默认的 master 分支，构建目录为 /8.14/，然后 Docker 就会自己去 git clone 这个项目、切换到指定分支、并进入到指定目录后开始构建。 用给定的 tar 压缩包构建:1$ docker build http://server/context.tar.gz 如果所给出的 URL 不是个 Git repo，而是个 tar 压缩包，那么 Docker 引擎会下载这个包，并自动解压缩，以其作为上下文，开始构建。 从标准输入中读取上下文压缩包进行构建:1$ docker build - &lt; context.tar.gz 自动解压缩，以其作为上下文，开始构建。 从标准输入中读取 Dockerfile 进行构建:123docker build - &lt; Dockerfile或cat Dockerfile | docker build - COPY复制文件12COPY &lt;源路径&gt;... &lt;目标路径&gt;COPY [&quot;&lt;源路径1&gt;&quot;,... &quot;&lt;目标路径&gt;&quot;] &lt;源路径&gt; 可以是多个，甚至可以是通配符，其通配符规则要满足 Go 的 filepath.Match 规则.目标路径不需要事先创建，如果目录不存在会在复制文件前先行创建缺失目录。123COPY package.json /usr/src/app/COPY hom* /mydir/COPY hom?.txt /mydir/ 注意:使用 COPY 指令，源文件的各种元数据都会保留。比如读、写、执行权限、文件变更时间等。 ADD更高级的复制文件如果 &lt;源路径&gt; 为一个 tar 压缩文件的话，压缩格式为 gzip, bzip2 以及 xz 的情况下，ADD 指令将会自动解压缩这个压缩文件到 &lt;目标路径&gt; 去。注意:ADD 指令会令镜像构建缓存失效，从而可能会令镜像构建变得比较缓慢。在 COPY 和 ADD 指令中选择的时候:所有的文件复制均使用 COPY 指令，仅在需要自动解压缩的场合使用 ADD。 CMD容器启动命令CMD 指令的格式和 RUN 相似，也支持shell格式和exec格式： shell 格式：CMD &lt;命令&gt; exec 格式：CMD [“可执行文件”, “参数1”, “参数2”…] 参数列表格式：CMD [“参数1”, “参数2”…]。在指定了 ENTRYPOINT 指令后，用 CMD 指定具体的参数。 如果使用 shell 格式的话，实际的命令会被包装为 sh -c 的参数的形式进行执行。比如：CMD echo $HOME在实际执行中，会将其变更为：CMD [ “sh”, “-c”, “echo $HOME” ]这就是为什么我们可以使用环境变量的原因，因为这些环境变量会被 shell 进行解析处理。 容器中应用在前台执行和后台执行的问题:Docker 不是虚拟机，容器中的应用都应该以前台执行，而不是像虚拟机、物理机里面那样，用 upstart/systemd 去启动后台服务，容器内没有后台服务的概念。一些初学者将 CMD 写为：CMD service nginx start然后发现容器执行后就立即退出了。甚至在容器内去使用 systemctl 命令结果却发现根本执行不了。这就是因为没有搞明白前台、后台的概念，没有区分容器和虚拟机的差异，依旧在以传统虚拟机的角度去理解容器。对于容器而言，其启动程序就是容器应用进程，容器就是为了主进程而存在的，主进程退出，容器就失去了存在的意义，从而退出.比较典型的例子就是nginx和supervisor:12CMD [&quot;nginx&quot;, &quot;-g&quot;, &quot;daemon off;&quot;] #&quot;daemon off;&quot;CMD /usr/bin/supervisord -n -c /etc/supervisor/supervisord.conf # -n no daemon ENTRYPOINT 入口点ENTRYPOINT 的目的和 CMD 一样，都是在指定容器启动程序及参数。ENTRYPOINT 在运行时也可以替代，不过比 CMD 要略显繁琐，需要通过 docker run 的参数 --entrypoint来指定。 当指定了 ENTRYPOINT 后，CMD 的含义就发生了改变，不再是直接的运行其命令，而是将 CMD 的内容作为参数传给 ENTRYPOINT 指令，换句话说实际执行时，将变为：&lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot; 那么有了 CMD 后，为什么还要有 ENTRYPOINT 呢？场景一：让镜像变成像命令一样使用12CMD [ &quot;curl&quot;, &quot;-s&quot;, &quot;http://ip.cn&quot; ]$ docker run myip # 不支持添加参数-i docker run myip -i 12ENTRYPOINT [ &quot;curl&quot;, &quot;-s&quot;, &quot;http://ip.cn&quot; ]$ docker run myip -i #支持添加参数，其实这里的 -i就相当于运行时指定的CMD.即CMD作为ENTRYPOINT的参数 场景二：应用运行前的准备工作启动容器就是启动主进程，但有些时候，启动主进程前，需要一些准备工作。比如 mysql 类的数据库，可能需要一些数据库配置、初始化的工作，这些工作要在最终的 mysql 服务器运行之前解决。此外，可能希望避免使用 root 用户去启动服务，从而提高安全性，而在启动服务前还需要以 root 身份执行一些必要的准备工作，最后切换到服务用户身份启动服务。这种情况下，可以写一个脚本，然后放入 ENTRYPOINT 中去执行，而这个脚本会将接到的参数（也就是 ）12345678FROM alpine:3.4...RUN addgroup -S redis &amp;&amp; adduser -S -G redis redis...ENTRYPOINT ["docker-entrypoint.sh"]EXPOSE 6379CMD [ "redis-server" ] docker-entrypoint.sh 脚本:123456789#!/bin/sh...# allow the container to be started with `--user`if [ "$1" = 'redis-server' -a "$(id -u)" = '0' ]; then chown -R redis . exec su-exec redis "$0" "$@"fiexec "$@" ENV 设置环境变量设置环境变量，无论是后面的其它指令，如 RUN，还是运行时的应用，都可以直接使用这里定义的环境变量。12ENV &lt;key&gt; &lt;value&gt;ENV &lt;key1&gt;=&lt;value1&gt; &lt;key2&gt;=&lt;value2&gt; 12ENV VERSION=1.0 DEBUG=on \ NAME=&quot;Happy Feet&quot; ARG 构建参数1格式：ARG &lt;参数名&gt;[=&lt;默认值&gt;] 构建参数和 ENV 的效果一样，都是设置环境变量。所不同的是，ARG 所设置的构建环境的环境变量，在将来容器运行时是不会存在这些环境变量的。该默认值可以在构建命令 docker build 中用 --build-arg &lt;参数名&gt;=&lt;值&gt; 来覆盖。 VOLUME定义匿名卷12VOLUME [&quot;&lt;路径1&gt;&quot;, &quot;&lt;路径2&gt;&quot;...]VOLUME &lt;路径&gt; 在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷1VOLUME /data 这里的 /data 目录就会在运行时自动挂载为匿名卷，任何向 /data 中写入的信息都不会记录进容器存储层，从而保证了容器存储层的无状态化。当然，运行时可以覆盖这个挂载设置。比如：docker run -d -v mydata:/data xxxx EXPOSE 声明端口1格式为 EXPOSE &lt;端口1&gt; [&lt;端口2&gt;...] EXPOSE 指令是声明运行时容器提供服务端口，这只是一个声明，在运行时并不会因为这个声明应用就会开启这个端口的服务。两个好处:1、相当于文档说明，以方便配置映射；2、docker run -P 时，会自动随机映射 EXPOSE 的端口。 在早期 Docker 版本中,所有容器都运行于默认桥接网络中，因此所有容器互相之间都可以直接访问，这样存在一定的安全性问题。于是有了一个 Docker 引擎参数--icc=false，当指定该参数后，容器间将默认无法互访，除非互相间使用了 --links 参数的容器才可以互通，并且只有镜像中 EXPOSE 所声明的端口才可以被访问。–icc=false 的用法，在引入了 docker network 后已经基本不用了，通过自定义网络可以很轻松的实现容器间的互联与隔离。 WORKDIR 指定工作目录格式为 WORKDIR &lt; 工作目录路径&gt;。使用 WORKDIR 指令可以来指定工作目录（或者称为当前目录），以后各层的当前目录就被改为指定的目录，如该目录不存在，WORKDIR 会帮你建立目录。使用cd行不行？如下会存在问题12RUN cd /appRUN echo &quot;hello&quot; &gt; world.txt 构建后会找不到 /app/world.txt 文件。原因其实很简单，在 Shell 中，连续两行是同一个进程执行环境，因此前一个命令修改的内存状态，会直接影响后一个命令；而在 Dockerfile 中，这两行 RUN 命令的执行环境根本不同，是两个完全不同的镜像层。这就是对 Dockerfile 构建分层存储的概念不了解所导致的错误。注意：每一个 指令 都是启动一个容器、执行命令、然后提交存储层文件变更因此如果需要改变以后各层的工作目录的位置，那么应该使用 WORKDIR 指令。 USER 指定当前用户格式：USER &lt;用户名&gt;USER 指令和 WORKDIR 相似，都是改变环境状态并影响以后的层。WORKDIR 是改变工作目录，USER 则是改变之后层的执行身份,这个用户必须是事先建立好的，否则无法切换。123RUN groupadd -r redis &amp;&amp; useradd -r -g redis redisUSER redisRUN [ "redis-server" ] shell用户身份切换可以使用su,sudo,或者去下载gosu HEALTHCHECK 健康检查 HEALTHCHECK [选项] CMD &lt;命令&gt;：设置检查容器健康状况的命令 HEALTHCHECK NONE：如果基础镜像有健康检查指令，使用这行可以屏蔽掉其健康检查指令 HEALTHCHECK 指令是告诉 Docker 应该如何进行判断容器的状态是否正常，这是 Docker 1.12 引入的新指令。在没有 HEALTHCHECK 指令前，Docker 引擎只可以通过容器内主进程是否退出来判断容器是否状态异常。当在一个镜像指定了 HEALTHCHECK 指令后，用其启动容器，初始状态会为 starting，在 HEALTHCHECK 指令检查成功后变为 healthy，如果连续一定次数失败，则会变为 unhealthy。 HEALTHCHECK 支持下列选项：–interval=&lt;间隔&gt;：两次健康检查的间隔，默认为 30 秒；–timeout=&lt;时长&gt;：健康检查命令运行超时时间，如果超过这个时间，本次健康检查就被视为失败，默认 30 秒；–retries=&lt;次数&gt;：当连续失败指定次数后，则将容器状态视为 unhealthy，默认 3 次。命令的返回值决定了该次健康检查的成功与否：0：成功；1：失败；2：保留.1234FROM nginxRUN apt-get update &amp;&amp; apt-get install -y curl &amp;&amp; rm -rf /var/lib/apt/lists/*HEALTHCHECK --interval=5s --timeout=3s \ CMD curl -fs http://localhost/ || exit 1 构建启动后可以通过docker ps查看健康状态，也可以使用docker inspect查看详情及健康检查命令的终端输出。12345678910111213$ docker inspect --format '&#123; &#123;json .State.Health&#125; &#125;' web | python -m json.tool&#123; "FailingStreak": 0, "Log": [ &#123; "End": "2016-11-25T14:35:37.940957051Z", "ExitCode": 0, "Output": "&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n", "Start": "2016-11-25T14:35:37.780192565Z" &#125; ], "Status": "healthy"&#125; ONBUILD 为他人做嫁衣裳格式：ONBUILD &lt;其它指令&gt;。ONBUILD 是一个特殊的指令，它后面跟的是其它指令，比如 RUN, COPY 等，而这些指令，在当前镜像构建时并不会被执行。只有当以当前镜像为基础镜像，去构建下一级镜像的时候才会被执行。Dockerfile 中的其它指令都是为了定制当前镜像而准备的，唯有 ONBUILD 是为了帮助别人定制自己而准备的。比较适用于做基础镜像的时候，通用化模板流程。123456789FROM node:slimRUN mkdir /appWORKDIR /app#以下三步是每个子镜像都需要执行的步骤，此时我们将其抽取成模板，定义在基础镜像中，具体执行在子镜像构建时执行#抽象上类似于maven的dependency manager与dependencies的区别ONBUILD COPY ./package.json /appONBUILD RUN [ "npm", "install" ]ONBUILD COPY . /app/CMD [ "npm", "start" ] 多阶段构建在 Docker 17.05 版本之前，我们构建 Docker 镜像时，通常会采用两种方式：1、全部放入一个 Dockerfile：缺点: Dockerfile 特别长，可维护性降低 镜像层次多，镜像体积较大，部署时间变长 源代码存在泄露的风险 2、分散到多个 Dockerfile，缺点：部署过程较复杂，需要管理多个dockerfile，同时需要一个整合的shell脚本 现在的方式：使用多阶段构建Docker v17.05 开始支持多阶段构建 (multistage builds).使用多阶段构建我们就可以很容易解决前面提到的问题，并且只需要编写一个 Dockerfile：1234567891011121314#编译阶段FROM golang:1.9-alpineRUN apk --no-cache add gitWORKDIR /go/src/github.com/go/helloworld/RUN go get -d -v github.com/go-sql-driver/mysqlCOPY app.go .RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .#最终需要构建的镜像FROM alpine:latestRUN apk --no-cache add ca-certificatesWORKDIR /root/#注意此处的--from=0,代指上一步(即前面编译阶段)的镜像COPY --from=0 /go/src/github.com/go/helloworld/app . CMD ["./app"] 其它制作镜像的方式除了标准的使用 Dockerfile 生成镜像的方法外，由于各种特殊需求和历史原因，还提供了一些其它方法用以生成镜像。 从 rootfs 压缩包导入：docker import [选项] &lt;文件&gt;|&lt;URL&gt;|- [&lt;仓库名&gt;[:&lt;标签&gt;]]123$ docker import \ http://download.openvz.org/template/precreated/ubuntu-14.04-x86_64-minimal.tar.gz \ openvz/ubuntu:14.04 docker save 和 docker load:用以将镜像保存为一个 tar 文件，然后传输到另一个位置上，再加载进来。这是在没有 Docker Registry 时的做法，现在已经不推荐，镜像迁移应该直接使用 Docker Registry.123$ docker save alpine | gzip &gt; alpine-latest.tar.gz$ docker load -i alpine-latest.tar.gzdocker save &lt;镜像名&gt; | bzip2 | pv | ssh &lt;用户名&gt;@&lt;主机名&gt; &apos;cat | docker load&apos; 操作Docker容器简单的说，容器是独立运行的一个或一组应用，以及它们的运行态环境。对应的，虚拟机可以理解为模拟运行的一整套操作系统（提供了运行态环境和其他系统环境）和跑在上面的应用。 启动容器：启动容器有两种方式，一种是基于镜像新建一个容器并启动，另外一个是将在终止状态（stopped）的容器重新启动。1234567891011121314$ docker run ubuntu:14.04 /bin/echo 'Hello world'$ docker run -t -i ubuntu:14.04 /bin/bash#启动已终止的容器$ docker container start container your_container_id#后台运行$ docker run -d ubuntu:17.10 /bin/sh -c "while true; do echo hello world; sleep 1; done"#获取容器终端输出$ docker container logs [container ID or NAMES]#终止容器docker container stop container your_container_iddocker container ls -adocker ps -adocker container restart container your_container_id 当利用 docker run 来创建容器时，Docker 在后台运行的标准操作包括： 检查本地是否存在指定的镜像，不存在就从公有仓库下载 利用镜像创建并启动一个容器 分配一个文件系统，并在只读的镜像层外面挂载一层可读写层 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去 从地址池配置一个 ip 地址给容器 执行用户指定的应用程序 执行完毕后容器被终止 进入容器:使用 docker attach 命令或 docker exec 命令，推荐大家使用 docker exec 命令,因为attach会在shell exit时使容器终止，即使仍然有前台进程在运行。1$ docker exec -it 69d1 bash 导出和导入容器：1234$ docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES7691a814370e ubuntu:14.04 &quot;/bin/bash&quot; 36 hours ago Exited (0) 21 hours ago test$ docker export 7691a814370e &gt; ubuntu.tar 12$ cat ubuntu.tar | docker import - test/ubuntu:v1.0$ docker import http://example.com/exampleimage.tgz example/imagerepo 注：用户既可以使用 docker load 来导入镜像存储文件到本地镜像库，也可以使用 docker import 来导入一个容器快照到本地镜像库。这两者的区别在于容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积也要大。此外，从容器快照文件导入时可以重新指定标签等元数据信息。 删除容器:可以使用 docker container rm 来删除一个处于终止状态的容器。例如1$ docker container rm trusting_newton 如果要删除一个运行中的容器，可以添加 -f 参数。Docker 会发送 SIGKILL 信号给容器。 清理所有处于终止状态的容器：用 docker container ls -a 命令可以查看所有已经创建的包括终止状态的容器，如果数量太多要一个个删除可能会很麻烦，用下面的命令可以清理掉所有处于终止状态的容器。1$ docker container prune 访问仓库仓库（Repository）是集中存放镜像的地方。12$ docker pull ubuntu:16.04$ docker search centos 另外，在查找的时候通过 –filter=stars=N 参数可以指定仅显示收藏数量为 N 以上的镜像注意，Docker 1.12 版本中已经不支持 –stars 参数，则可以使用 -f stars=N 参数。如：docker search --filter=stars=3000 ubuntu 推送镜像:用户也可以在登录后通过 docker push 命令来将自己的镜像推送到 Docker Hub。以下命令中的 username 请替换为你的 Docker 账号用户名。123$ docker tag ubuntu:17.10 username/ubuntu:17.10$ docker push username/ubuntu:17.10$ docker search username 自动创建:自动创建（Automated Builds）功能对于需要经常升级镜像内程序来说，十分方便。有时候，用户创建了镜像，安装了某个软件，如果软件发布新版本则需要手动更新镜像。而自动创建允许用户通过 Docker Hub 指定跟踪一个目标网站（目前支持 GitHub 或 BitBucket）上的项目，一旦项目发生新的提交或者创建新的标签（tag），Docker Hub 会自动构建镜像并推送到 Docker Hub 中.具体步骤：略。 私有仓库docker-registry 是官方提供的工具，可以用于构建私有的镜像仓库1234$ docker run -d \ -p 5000:5000 \ -v /opt/data/registry:/var/lib/registry \ registry 在私有仓库上传、搜索、下载镜像创建好私有仓库之后，就可以使用 docker tag 来标记一个镜像，然后推送它到仓库。格式为 docker tag IMAGE[:TAG] [REGISTRY_HOST[:REGISTRY_PORT]/]REPOSITORY[:TAG]1234$ docker tag ubuntu:latest 127.0.0.1:5000/ubuntu:latest$ docker push 127.0.0.1:5000/ubuntu:latest#用 curl 查看仓库中的镜像$ curl 127.0.0.1:5000/v2/_catalog 注意： Docker 默认不允许非 HTTPS 方式推送镜像。我们可以通过 Docker 的配置选项来取消这个限制对于使用 systemd 的系统，请在/etc/docker/daemon.json 中写入如下内容（如果文件不存在请新建该文件）12345678&#123; &quot;registry-mirror&quot;: [ &quot;https://registry.docker-cn.com&quot; ], &quot;insecure-registries&quot;: [ &quot;192.168.199.100:5000&quot; ]&#125; 使用 Docker Compose搭建一个拥有权限认证、TLS 的私有仓库略 Docker 数据管理：卷与挂载在容器中管理数据主要有两种方式： 数据卷（Volumes） 挂载主机目录 (Bind mounts) 数据卷：选择-v 还是 -–mount 参数：推荐使用 –mount 参数。 12345678910111213$ docker volume create my-vol$ docker volume ls$ docker volume inspect my-vol$ docker run -d -P \ --name web \ # -v my-vol:/wepapp \ --mount source=my-vol,target=/webapp \ training/webapp \ python app.py$ docker inspect web$ docker volume rm my-vol 数据卷 是被设计用来持久化数据的，它的生命周期独立于容器,如果需要在删除容器的同时移除数据卷。可以在删除容器的时候使用 docker rm -v这个命令。无主的数据卷可能会占据很多空间，要清理请使用以下命令1$ docker volume prune 挂载主机目录：1234567891011121314$ docker run -d -P \ --name web \ # -v /src/webapp:/opt/webapp \ --mount type=bind,source=/src/webapp,target=/opt/webapp \ training/webapp \ python app.py#只读$ docker run -d -P \ --name web \ # -v /src/webapp:/opt/webapp:ro \ --mount type=bind,source=/src/webapp,target=/opt/webapp,readonly \ training/webapp \ python app.py 使用网络容器中可以运行一些网络应用，要让外部也可以访问这些应用，可以通过-P 或 -p 参数来指定端口映射。当使用 -P 标记时，Docker 会随机映射一个 49000~49900 的端口到内部容器开放的网络端口。123456789$ docker run -d -P training/webapp python app.py$ docker run -d -p 5000:5000 training/webapp python app.py$ docker run -d -p 127.0.0.1:5000:5000 training/webapp python app.py$ docker run -d -p 127.0.0.1:5000:5000/udp training/webapp python app.py$ docker run -d \ -p 5000:5000 \ -p 3000:80 \ training/webapp \ python app.py 容器互联：如果你之前有 Docker 使用经验，你可能已经习惯了使用 –link 参数来使容器互联。随着 Docker 网络的完善，建议大家将容器加入自定义的 Docker 网络来连接多个容器，而不是使用 –link 参数。 新建网络1$ docker network create -d bridge my-net -d 参数指定 Docker 网络类型，有 bridge overlay。其中 overlay 网络类型用于 Swarm mode。 连接容器12345678#运行一个容器并连接到新建的 my-net 网络$ docker run -it --rm --name busybox1 --network my-net busybox sh#打开新的终端，再运行一个容器并加入到 my-net 网络$ docker run -it --rm --name busybox2 --network my-net busybox sh#下面通过 ping 来证明 busybox1 容器和 busybox2 容器建立了互联关系。#在 busybox1 容器输入以下命令/ # ping busybox2 如果你有多个容器之间需要互相连接，推荐使用 Docker Compose。 配置 DNS：略 高级网络配置当 Docker 启动时，会自动在主机上创建一个 docker0 虚拟网桥，实际上是 Linux 的一个 bridge，可以理解为一个软件交换机。它会在挂载到它的网口之间进行转发。同时，Docker 随机分配一个本地未占用的私有网段中的一个地址给 docker0 接口。比如典型的 172.17.42.1，掩码为 255.255.0.0。此后启动的容器内的网口也会自动分配一个同一网段（172.17.0.0/16）的地址。 当创建一个 Docker 容器的时候，同时会创建了一对 veth pair 接口（当数据包发送到一个接口时，另外一个接口也可以收到相同的数据包）。这对接口一端在容器内，即 eth0；另一端在本地并被挂载到 docker0 网桥，名称以 veth 开头（例如 vethAQI2QT）。通过这种方式，主机可以跟容器通信，容器之间也可以相互通信。Docker 就创建了在主机和所有容器之间一个虚拟共享网络。 容器的访问控制，主要通过 Linux 上的 iptables 防火墙来进行管理和实现。容器要想访问外部网络，需要本地系统的转发支持。在Linux 系统中，检查转发是否打开。1234$sysctl net.ipv4.ip_forwardnet.ipv4.ip_forward = 1#如果为 0，说明没有开启转发，则需要手动打开。$sysctl -w net.ipv4.ip_forward=1 容器之间相互访问，需要两方面的支持。 容器的网络拓扑是否已经互联。默认情况下，所有容器都会被连接到 docker0网桥上。 本地系统的防火墙软件 iptables 是否允许通过。 当启动 Docker 服务时候，默认会添加一条转发策略到 iptables 的 FORWARD 链上。策略为通过（ACCEPT）还是禁止（DROP）取决于配置–icc=true（缺省值）还是 –icc=false 默认情况下，不同容器之间是允许网络互通的。如果为了安全考虑，可以在 /etc/default/docker 文件中配置 DOCKER_OPTS=--icc=false 来禁止它 自定义网络拓扑配置工具:pipework、playground 更复杂的请看书籍，此处略。 Docker ComposeDocker Compose 是 Docker 官方编排（Orchestration）项目之一，负责快速在集群中部署分布式应用。Compose允许用户通过一个单独的 docker-compose.yml 模板文件来定义一组相关联的应用容器为一个项目（project）。Compose 中有两个重要的概念： 服务 (service)：一个应用的容器，实际上可以包括若干运行相同镜像的容器实例。 项目 (project)：由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 Compose 的默认管理对象是项目，通过子命令对项目中的一组容器进行便捷地生命周期管理。 安装与卸载:略1$ docker-compose --version 场景：下面我们用 Python 来建立一个能够记录页面访问次数的 web 网站。12345678910111213from flask import Flaskfrom redis import Redisapp = Flask(__name__)redis = Redis(host='redis', port=6379)@app.route('/')def hello(): count = redis.incr('hits') return 'Hello World! 该页面已被访问 &#123;&#125; 次。\n'.format(count)if __name__ == "__main__": app.run(host="0.0.0.0", debug=True) 12345FROM python:3.6-alpineADD . /codeWORKDIR /codeRUN pip install redis flaskCMD ["python", "app.py"] docker-compose.yml:12345678910version: '3'services: web: build: . ports: - "5000:5000" redis: image: "redis:alpine" 运行 compose 项目1$ docker-compose up compose命令说明命令对象与格式：大部分命令的对象既可以是项目本身，也可以指定为项目中的服务或者容器。如果没有特别的说明，命令对象将是项目，这意味着项目中所有的服务都会受到命令影响docker-compose [-f=&lt; arg&gt;...] [options] [COMMAND] [ARGS...]选项：-f, –file FILE 指定使用的 Compose 模板文件，默认为 docker-compose.yml，可以多次指定。-p, –project-name NAME 指定项目名称，默认将使用所在目录名称作为项目名。–x-networking 使用 Docker 的可拔插网络后端特性–x-network-driver DRIVER 指定网络后端的驱动，默认为 bridge–verbose 输出更多调试信息。-v, –version 打印版本并退出。子命令： build 构建（重新构建）项目中的服务容器。 config 验证 Compose 文件格式是否正确 down 此命令将会停止 up 命令所启动的容器，并移除网络 exec 进入指定的容器。 help images 列出 Compose 文件中包含的镜像。 kill 通过发送 SIGKILL 信号来强制停止服务容器。 logs 查看服务容器的输出。 pause 暂停一个服务容器。 port 打印某个容器端口所映射的公共端口。 ps 列出项目中目前的所有容器。 pull 拉取服务依赖的镜像。 push 推送服务依赖的镜像到 Docker 镜像仓库 restart 重启项目中的服务。 rm 删除所有（停止状态的）服务容器。推荐先执行 docker-compose stop 命令来停止容器。 run 在指定服务上执行一个命令。 scale 设置指定服务运行的容器个数。$ docker-compose scale web=3 db=2 start 启动已经存在的服务容器。 stop 停止已经处于运行状态的容器，但不删除它。 top 查看各个服务容器内运行的进程。 unpause 恢复处于暂停状态中的服务。 up 自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。链接的服务都将会被自动启动，除非已经处于运行状态。默认情况，docker-compose up 启动的容器都在前台.如果使用 docker-compose up -d，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。 version compose模板语法说明这里面大部分指令跟 docker run 相关参数的含义都是类似的。注意每个服务都必须通过 image 指令指定镜像或 build 指令（需要 Dockerfile）等来自动构建生成镜像。如果使用 build 指令，在 Dockerfile 中设置的选项(例如：CMD, EXPOSE, VOLUME, ENV 等) 将会自动被获取，无需在 docker-compose.yml 中再次设置。 build指定 Dockerfile 所在文件夹的路径（可以是绝对路径，或者相对 docker-compose.yml 文件的路径）你也可以使用 context 指令指定 Dockerfile 所在文件夹的路径,使用 dockerfile 指令指定 Dockerfile 文件名。使用 arg 指令指定构建镜像时的变量 12345678910version: '3'services: webapp: #build: ./dir build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 capadd, capdrop 指定容器的内核能力（capacity）分配。 command 覆盖容器启动后默认执行的命令command: echo &quot;hello world&quot; configs 仅用于 Swarm mode cgroup_parent 指定父 cgroup 组，意味着将继承该组的资源限制。 containername 指定容器名称。默认将会使用 `项目名称服务名称_序号` .注意: 指定容器名称后，该服务将无法进行扩展（scale） deploy 仅用于 Swarm mode devices 指定设备映射关系。 devices: &quot;/dev/ttyUSB1:/dev/ttyUSB0&quot; depends_on 解决容器的依赖、启动先后的问题 123456services: web: build: . depends_on: - db - redis dns 自定义 DNS 服务器。可以是一个值，也可以是一个列表。 dns_search 配置 DNS 搜索域。 tmpfs 挂载一个 tmpfs 文件系统到容器。 env_file 从文件中获取环境变量，可以为单独的文件路径或列表。 environment 设置环境变量。你可以使用数组或字典两种格式。 expose 暴露端口，但不映射到宿主机，只被连接的服务访问。 external_links 链接到 docker-compose.yml 外部的容器，甚至并非 Compose 管理的外部容器。 extra_hosts 类似 Docker 中的 –add-host 参数，指定额外的 host 名称映射信息。 healthcheck 通过命令检查容器是否健康运行。 12345healthcheck: test: [&quot;CMD&quot;, &quot;curl&quot;, &quot;-f&quot;, &quot;http://localhost&quot;] interval: 1m30s timeout: 10s retries: 3 image 指定为镜像名称或镜像 ID。如果镜像在本地不存在，Compose 将会尝试拉去这个镜像。 labels 为容器添加 Docker 元数据（metadata）信息 links 不推荐使用该指令。 logging 配置日志选项。 network_mode 设置网络模式。使用和 docker run 的 –network 参数一样的值。 networks 配置容器连接的网络。 pid 跟主机系统共享进程命名空间。打开该选项的容器之间，以及容器和宿主机系统之间可以通过进程 ID 来相互访问和操作。pid: &quot;host&quot; ports 暴露端口信息。 使用宿主端口：容器端口 (HOST:CONTAINER) 格式 12345ports: - &quot;3000&quot; - &quot;8000:8000&quot; - &quot;49100:22&quot; - &quot;127.0.0.1:8001:8001&quot; secrets 存储敏感数据，例如 mysql 服务密码。 security_opt 指定容器模板标签（label）机制的默认属性（用户、角色、类型、级别等） stop_signal 设置另一个信号来停止容器 sysctls 配置容器内核参数。 ulimits 指定容器的 ulimits 限制值。 volumes 数据卷所挂载路径设置。可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）。 1234volumes: - /var/lib/mysql - cache/:/tmp/cache - ~/configs:/etc/configs/:ro 其它指令 包括 domainname, entrypoint, hostname, ipc, mac_address, privileged, read_only, shm_size, restart, stdin_open, tty, user, working_dir 等指令，基本跟 docker run 中对应参数的功能一致。 读取变量 Compose 模板文件支持动态读取主机的系统环境变量和当前目录下的 .env 文件中的变量。12db: image: &quot;mongo:$&#123;MONGO_VERSION&#125;&quot; 实战Django1234567FROM python:3ENV PYTHONUNBUFFERED 1RUN mkdir /codeWORKDIR /codeADD requirements.txt /code/RUN pip install -r requirements.txtADD . /code/ docker-compose.yml 文件将把所有的东西关联起来。它描述了应用的构成（一个 web 服务和一个数据库）、使用的 Docker 镜像、镜像之间的连接、挂载到容器的卷，以及服务开放的端口123456789101112131415version: "3"services: db: image: postgres web: build: . command: python3 manage.py runserver 0.0.0.0:8000 volumes: - .:/code ports: - "8000:8000" links: - db Docker MachineDocker Machine 是 Docker 官方编排（Orchestration）项目之一，负责在多种平台上快速安装 Docker 环境。1$ docker-machine -v Docker Machine 支持多种后端驱动. 创建本地主机实例1、Virtualbox 驱动(win10不支持该类型)12345678#使用 virtualbox 类型的驱动，创建一台 Docker 主机，命名为 test。$ docker-machine create -d virtualbox test$ docker-machine ls#创建主机成功后，可以通过 env 命令来让后续操作对象都是目标主机。$ docker-machine env test#也可以通过 SSH 登录到主机。$ docker-machine ssh test 你也可以在创建时加上如下参数，来配置主机或者主机上的 Docker。–engine-opt dns=114.114.114.114 配置 Docker 的默认 DNS–engine-registry-mirror https://registry.docker-cn.com 配置 Docker 的仓库镜像–virtualbox-memory 2048 配置主机内存–virtualbox-cpu-count 2 配置主机 CPU更多参数请使用 docker-machine create –driver virtualbox –help 命令查看。 2、macOS xhyve 驱动3、Windows 10 hyperv 驱动4、官方支持是10多种驱动 操作命令： active 查看活跃的 Docker 主机 config 输出连接的配置信息 create 创建一个 Docker 主机 env 显示连接到某个主机需要的环境变量 inspect 输出主机更多信息 ip 获取主机地址 kill 停止某个主机 ls 列出所有管理的主机 provision 重新设置一个已存在的主机 regenerate-certs 为某个主机重新生成 TLS 认证信息 restart 重启主机 rm 删除某台主机 ssh SSH 到主机上执行命令 scp 在主机之间复制文件 mount 挂载主机目录到本地 start 启动一个主机 status 查看主机状态 stop 停止一个主机 upgrade 更新主机 Docker 版本为最新 url 获取主机的 URL version 输出 docker-machine 版本信息 help 输出帮助信息 Docker Swarm / Swarm modeDocker Swarm 是 Docker 官方三剑客项目之一，提供 Docker 容器集群管理和编排工具，是 Docker 官方对容器云生态进行支持的核心方案。使用它，用户可以将多个 Docker 主机封装为单个大型的虚拟 Docker 主机，快速打造一套容器云平台。注意：Docker 1.12.0+ Swarm mode 已经内嵌入 Docker 引擎，成为了 docker 子命令docker swarm. Swarm mode 内置 kv 存储功能，提供了众多的新特性，比如：具有容错能力的去中心化设计、内置服务发现、负载均衡、路由网格、动态伸缩、滚动更新、安全传输等。使得 Docker 原生的 Swarm 集群具备与 Mesos、Kubernetes 竞争的实力。 几个概念：节点:分为管理 (manager) 节点和工作 (worker) 节点.docker swarm 命令基本只能在管理节点执行（节点退出集群命令 docker swarm leave 可以在工作节点执行）。只有一个管理节点可以成为 leader，leader 通过 raft 协议实现。工作节点是任务执行节点，管理节点将服务 (service) 下发至工作节点执行。管理节点默认也作为工作节点。 任务 （Task）是 Swarm 中的最小的调度单位，目前来说就是一个单一的容器。服务 （Services） 是指一组任务的集合，服务定义了任务的属性。服务有两种模式： replicated services 按照一定规则在各个工作节点上运行指定个数的任务。 global services 每个工作节点上运行一个任务两种模式通过 docker service create 的 --mode 参数指定。 初始化集群我们使用 docker swarm init 在本机初始化一个 Swarm 集群。1$ docker swarm init --advertise-addr 192.168.99.100 如果你的 Docker 主机有多个网卡，拥有多个 IP，必须使用 --advertise-addr 指定 IP。执行 docker swarm init 命令的节点自动成为管理节点。 增加工作节点123456789$ docker-machine create -d virtualbox worker1$ docker-machine ssh worker1docker@worker1:~$ docker swarm join \ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \ 192.168.99.100:2377This node joined a swarm as a worker. 类似地可以创建很多个。 查看集群：在管理节点使用 docker node ls 查看集群。12345$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS03g1y59jwfg7cf99w4lt0f662 worker2 Ready Active9j68exjopxe7wfl6yuxml7a7j worker1 Ready Activedxn1zf6l61qsb1josjja83ngz * manager1 Ready Active Leader 部署服务使用 docker service 命令来管理 Swarm 集群中的服务，该命令只能在管理节点运行12345678910111213141516$ docker service create --replicas 3 -p 80:80 --name nginx nginx:1.13.7-alpine#现在我们使用浏览器，输入任意节点 IP ,即可看到 nginx 默认页面。#查看当前 Swarm 集群运行的服务。$ docker service lsID NAME MODE REPLICAS IMAGE PORTSkc57xffvhul5 nginx replicated 3/3 nginx:1.13.7-alpine *:80-&gt;80/tcp#查看某个服务的详情。在各节点分布状况$ docker service ps nginx#查看某个服务的日志$ docker service logs nginx#从 Swarm 集群移除某个服务$ docker service rm nginx 在 Swarm 集群中使用 compose 文件我们使用 docker service create 一次只能部署一个服务，使用 docker-compose.yml 我们可以一次启动多个关联的服务1234567891011121314151617181920212223242526272829303132333435363738394041424344454647version: "3"services: wordpress: image: wordpress ports: - 80:80 networks: - overlay environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_USER: wordpress WORDPRESS_DB_PASSWORD: wordpress deploy: mode: replicated replicas: 3 db: image: mysql networks: - overlay volumes: - db-data:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: somewordpress MYSQL_DATABASE: wordpress MYSQL_USER: wordpress MYSQL_PASSWORD: wordpress deploy: placement: constraints: [node.role == manager] visualizer: image: dockersamples/visualizer:stable ports: - "8080:8080" stop_grace_period: 1m30s volumes: - "/var/run/docker.sock:/var/run/docker.sock" deploy: placement: constraints: [node.role == manager]volumes: db-data:networks: overlay: 在 Swarm 集群管理节点新建该文件，其中的 visualizer 服务提供一个可视化页面，我们可以从浏览器中很直观的查看集群中各个服务的运行节点。部署服务使用 docker stack deploy，其中 -c 参数指定 compose 文件名。1$ docker stack deploy -c docker-compose.yml wordpress 现在我们打开浏览器输入 任一节点IP:8080 即可看到各节点运行状态。123456#查看服务$ docker stack ls#移除服务,该命令不会移除服务所使用的 数据卷，如果你想移除数据卷请使用 docker volume rm$ docker stack down wordpress$ docker volume rm db-data 管理敏感数据管理和分发 密码、证书 等敏感信息.我们可以用 docker secret 命令来管理敏感信息。我们使用 docker secret create 命令以管道符的形式创建 secret123456$ openssl rand -base64 20 | docker secret create mysql_password -$ openssl rand -base64 20 | docker secret create mysql_root_password -#查看$ docker secret ls 1234567891011121314$ docker network create -d overlay mysql_private$ docker service create \ --name mysql \ --replicas 1 \ --network mysql_private \ --mount type=volume,source=mydata,destination=/var/lib/mysql \ --secret source=mysql_root_password,target=mysql_root_password \ --secret source=mysql_password,target=mysql_password \ -e MYSQL_ROOT_PASSWORD_FILE="/run/secrets/mysql_root_password" \ -e MYSQL_PASSWORD_FILE="/run/secrets/mysql_password" \ -e MYSQL_USER="wordpress" \ -e MYSQL_DATABASE="wordpress" \ mysql:latest 当然也可以通过compose文件来引入secrets 管理配置数据在 Docker 17.06 以上版本中，Docker 新增了 docker config 子命令来管理集群中的配置信息，以后你无需将配置文件放入镜像或挂载到容器中就可实现对服务的配置。我们使用 docker config create 命令创建 config12345678910$ docker config create redis.conf redis.conf$ docker config ls$ docker service create \ --name redis \ # --config source=redis.conf,target=/etc/redis.conf \ --config redis.conf \ -p 6379:6380 \ redis:latest \ redis-server /redis.conf 如果你没有在 target 中显式的指定路径时，默认的 redis.conf 以 tmpfs 文件系统挂载到容器的 /config.conf 用 docker config 来管理服务的配置信息，我们只需在集群中的管理节点创建 config，当部署服务时，集群会自动的将配置文件分发到运行服务的各个节点中. 安全评估 Docker 的安全性时，主要考虑三个方面: 由内核的命名空间和控制组机制提供的容器内在安全 Docker 程序（特别是服务端）本身的抗攻击性 内核安全性的加强机制对容器安全性的影响 内核命名空间：当用 docker run 启动一个容器时，在后台 Docker 为容器创建了一个独立的命名空间和控制组集合。命名空间提供了最基础也是最直接的隔离，在容器中运行的进程不会被运行在主机上的进程和其它容器发现和作用。每个容器都有自己独有的网络栈，意味着它们不能访问其他容器的 sockets 或接口。只能通过docker0来桥接转发。从网络架构的角度来看，所有的容器通过本地主机的网桥接口相互通信，就像物理机器通过物理交换机通信一样。 控制组：控制组是 Linux 容器机制的另外一个关键组件，负责实现资源的审计和限制。特性:确保各个容器可以公平地分享主机的内存、CPU、磁盘 IO 等资源；确保当容器内的资源使用产生压力时不会连累主机系统。例如，当某些应用程序表现异常的时候，可以保证一致地正常运行和性能。 Docker服务端的防护运行一个容器或应用程序的核心是通过 Docker 服务端。Docker 服务的运行目前需要 root 权限.终极目标是改进 2 个重要的安全特性： 将容器的 root 用户映射到本地主机上的非 root 用户，减轻容器和主机之间因权限提升而引起的安全问题； 允许 Docker 服务端在非 root 权限下运行，利用安全可靠的子进程来代理执行需要特权权限的操作。这些子进程将只允许在限定范围内进行操作，例如仅仅负责虚拟网络设定或文件系统管理、配置操作等。 内核能力机制能力机制（Capability）是 Linux 内核一个强大的特性，可以提供细粒度的权限访问控制,既可以作用在进程上，也可以作用在文件上.为了加强安全，容器可以禁用一些没必要的权限。 完全禁止任何 mount 操作； 禁止直接访问本地主机的套接字； 禁止访问一些文件系统的操作，比如创建新的设备、修改文件属性等； 禁止模块加载。这样，就算攻击者在容器中取得了 root 权限，也不能获得本地主机的较高权限，能进行的破坏也有限。默认情况下，Docker采用白名单机制，禁用必需功能之外的其它权限。 除了能力机制之外，还可以利用一些现有的安全机制来增强使用 Docker 的安全性，例如 TOMOYO, AppArmor, SELinux, GRSEC 等。 Docker 当前默认只启用了能力机制。用户可以采用多种方案来加强 Docker 主机的安全 总体来看，Docker 容器还是十分安全的，特别是在容器内不使用 root 权限来运行进程的话。 底层实现Docker 底层的核心技术包括 Linux 上的命名空间（Namespaces）、控制组（Control groups）、Union 文件系统（Union file systems）和容器格式（Container format）。传统的虚拟机通过在宿主主机中运行 hypervisor 来模拟一整套完整的硬件环境提供给虚拟机的操作系统。在操作系统中，包括内核、文件系统、网络、PID、UID、IPC、内存、硬盘、CPU 等等，所有的资源都是应用进程直接共享的。 要想实现虚拟化，除了要实现对内存、CPU、网络IO、硬盘IO、存储空间等的限制外，还要实现文件系统、网络、PID、UID、IPC等等的相互隔离. 随着 Linux 系统对于命名空间功能的完善实现，已经可以实现上面的所有需求:让某些进程在彼此隔离的命名空间中运行。大家虽然都共用一个内核和某些运行时环境（例如一些系统命令和系统库），但是彼此却看不到，都以为系统中只有自己的存在。这种机制就是容器（Container），利用命名空间来做权限的隔离控制，利用 cgroups 来做资源分配。 Docker 采用了 C/S 架构，包括客户端和服务端。Docker 守护进程 （Daemon）作为服务端接受来自客户端的请求，并处理这些请求（创建、运行、分发容器）。客户端和服务端既可以运行在一个机器上，也可通过 socket 或者 RESTful API 来进行通信。 命名空间命名空间是 Linux 内核一个强大的特性。每个容器都有自己单独的命名空间，运行在其中的应用都像是在独立的操作系统中运行一样。命名空间保证了容器之间彼此互不影响。 pid 命名空间不同用户的进程就是通过 pid 命名空间隔离开的，且不同命名空间中可以有相同 pid。 net 命名空间网络隔离是通过 net 命名空间实现的， 每个 net 命名空间有独立的 网络设备, IP 地址, 路由表, /proc/net 目录。这样每个容器的网络就能隔离开来。Docker 默认采用 veth 的方式，将容器中的虚拟网卡同 host 上的一 个Docker 网桥 docker0 连接在一起。 ipc 命名空间容器中进程交互还是采用了 Linux 常见的进程间交互方法(interprocess communication -IPC), 包括信号量、消息队列和共享内存等。然而同 VM 不同的是，容器的进程间交互实际上还是 host 上具有相同 pid 命名空间中的进程间交互。 mnt 命名空间mnt 命名空间允许不同命名空间的进程看到的文件结构不同，这样每个命名空间 中的进程所看到的文件目录就被隔离开了。 uts 命名空间UTS(“UNIX Time-sharing System”) 命名空间允许每个容器拥有独立的 hostname 和 domain name, 使其在网络上可以被视作一个独立的节点而非 主机上的一个进程。 user 命名空间每个容器可以有不同的用户和组 id, 也就是说可以在容器内用容器内部的用户执行程序而非主机上的用户。 总结：在虚拟化的系统中，一台物理计算机可以运行多个内核，可能是并行的多个不同的操作系统。而命名空间则只使用一个内核在一台物理计算机上运作，前述的所有全局资源都通过命名空间抽象起来。这使得可以将一组进程放置到容器中，各个容器彼此隔离。隔离可以使容器的成员与其他容器毫无关系。但也可以通过允许容器进行一定的共享，来降低容器之间的分隔。本质上，命名空间建立了系统的不同视图。 注意：Linux系统对简单形式的命名空间的支持已经有很长一段时间了，主要是chroot系统调用。该方法可以将进程限制到文件系统的某一部分，因而是一种简单的命名空间机制。但真正的命名空间能够控制的功能远远超过文件系统视图。新的命名空间可以用下面两种方法创建。(1) 在用fork或clone系统调用创建新进程时，有特定的选项可以控制是与父进程共享命名空间，还是建立新的命名空间。(2) unshare系统调用将进程的某些部分从父进程分离，其中也包括命名空间。 控制组控制组（cgroups）是 Linux 内核的一个特性，主要用来对共享资源进行隔离、限制、审计等。避免当多个容器同时运行时的对系统资源的竞争。 UnionFS联合文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下.联合文件系统是 Docker 镜像的基础。镜像可以通过分层来进行继承.以前Docker 中使用的 AUFS（AnotherUnionFS）就是一种联合文件系统。在可能的情况下，推荐使用 overlay2 存储驱动，overlay2 是目前 Docker 默认的存储驱动，以前则是 aufs。 容器格式最初，Docker 采用了 LXC 中的容器格式。从 0.7 版本以后开始去除 LXC，转而使用自行开发的 libcontainer，从 1.11 开始，则进一步演进为使用 runC 和 containerd。 Docker 网络实现Docker 的网络实现其实就是利用了 Linux 上的网络命名空间和虚拟网络设备（特别是 veth pair） 基本原理:首先，要实现网络通信，机器需要至少一个网络接口（物理接口或虚拟接口）来收发数据包；此外，如果不同子网之间要进行通信，需要路由机制。 Docker 中的网络接口默认都是虚拟的接口。虚拟接口的优势之一是转发效率较高。 Linux 通过在内核中进行数据复制来实现虚拟接口之间的数据转发，发送接口的发送缓存中的数据包被直接复制到接收接口的接收缓存中。对于本地系统和容器内系统看来就像是一个正常的以太网卡，只是它不需要真正同外部网络设备通信，速度要快很多。 Docker 容器网络就利用了这项技术。它在本地主机和容器内分别创建一个虚拟接口，并让它们彼此连通（这样的一对接口叫做 veth pair）。 Docker 创建一个容器的时候，会执行如下操作： 创建一对虚拟接口，分别放到本地主机和新容器中； 本地主机一端桥接到默认的 docker0 或指定网桥上，并具有一个唯一的名字，如 veth65f9； 容器一端放到新容器中，并修改名字作为 eth0，这个接口只在容器的命名空间可见； 从网桥可用地址段中获取一个空闲地址分配给容器的 eth0，并配置默认路由到桥接网卡 veth65f9。完成这些之后，容器就可以使用 eth0 虚拟网卡来连接其他容器和其他网络。 可以在 docker run 的时候通过 --net 参数来指定容器的网络配置，有4个可选值： –net=bridge 这个是默认值，连接到默认的网桥。 –net=host 告诉 Docker 不要将容器网络放到隔离的命名空间中，即不要容器化容器内的网络。此时容器使用本地主机的网络，它拥有完全的本地主机接口访问权限。容器进程可以跟主机其它 root 进程一样可以打开低范围的端口，可以访问本地网络服务比如 D-bus，还可以让容器做一些影响整个主机系统的事情，比如重启主机。因此使用这个选项的时候要非常小心。如果进一步的使用--privileged=true，容器会被允许直接配置主机的网络堆栈。 –net=container:NAME_or_ID 让 Docker 将新建容器的进程放到一个已存在容器的网络栈中，新容器进程有自己的文件系统、进程列表和资源限制，但会和已存在的容器共享 IP 地址和端口等网络资源，两者进程可以直接通过 lo 环回接口通信。 –net=none 让 Docker 将新容器放到隔离的网络栈中，但是不进行网络配置。之后，用户可以自己进行配置。 etcd项目etcd 是 CoreOS 团队发起的一个管理配置信息和服务发现的项目etcd受到 Apache ZooKeeper 项目和 doozer 项目的启发，同时采用Raft一致性算法。,同时etcd还是一个kv存储数据库。 Apache ZooKeeper 是一套知名的分布式系统中进行同步和一致性管理的工具。 doozer 是一个一致性分布式数据库。 Raft 是一套通过选举主节点来实现分布式系统一致性的算法，相比于大名鼎鼎的 Paxos 算法，它的过程更容易被人理解.用户使用 etcd 可以在多个节点上启动多个实例，并添加它们为一个集群。同一个集群中的 etcd 实例将会保持彼此信息的一致性。 默认 2379 端口处理客户端的请求，2380 端口用于集群各成员间的通信. 其他略。K8s好像就采用了它。 CoreOSCoreOS 被设计成一个基于容器的最小化的现代操作系统。CoreOS 的设计是为你提供能够像谷歌一样的大型互联网公司一样的基础设施管理能力来动态扩展和管理的计算能力。CoreOS 的安装文件和运行依赖非常小,它提供了精简的 Linux 系统。CoreOS 对 Docker 甚至容器技术的发展都带来了巨大的推动作用。其提供了运行现代基础设施的特性，支持大规模服务部署，使得在基于最小化的现代操作系统上构建规模化的计算仓库成为了可能。 应用作为 Docker 容器运行在 CoreOS 上。容器以包的形式提供最大得灵活性并且可以在几毫秒启动。CoreOS 可以在一个机器上很好地运行，但是它被设计用来搭建集群。可以通过 k8s 很容易得使应用容器部署在多台机器上并且通过服务发现把他们连接在一起。CoreOS内置诸如分布式锁和主选举等原生工具用来构建大规模分布式系统得构建模块。同时提供类似于Zookeeper那样的服务发现及变更通知。 CoreOS 内置了 服务发现，容器管理 工具。第一个重要组件就是使用 etcd 来实现的服务发现。在 CoreOS 中 etcd 默认以 rkt 容器(由CoreOS自己开发的容器)方式运行。第二个组件就是 Docker，它用来运行你的代码和应用。CoreOS 内置 Docker. 快速搭建 CoreOS 集群:为了在你的电脑运行一个集群环境，我们使用 Vagrant.具体略。 KubernetesKubernetes 是 Google 团队发起并维护的基于 Docker 的开源容器集群管理系统，它不仅支持常见的云平台，而且支持内部数据中心。其核心概念是 Container Pod。一个 Pod 由一组工作于同一物理工作节点的容器构成。这些组容器拥有相同的网络命名空间、IP以及存储配额.Kubernetes 依赖 Etcd 服务来维护所有主节点的状态。 quickstart123456789#启动etcddocker run --net=host -d gcr.io/google_containers/etcd:2.0.9 /usr/local/bin/etcd --addr=127.0.0.1:4001 --bind-addr=0.0.0.0:4001 --data-dir=/var/etcd/data#启动kubeletdocker run --net=host -d -v /var/run/docker.sock:/var/run/docker.sock gcr.io/google_containers/hyperkube:v0.17.0 /hyperkube kubelet --api_servers=http://localhost:8080 --v=2 --address=0.0.0.0 --enable_server --hostname_override=127.0.0.1 --config=/etc/kubernetes/manifests#启动服务代理docker run -d --net=host --privileged gcr.io/google_containers/hyperkube:v0.17.0 /hyperkube proxy --master=http://127.0.0.1:8080 --v=2#测试状态$ curl 127.0.0.1:8080 所有服务启动后，查看本地实际运行的 Docker 容器，有如下几个。这些服务大概分为三类：主节点服务、工作节点服务和其它服务。 其它服务 Etcd 是所有状态的存储数据库； gcr.io/google_containers/pause:0.8.0 是 Kubernetes 启动后自动 pull 下来的测试镜像。 架构 节点（Node）：一个节点是一个运行 Kubernetes 中的主机。 容器组（Pod）：一个 Pod 对应于由若干容器组成的一个容器组，同个组内的容器共享一个存储卷(volume)。 容器组生命周期（pos-states）：包含所有容器状态集合，包括容器组状态类型，容器组生命周期，事件，重启策略，以及 replication controllers。 Replication Controllers：主要负责指定数量的 pod 在同一时间一起运行。 服务（services）：一个 Kubernetes 服务是容器组逻辑的高级抽象，同时也对外提供访问容器组的策略。 卷（volumes）：一个卷就是一个目录，容器对其有访问权限。 标签（labels）：标签是用来连接一组对象的，比如容器组。标签可以被用来组织和选择子对象。 主节点服务 apiserver 是整个系统的对外接口，提供 RESTful 方式供客户端和其它组件调用； scheduler 负责对资源进行调度，分配某个 pod 到某个节点上； controller-manager 负责管理控制器，包括 endpoint-controller（刷新服务和 pod 的关联信息）和 replication-controller（维护某个 pod 的复制为配置的数值）。 Etcd这里 Etcd 即作为数据后端，又作为消息中间件。通过 Etcd 来存储所有的主节点上的状态信息，很容易实现主节点的分布式扩展。组件可以自动的去侦测 Etcd 中的数值变化来获得通知，并且获得更新后的数据来执行相应的操作。 工作节点服务 kubelet 是工作节点执行操作的 agent，负责具体的容器生命周期管理，根据从数据库中获取的信息来管理容器，并上报 pod 运行状态等； proxy 为 pod 上的服务提供访问的代理， 是一个简单的网络访问代理，同时也是一个 Load Balancer。它负责将访问到某个服务的请求具体分配给工作节点上的 Pod（同一类标签） Mesos - 优秀的集群资源调度平台Mesos 可以将整个数据中心的资源（包括 CPU、内存、存储、网络等）进行抽象和调度，使得多个应用同时运行在集群中分享资源，并无需关心资源的物理分布情况。如果把数据中心中的集群资源看做一台服务器，那么 Mesos 要做的事情，其实就是今天操作系统内核的职责：抽象资源 + 调度任务.Mesos 拥有许多引人注目的特性，包括： 支持数万个节点的大规模场景（Apple、Twitter、eBay 等公司实践）； 支持多种应用框架，包括 Marathon、Singularity、Aurora 等； 支持 HA（基于 ZooKeeper 实现）； 支持 Docker、LXC 等容器机制进行任务隔离； 提供了多个流行语言的 API，包括 Python、Java、C++ 等； 自带了简洁易用的 WebUI，方便用户直接进行操作。 Mesos 安装与使用Mesos 也采用了经典的主-从结构，一般包括若干主节点和大量从节点。其中，mesos master 服务和 zookeeper 需要部署到所有的主节点，mesos slave 服务需要部署到所有从节点。 原理与架构首先，再次需要强调 Mesos 自身只是一个资源调度框架，并非一整套完整的应用管理平台，所以只有 Mesos 自己是不能干活的。但是基于 Mesos，可以比较容易地为各种应用管理框架或者中间件平台（作为 Mesos 的应用）提供分布式运行能力；同时多个框架也可以同时运行在一个 Mesos 集群中，提高整体的资源使用效率。 Mesos 中有三个基本的组件：管理服务（master）、任务服务（slave）以及应用框架（framework） 管理服务 - master负责不同应用框架之间的资源调度和逻辑控制。应用框架需要注册到管理服务上才能被使用。 任务服务 - slave负责汇报本从节点上的资源状态（空闲资源、运行状态等等）给主节点，并负责隔离本地资源来执行主节点分配的具体任务。隔离机制目前包括各种容器机制，包括 LXC、Docker 等。 应用框架 - framework应用框架是实际干活的，包括两个主要组件： 调度器（scheduler）：注册到主节点，等待分配资源； 执行器（executor）：在从节点上执行框架指定的任务（框架也可以使用 Mesos 自带的执行器，包括 shell 脚本执行器和 Docker 执行器）。应用框架可以分两种：一种是对资源的需求是会扩展的（比如 Hadoop、Spark 等），申请后还可能调整；一种是对资源需求大小是固定的（MPI 等），一次申请即可。 Mesos 为了实现尽量优化的资源调度，采取了两层（two-layer）的调度算法。 调度通过 offer发送的方式进行交互。一个 offer 是一组资源，例如 。基本调度过程如下： 首先，slave 节点会周期性汇报自己可用的资源给 master； 某个时候，master 收到应用框架发来的资源请求，根据调度策略，计算出来一个资源 offer 给 framework； framework 收到 offer 后可以决定要不要，如果接受的话，返回一个描述，说明自己希望如何使用和分配这些资源来运行某些任务（可以说明只希望使用部分资源，则多出来的会被 master 收回）； 最后，master 则根据 framework 答复的具体分配情况发送给 slave，以使用 framework 的 executor 来按照分配的资源策略执行任务。 其他：略 容器与云计算Docker 目前已经得到了众多公有云平台的支持，并成为除虚拟机之外的核心云业务。目前与容器相关的云计算主要分为两种类型。一种是传统的 IaaS 服务商提供对容器相关的服务，包括镜像下载、容器托管等。另一种是直接基于容器技术对外提供容器云服务，所谓 Container as a Service（CaaS）。 实战-操作系统1、BusyBox2、Alpine 操作系统是一个面向安全的轻型 Linux 发行版。它不同于通常 Linux 发行版，Alpine 采用了 musl libc 和 busybox 以减小系统的体积和运行时资源消耗，但功能上比 busybox 又完善的多，因此得到开源社区越来越多的青睐。在保持瘦身的同时，Alpine 还提供了自己的包管理工具 apk，可以通过 https://pkgs.alpinelinux.org/packages 网站上查询包信息，也可以直接通过 apk命令直接查询和安装各种软件。Alpine Docker 镜像仅仅只有 5 MB 左右（对比 Ubuntu 系列镜像接近 200 MB），且拥有非常友好的包管理机制。目前 Docker 官方已开始推荐使用 Alpine 替代之前的 Ubuntu 做为基础镜像环境。这样会带来多个好处。包括镜像下载速度加快，镜像安全性提高，主机之间的切换更方便，占用更少磁盘空间等。 迁移至 Alpine 基础镜像如果使用 Alpine 镜像替换 Ubuntu 基础镜像，安装软件包时需要用 apk 包管理器替换 apt 工具，如1$ apk add --no-cache &lt;package&gt; Alpine 中软件安装包的名字可能会与其他发行版有所不同，可以在 https://pkgs.alpinelinux.org/packages 网站搜索并确定安装包名称。如果需要的安装包不在主索引内，但是在测试或社区索引中。那么可以按照以下方法使用这些安装包。12$ echo &quot;http://dl-4.alpinelinux.org/alpine/edge/testing&quot; &gt;&gt; /etc/apk/repositories$ apk --update add --no-cache &lt;package&gt; 实战-CI/CD持续集成(Continuous integration)是一种软件开发实践，每次集成都通过自动化的构建（包括编译，发布，自动化测试）来验证，从而尽早地发现集成错误。 持续部署（continuous deployment）是通过自动化的构建、测试和部署循环来快速交付高质量的产品。 与 Jenkins 不同的是，基于 Docker 的 CI/CD 每一步都运行在 Docker 镜像中，所以理论上支持所有的编程语言。 Drone:基于 Docker 的 CI/CD 工具开发者只需在项目中包含 .drone.yml 文件，将代码推送到 git 仓库，Drone 就能够自动化的进行编译、测试、发布。 我们通过使用 Docker Compose 来启动 Drone，编写 docker-compose.yml 文件。12345678910111213141516171819202122232425262728293031323334353637version: '3'services: drone-server: image: drone/drone:0.8-alpine ports: - 443:443 # - "$&#123;PRO_PUBLIC_IP&#125;:8000:8000" volumes: - drone-data:/var/lib/drone/:rw - $&#123;SSL_PATH&#125;:/etc/certs:rw restart: always environment: - DRONE_SECRET=drone - DRONE_OPEN=false - DRONE_ADMIN=$&#123;GITHUB_SERNAME&#125; - DRONE_HOST=$&#123;DRONE_HOST&#125; - DRONE_GITHUB=true - DRONE_GITHUB_CLIENT=$&#123;DRONE_GITHUB_CLIENT&#125; - DRONE_GITHUB_SECRET=$&#123;DRONE_GITHUB_SECRET&#125; - DRONE_SERVER_CERT=/etc/certs/drone.domain.com.crt - DRONE_SERVER_KEY=/etc/certs/drone.domain.com.key drone-agent: image: drone/agent:0.8-alpine restart: always depends_on: - drone-server volumes: - /var/run/docker.sock:/var/run/docker.sock:rw environment: - DRONE_SECRET=drone - DRONE_SERVER=drone-server:9000 dns: 114.114.114.114volumes: drone-data: 启动 Drone1$ docker-compose up -d 后续步骤：新建仓库，关联，提交包含.drone.yml 文件的项目，在Drone管理界面中查看项目构建过程及结果。 附录常见问题筛选构建 Docker 镜像应该遵循哪些原则？ 答：整体原则上，尽量保持镜像功能的明确和内容的精简，要点包括 尽量选取满足需求但较小的基础系统镜像，例如大部分时候可以选择 debian:wheezy 或 debian:jessie 镜像，仅有不足百兆大小； 清理编译生成文件、安装包的缓存等临时文件； 安装各个软件时候要指定准确的版本号，并避免引入不需要的依赖； 从安全角度考虑，应用要尽量使用系统的库和依赖； 如果安装应用时候需要配置一些特殊的环境变量，在安装后要还原不需要保持的变量值； 使用 Dockerfile 创建镜像时候要添加 .dockerignore 文件或使用干净的工作目录。 如何停止所有正在运行的容器？答：可以使用 docker kill $(docker ps -q) 命令。 如何获取某个容器的 PID 信息？答：可以使用 docker inspect --format &#39;{ { .State.Pid } }&#39; &lt;CONTAINER ID or NAME&gt;命令。 如何获取某个容器的 IP 地址？答：可以使用 docker inspect --format &#39;{ { .NetworkSettings.IPAddress } }&#39; &lt;CONTAINER ID or NAME&gt; 命令 如何给容器指定一个固定 IP 地址，而不是每次重启容器 IP 地址都会变？答：使用以下命令启动容器可以使容器 IP 固定不变12$ docker network create -d bridge --subnet 172.25.0.0/16 my-net$ docker run --network=my-net --ip=172.25.3.3 -itd --name=my-container busybox 如何临时退出一个正在交互的容器的终端，而不终止它？答：按 Ctrl-p Ctrl-q。如果按 Ctrl-c 往往会让容器内应用进程终止，进而会终止容器。 如何控制容器占用系统资源（CPU、内存）的份额？答：在使用 docker create 命令创建容器或使用 docker run 创建并启动容器的时候，可以使用 -c|--cpu-shares[=0]参数来调整容器使用 CPU 的权重；使用 -m|--memory[=MEMORY] 参数来调整容器使用内存的大小。 热门镜像筛选具体Dockerfile请看：https://github.com/docker-library/docsUbuntuCentOSMySQLRedisNginxNode 关于Docker命令全景图 Dockerfile 最佳实践https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/https://yeasy.gitbooks.io/docker_practice/content/appendix/best_practices.html]]></content>
      <categories>
        <category>读书笔记</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http与TCP长短连接]]></title>
    <url>%2F2017%2F12%2F25%2FHttp%E4%B8%8ETCP%E9%95%BF%E7%9F%AD%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[TCP长连接与短连接的区别当网络通信时采用TCP协议时，在真正的读写操作之前，server与client之间必须建立一个连接，当读写操作完成后，双方不再需要这个连接时它们可以释放这个连接，连接的建立是需要三次握手的，而释放则需要4次握手，所以说每个连接的建立都是需要资源消耗和时间消耗的。经典的三次握手示意图：经典的四次握手关闭图： TCP短连接我们模拟一下TCP短连接的情况，client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起close操作。短连接一般只会在client/server间传递一次读写操作连接-&gt;传输数据-&gt;关闭连接 短连接的优点是：管理起来比较简单，存在的连接都是有用的连接，不需要额外的控制手段 TCP长连接client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 TCP保活功能，保活功能主要为服务器应用提供，服务器应用希望知道客户主机是否崩溃，从而可以代表客户使用资源。如果客户已经消失，使得服务器上保留一个半开放的连接，而服务器又在等待来自客户端的数据，则服务器将应永远等待客户端的数据，保活功能就是试图在服务器端检测到这种半开放的连接。 如果一个给定的连接在两小时内没有任何的动作，则服务器就向客户发一个探测报文段，客户主机必须处于以下4个状态之一： 客户主机依然正常运行，并从服务器可达。客户的TCP响应正常，而服务器也知道对方是正常的，服务器在两小时后将保活定时器复位。 客户主机已经崩溃，并且关闭或者正在重新启动。在任何一种情况下，客户的TCP都没有响应。服务端将不能收到对探测的响应，并在75秒后超时。服务器总共发送10个这样的探测 ，每个间隔75秒。如果服务器没有收到一个响应，它就认为客户主机已经关闭并终止连接。 客户主机崩溃并已经重新启动。服务器将收到一个对其保活探测的响应，这个响应是一个复位，使得服务器终止这个连接。 客户机正常运行，但是服务器不可达，这种情况与2类似，TCP能发现的就是没有收到探查的响应。从上面可以看出，TCP保活功能主要为探测长连接的存活状况，不过这里存在一个问题，存活功能的探测周期太长，还有就是它只是探测TCP连接的存活，属于比较斯文的做法，遇到恶意的连接时，保活功能就不够使了。 在长连接的应用场景下，client端一般不会主动关闭它们之间的连接，Client与server之间的连接如果一直不关闭的话，会存在一个问题，随着客户端连接越来越多，server早晚有扛不住的时候，这时候server端需要采取一些策略，如关闭一些长时间没有读写事件发生的连接，这样可以避免一些恶意连接导致server端服务受损；如果条件再允许就可以以客户端机器为颗粒度，限制每个客户端的最大长连接数，这样可以完全避免某个蛋疼的客户端连累后端服务。 长连接和短连接的产生在于client和server采取的关闭策略，具体的应用场景采用具体的策略，没有十全十美的选择，只有合适的选择。 建立连接——数据传输…（保持连接）…数据传输——关闭连接 应用场景：长连接多用于操作频繁（读写），点对点的通讯，而且连接数不能太多情况。每个TCP连接都需要三步握手，这需要时间，如果每个操作都是先连接，再操作的话那么处理速度会降低很多，所以每个操作完后都不断开，次处理时直接发送数据包就OK了，不用建立TCP连接。例如：数据库的连接用长连接， 如果用短连接频繁的通信会造成socket错误，而且频繁的socket 创建也是对资源的浪费。 而像WEB网站的http服务一般都用短链接（http1.0只支持短连接，1.1keep alive 带时间，操作次数限制的长连接），因为长连接对于服务端来说会耗费一定的资源，而像WEB网站这么频繁的成千上万甚至上亿客户端的连接用短连接会更省一些资源，如果用长连接，而且同时有成千上万的用户，如果每个用户都占用一个连接的话，那可想而知吧。所以并发量大，但每个用户无需频繁操作情况下需用短连好； 在长连接中一般是没有条件能够判断读写什么时候结束，所以必须要加长度报文头。读函数先是读取报文头的长度，再根据这个长度去读相应长度的报文。 SocketSocket长连接和短连接其实就是TCP/IP长连接和短连接Socket是应用层与TCP/IP协议族通信中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个facade模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。参考：TCP/IP详解http://blog.jobbole.com/93960/http://blog.csdn.net/jasonjwl/article/details/52085264http://www.cnblogs.com/beifei/archive/2011/06/26/2090611.htmlhttps://www.cnblogs.com/onlysun/p/4520553.html HTTP 长连接和短连接 HTTP协议与TCP/IP协议的关系HTTP的长连接和短连接本质上是TCP长连接和短连接。HTTP属于应用层协议，在传输层使用TCP协议，在网络层使用IP协议。IP协议主要解决网络路由和寻址问题，TCP协议主要解决如何在IP层之上可靠的传递数据包，使在网络上的另一端收到发端发出的所有包，并且顺序与发出顺序一致。TCP有可靠，面向连接的特点。 如何理解HTTP协议是无状态的HTTP协议是无状态的，指的是协议对于事务处理没有记忆能力，服务器不知道客户端是什么状态。也就是说，打开一个服务器上的网页和你之前打开这个服务器上的网页之间没有任何联系。HTTP是一个无状态的面向连接的协议，无状态不代表HTTP不能保持TCP连接，更不能代表HTTP使用的是UDP协议（无连接）。 什么是长连接、短连接？在HTTP/1.0中，默认使用的是短连接。也就是说，浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。如果客户端浏览器访问的某个HTML或其他类型的 Web页中包含有其他的Web资源，如JavaScript文件、图像文件、CSS文件等；当浏览器每遇到这样一个Web资源，就会建立一个HTTP会话。但从 HTTP/1.1起，默认使用长连接，用以保持连接特性。使用长连接的HTTP协议，会在响应头有加入这行代码：Connection:keep-alive 在使用长连接的情况下，当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的 TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接要客户端和服务端都支持长连接。 HTTP协议的长连接和短连接，实质上是TCP协议的长连接和短连接。 短连接的操作步骤是：建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接 长连接的操作步骤是：建立连接——数据传输…（保持连接）…数据传输——关闭连接 参考：http://blog.jobbole.com/93960/https://www.cnblogs.com/cswuyg/p/3653263.htmlhttp://www.cnblogs.com/skynet/archive/2010/12/11/1903347.html 长连接的数据传输完成识别使用长连接之后，客户端、服务端怎么知道本次传输结束呢？两部分：1是判断传输数据是否达到了Content-Length指示的大小；2动态生成的文件没有Content-Length，它是分块传输（Transfer-Encoding: chunked），这时候就要根据chunked编码来判断，chunked编码的数据在最后有一个空chunked块，表明本次传输数据结束。 chunked编码将数据分成一块一块的发生。Chunked编码将使用若干个Chunk串连而成，由一个标明长度为0的chunk标示结束。每个Chunk分为头部和正文两部分，头部内容指定正文的字符总数（十六进制的数字）和数量单位（一般不写），正文部分就是指定长度的实际内容，两部分之间用回车换行(CRLF)隔开。在最后一个长度为0的Chunk中的内容是称为footer的内容，是一些附加的Header信息（通常可以直接忽略）。 并发连接数的数量限制在web开发中需要关注浏览器并发连接的数量，RFC文档说，客户端与服务器最多就连上两通道。但服务器、个人客户端要不要这么做就随人意了，有些服务器就限制同时只能有1个TCP连接，导致客户端的多线程下载（客户端跟服务器连上多条TCP通道同时拉取数据）发挥不了威力，有些服务器则没有限制。并发数量的限制也跟长连接有关联，打开一个网页，很多个资源的下载可能就只被放到了少数的几条TCP连接里，这就是TCP通道复用（长连接）。如果并发连接数少，意味着网页上所有资源下载完需要更长的时间（用户感觉页面打开卡了）；并发数多了，服务器可能会产生更高的资源消耗峰值。浏览器只对同域名下的并发连接做了限制，也就意味着，web开发者可以把资源放到不同域名下，同时也把这些资源放到不同的机器上，这样就完美解决了。 容易混淆的概念——TCP的keep alive和HTTP的Keep-aliveTCP的keep alive是检查当前TCP连接是否活着（保活策略）；HTTP的Keep-alive是要让一个TCP连接活久点。它们是不同层次的概念。 TCP keep alive的表现：当一个连接“一段时间”没有数据通讯时，一方会发出一个心跳包（Keep Alive包），如果对方有回包则表明当前连接有效，继续监控。这个“一段时间”可以设置。 参考：TCP/IP详解http://blog.jobbole.com/93960/http://blog.csdn.net/jasonjwl/article/details/52085264http://www.cnblogs.com/beifei/archive/2011/06/26/2090611.htmlhttps://www.cnblogs.com/onlysun/p/4520553.htmlhttp://blog.jobbole.com/93960/https://www.cnblogs.com/cswuyg/p/3653263.htmlhttp://www.cnblogs.com/skynet/archive/2010/12/11/1903347.html]]></content>
      <categories>
        <category>网络基础</category>
      </categories>
      <tags>
        <tag>tcp</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo手册笔记]]></title>
    <url>%2F2017%2F12%2F25%2Fdubbo%E6%89%8B%E5%86%8C%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[用户手册架构图：provider.xml:123456789101112131415161718192021&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;!-- 提供方应用信息，用于计算依赖关系 --&gt; &lt;dubbo:application name="hello-world-app" /&gt; &lt;!-- 使用multicast广播注册中心暴露服务地址 --&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234" /&gt; &lt;!-- 用dubbo协议在20880端口暴露服务 --&gt; &lt;dubbo:protocol name="dubbo" port="20880" /&gt; &lt;!-- 声明需要暴露的服务接口 --&gt; &lt;dubbo:service interface="com.alibaba.dubbo.demo.DemoService" ref="demoService" /&gt; &lt;!-- 和本地bean一样实现服务 --&gt; &lt;bean id="demoService" class="com.alibaba.dubbo.demo.provider.DemoServiceImpl" /&gt;&lt;/beans&gt; consumer.xml：123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dubbo="http://code.alibabatech.com/schema/dubbo" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://code.alibabatech.com/schema/dubbo http://code.alibabatech.com/schema/dubbo/dubbo.xsd"&gt; &lt;!-- 消费方应用名，用于计算依赖关系，不是匹配条件，不要与提供方一样 --&gt; &lt;dubbo:application name="consumer-of-helloworld-app" /&gt; &lt;!-- 使用multicast广播注册中心暴露发现服务地址 --&gt; &lt;dubbo:registry address="multicast://224.5.6.7:1234" /&gt; &lt;!-- 生成远程服务代理，可以和本地bean一样使用demoService --&gt; &lt;dubbo:reference id="demoService" interface="com.alibaba.dubbo.demo.DemoService" /&gt;&lt;/beans&gt; 属性配置：Dubbo 将自动加载 classpath 根目录下的 dubbo.properties，可以通过JVM启动参数 -Ddubbo.properties.file=xxx.properties 改变缺省配置位置。 启动时检查Dubbo 缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止 Spring 初始化完成，以便上线时，能及早发现问题，默认 check=”true”。可以通过 check=”false” 关闭检查，比如，测试时，有些服务不关心，或者出现了循环依赖，必须有一方先启动。另外，如果你的 Spring 容器是懒加载的，或者通过 API 编程延迟引用服务，请关闭 check，否则服务临时不可用时，会抛出异常，拿到 null 引用，如果 check=”false”，总是会返回引用，当服务恢复时，能自动连上。123456关闭某个服务的启动时检查 (没有提供者时报错)：&lt;dubbo:reference interface="com.foo.BarService" check="false" /&gt;关闭所有服务的启动时检查 (没有提供者时报错)：&lt;dubbo:consumer check="false" /&gt;关闭注册中心启动时检查 (注册订阅失败时报错)：&lt;dubbo:registry check="false" /&gt; 通过 dubbo.properties1234dubbo.reference.com.foo.BarService.check=falsedubbo.reference.check=falsedubbo.consumer.check=falsedubbo.registry.check=false 集群容错在集群调用失败时，Dubbo 提供了多种容错方案，缺省为 failover 重试。 Failover Cluster失败自动切换，当出现失败，重试其它服务器。通常用于读操作，但重试会带来更长延迟。可通过 retries=”2” 来设置重试次数(不含第一次)。 重试次数配置如下：1234567&lt;dubbo:service retries=&quot;2&quot; /&gt;或&lt;dubbo:reference retries=&quot;2&quot; /&gt;或&lt;dubbo:reference&gt; &lt;dubbo:method name=&quot;findFoo&quot; retries=&quot;2&quot; /&gt;&lt;/dubbo:reference&gt; Failfast Cluster快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。Failsafe Cluster失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。Failback Cluster失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。Forking Cluster并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks=”2” 来设置最大并行数。Broadcast Cluster广播调用所有提供者，逐个调用，任意一台报错则报错 2。通常用于通知所有提供者更新缓存或日志等本地资源信息。 集群模式配置按照以下示例在服务提供方和消费方配置集群模式123&lt;dubbo:service cluster=&quot;failsafe&quot; /&gt;或&lt;dubbo:reference cluster=&quot;failsafe&quot; /&gt; 负载均衡在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。 Random LoadBalance:随机，按权重设置随机概率。在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 RoundRobin LoadBalance:轮循，按公约后的权重设置轮循比率。存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。 LeastActive LoadBalance:最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。 ConsistentHash LoadBalance:一致性 Hash，相同参数的请求总是发到同一提供者。当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。缺省只对第一个参数 Hash，如果要修改，请配置 &lt; dubbo:parameter key=”hash.arguments” value=”0,1” /&gt;缺省用 160 份虚拟节点，如果要修改，请配置 &lt; dubbo:parameter key=”hash.nodes” value=”320” /&gt; 服务端服务级别1&lt;dubbo:service interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 客户端服务级别1&lt;dubbo:reference interface=&quot;...&quot; loadbalance=&quot;roundrobin&quot; /&gt; 线程模型1&lt;dubbo:protocol name=&quot;dubbo&quot; dispatcher=&quot;all&quot; threadpool=&quot;fixed&quot; threads=&quot;100&quot; /&gt; Dispatcher all 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。 direct 所有消息都不派发到线程池，全部在 IO 线程上直接执行。 message 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 execution 只请求消息派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在 IO 线程上执行。 connection 在 IO 线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池。 ThreadPool fixed 固定大小线程池，启动时建立线程，不关闭，一直持有。(缺省) cached 缓存线程池，空闲一分钟自动删除，需要时重建。 limited 可伸缩线程池，但池中的线程数只会增长不会收缩。只增长不收缩的目的是为了避免收缩时突然来了大流量引起的性能问题。 策略选择原则：如果事件处理的逻辑能迅速完成，并且不会发起新的 IO 请求，比如只是在内存中记个标识，则直接在 IO 线程上处理更快，因为减少了线程池调度。但如果事件处理逻辑较慢，或者需要发起新的 IO 请求，比如需要查询数据库，则必须派发到线程池，否则 IO 线程阻塞，将导致不能接收其它请求。 直连提供者在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连，点对点直联方式，将以服务接口为单位，忽略注册中心的提供者列表，A 接口配置点对点，不影响 B 接口从注册中心获取列表。 通过文件映射如果服务比较多，也可以用文件映射，用 -Ddubbo.resolve.file 指定映射文件路径，此配置优先级高于 &lt; dubbo:reference&gt; 中的配置，如：java -Ddubbo.resolve.file=xxx.properties然后在映射文件 xxx.properties 中加入配置，其中 key 为服务名，value 为服务提供者 URL：com.alibaba.xxx.XxxService=dubbo://localhost:20890 多协议支持Dubbo 允许配置多协议，在不同服务上支持不同协议或者同一服务上同时支持多种协议。不同服务在性能上适用不同协议进行传输，比如大数据用短连接协议，小数据大并发用长连接协议.123456789101112&lt;dubbo:application name="world" /&gt;&lt;dubbo:registry id="registry" address="10.20.141.150:9090" username="admin" password="hello1234" /&gt;&lt;!-- 多协议配置 --&gt;&lt;dubbo:protocol name="dubbo" port="20880" /&gt;&lt;dubbo:protocol name="rmi" port="1099" /&gt;&lt;dubbo:protocol name="hessian" port="8080" /&gt;&lt;!-- 使用dubbo协议暴露服务 --&gt;&lt;dubbo:service interface="com.alibaba.hello.api.HelloService" version="1.0.0" ref="helloService" protocol="dubbo" /&gt;&lt;!-- 使用rmi协议暴露服务 --&gt;&lt;dubbo:service interface="com.alibaba.hello.api.DemoService" version="1.0.0" ref="demoService" protocol="rmi" /&gt; &lt;!-- 使用多个协议暴露服务 --&gt;&lt;dubbo:service id="helloService" interface="com.alibaba.hello.api.HelloService" version="1.0.0" protocol="dubbo,hessian" /&gt; 服务分组当一个接口有多种实现时，可以用 group 区分。12345678服务&lt;dubbo:service group="feedback" interface="com.xxx.IndexService" /&gt;&lt;dubbo:service group="member" interface="com.xxx.IndexService" /&gt;引用&lt;dubbo:reference id="feedbackIndexService" group="feedback" interface="com.xxx.IndexService" /&gt;&lt;dubbo:reference id="memberIndexService" group="member" interface="com.xxx.IndexService" /&gt;任意组：&lt;dubbo:reference id="barService" interface="com.foo.BarService" group="*" /&gt; 多版本当一个接口实现，出现不兼容升级时，可以用版本号过渡，版本号不同的服务相互间不引用。可以按照以下的步骤进行版本迁移： 在低压力时间段，先升级一半提供者为新版本 再将所有消费者升级为新版本 然后将剩下的一半提供者升级为新版本1234567891011121314老版本服务提供者配置：&lt;dubbo:service interface="com.foo.BarService" version="1.0.0" /&gt;新版本服务提供者配置：&lt;dubbo:service interface="com.foo.BarService" version="2.0.0" /&gt;老版本服务消费者配置：&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="1.0.0" /&gt;新版本服务消费者配置：&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="2.0.0" /&gt;如果不需要区分版本，可以按照以下的方式配置 1：&lt;dubbo:reference id="barService" interface="com.foo.BarService" version="*" /&gt; 分组聚合按组合并返回结果，比如菜单服务，接口一样，但有多种实现，用group区分，现在消费方需从每种group中调用一次返回结果，合并结果返回，这样就可以实现聚合菜单项。参考见此 参数验证参数验证功能是基于 JSR303 实现的，用户只需标识 JSR303 标准的验证 annotation，并通过声明 filter 来实现验证。具体略。 结果缓存 lru 基于最近最少使用原则删除多余缓存，保持最热的数据被缓存。 threadlocal 当前线程缓存，比如一个页面渲染，用到很多 portal，每个 portal 都要去查用户信息，通过线程缓存，可以减少这种多余访问。 jcache 与 JSR107 集成，可以桥接各种缓存实现。 1&lt;dubbo:reference interface=&quot;com.foo.BarService&quot; cache=&quot;lru&quot; /&gt; 上下文信息RpcContext 是一个 ThreadLocal 的临时状态记录器，当接收到 RPC 请求，或发起 RPC 请求时，RpcContext 的状态都会变化。服务消费方12345678910// 远程调用xxxService.xxx();// 本端是否为消费端，这里会返回trueboolean isConsumerSide = RpcContext.getContext().isConsumerSide();// 获取最后一次调用的提供方IP地址String serverIP = RpcContext.getContext().getRemoteHost();// 获取当前服务配置信息，所有配置信息都将转换为URL的参数String application = RpcContext.getContext().getUrl().getParameter("application");// 注意：每发起RPC调用，上下文状态会变化yyyService.yyy(); 服务提供方123456789101112131415public class XxxServiceImpl implements XxxService &#123; public void xxx() &#123; // 本端是否为提供端，这里会返回true boolean isProviderSide = RpcContext.getContext().isProviderSide(); // 获取调用方IP地址 String clientIP = RpcContext.getContext().getRemoteHost(); // 获取当前服务配置信息，所有配置信息都将转换为URL的参数 String application = RpcContext.getContext().getUrl().getParameter("application"); // 注意：每发起RPC调用，上下文状态会变化 yyyService.yyy(); // 此时本端变成消费端，这里会返回false boolean isProviderSide = RpcContext.getContext().isProviderSide(); &#125; &#125; 隐式参数可以通过 RpcContext 上的 setAttachment 和 getAttachment 在服务消费方和提供方之间进行参数的隐式传递。在服务消费方端设置隐式参数setAttachment 设置的 KV 对，在完成下面一次远程调用会被清空，即多次远程调用要多次设置。123RpcContext.getContext().setAttachment("index", "1"); // 隐式传参，后面的远程调用都会隐式将这些参数发送到服务器端，类似cookie，用于框架集成，不建议常规业务使用xxxService.xxx(); // 远程调用// ... 12// 获取客户端隐式传入的参数，用于框架集成，不建议常规业务使用 String index = RpcContext.getContext().getAttachment("index"); 异步调用基于 NIO 的非阻塞实现并行调用，客户端不需要启动多线程即可完成并行调用多个远程服务，相对多线程开销较小。在 consumer.xml 中配置：123456&lt;dubbo:reference id="fooService" interface="com.alibaba.foo.FooService"&gt; &lt;dubbo:method name="findFoo" async="true" /&gt;&lt;/dubbo:reference&gt;&lt;dubbo:reference id="barService" interface="com.alibaba.bar.BarService"&gt; &lt;dubbo:method name="findBar" async="true" /&gt;&lt;/dubbo:reference&gt; 123456789101112131415161718// 此调用会立即返回nullfooService.findFoo(fooId);// 拿到调用的Future引用，当结果返回后，会被通知和设置到此FutureFuture&lt;Foo&gt; fooFuture = RpcContext.getContext().getFuture(); // 此调用会立即返回nullbarService.findBar(barId);// 拿到调用的Future引用，当结果返回后，会被通知和设置到此FutureFuture&lt;Bar&gt; barFuture = RpcContext.getContext().getFuture(); // 此时findFoo和findBar的请求同时在执行，客户端不需要启动多线程来支持并行，而是借助NIO的非阻塞完成// 如果foo已返回，直接拿到返回值，否则线程wait住，等待foo返回后，线程会被notify唤醒Foo foo = fooFuture.get(); // 同理等待bar返回Bar bar = barFuture.get(); // 如果foo需要5秒返回，bar需要6秒返回，实际只需等6秒，即可获取到foo和bar，进行接下来的处理。 如果你只是想异步，完全忽略返回值，可以配置 return=”false”，以减少 Future 对象的创建和管理成本：1&lt;dubbo:method name="findFoo" async="true" return="false" /&gt; 参数回调参数回调方式与调用本地 callback 或 listener 相同，只需要在 Spring 的配置文件中声明哪个参数是 callback 类型即可。Dubbo 将基于长连接生成反向代理，这样就可以从服务器端调用客户端逻辑请参考这里 事件通知在调用之前、调用之后、出现异常时，会触发 oninvoke、onreturn、onthrow 三个事件，可以配置当事件发生时，通知哪个类的哪个方法.1234&lt;bean id ="demoCallback" class = "com.alibaba.dubbo.callback.implicit.NofifyImpl" /&gt;&lt;dubbo:reference id="demoService" interface="com.alibaba.dubbo.callback.implicit.IDemoService" version="1.0.0" group="cn" &gt; &lt;dubbo:method name="get" async="true" onreturn = "demoCallback.onreturn" onthrow="demoCallback.onthrow" /&gt;&lt;/dubbo:reference&gt; 本地存根远程服务后，客户端通常只剩下接口，而实现全在服务器端，但提供方有些时候想在客户端也执行部分逻辑，比如：做 ThreadLocal 缓存，提前验证参数，调用失败后伪造容错数据等等，此时就需要在 API 中带上 Stub，客户端生成 Proxy 实例，会把 Proxy 通过构造函数传给 Stub 1，然后把 Stub 暴露给用户，Stub 可以决定要不要去调 Proxy。 本地伪装本地伪装 通常用于服务降级，比如某验权服务，当服务提供方全部挂掉后，客户端不抛出异常，而是通过 Mock 数据返回授权失败。在 spring 配置文件中按以下方式配置：123&lt;dubbo:service interface="com.foo.BarService" mock="true" /&gt;或&lt;dubbo:service interface="com.foo.BarService" mock="com.foo.BarServiceMock" /&gt; 在工程中提供 Mock 实现：1234567package com.foo;public class BarServiceMock implements BarService &#123; public String sayHello(String name) &#123; // 你可以伪造容错数据，此方法只在出现RpcException时被执行 return "容错数据"; &#125;&#125; 如果只是想简单返回null:1&lt;dubbo:service interface="com.foo.BarService" mock="return null" /&gt; Mock 是 Stub 的一个子集，便于服务提供方在客户端执行容错逻辑，因经常需要在出现 RpcException (比如网络失败，超时等)时进行容错，而在出现业务异常(比如登录用户名密码错误)时不需要容错，如果用 Stub，可能就需要捕获并依赖 RpcException 类，而用 Mock 就可以不依赖 RpcException，因为它的约定就是只有出现 RpcException 时才执行。 并发及连接控制可以限制单个提供方或消费方的并发数，连接数，或改变负载均衡策略。 路由规则、配置规则、服务降级路由规则可以实现的如： 排除预发布机 黑白名单 只暴露部分机器 为重要应用提供额外的机器 读写分离 前后台分离 隔离不同网段机房 路由规则决定一次 dubbo 服务调用的目标服务器，分为条件路由规则和脚本路由规则，并且支持可扩展。向注册中心写入路由规则的操作通常由监控中心或治理中心的页面完成123RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf("zookeeper://10.20.153.10:2181"));registry.register(URL.valueOf("condition://0.0.0.0/com.foo.BarService?category=routers&amp;dynamic=false&amp;rule=" + URL. 基于条件表达式的路由规则，如：host = 10.20.153.10 =&gt; host = 10.20.153.11脚本路由规则支持 JDK 脚本引擎的所有脚本，比如：javascript, jruby, groovy 等，通过 type=javascript 参数设置脚本类型，缺省为 javascript。 配置规则可以实现的如: 禁用提供者(禁用消费者访问请使用路由规则) 调整权重(默认100) 调整负载均衡策略(默认random) 服务降级：(通常用于临时屏蔽某个出错的非关键服务) 向注册中心写入动态配置覆盖规则。该功能通常由监控中心或治理中心的页面完成。123RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf("zookeeper://10.20.153.10:2181"));registry.register(URL.valueOf("override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;timeout=1000")); 服务降级可以通过服务降级功能临时屏蔽某个出错的非关键服务，并定义降级后的返回策略。向注册中心写入动态配置覆盖规则：123RegistryFactory registryFactory = ExtensionLoader.getExtensionLoader(RegistryFactory.class).getAdaptiveExtension();Registry registry = registryFactory.getRegistry(URL.valueOf("zookeeper://10.20.153.10:2181"));registry.register(URL.valueOf("override://0.0.0.0/com.foo.BarService?category=configurators&amp;dynamic=false&amp;application=foo&amp;mock=force:return+null")); mock=force:return+null 表示消费方对该服务的方法调用都直接返回 null 值，不发起远程调用。用来屏蔽不重要服务不可用时对调用方的影响。 还可以改为 mock=fail:return+null 表示消费方对该服务的方法调用在失败后，再返回 null 值，不抛异常。用来容忍不重要服务不稳定时对调用方的影响。 优雅停机Dubbo 是通过 JDK 的 ShutdownHook 来完成优雅停机的，所以如果用户使用 kill -9 PID 等强制关闭指令，是不会执行优雅停机的，只有通过 kill PID 时，才会执行。设置优雅停机超时时间，缺省超时时间是 10 秒，如果超时则强制关闭。dubbo.properties1dubbo.service.shutdown.wait=15000 如果 ShutdownHook 不能生效，可以自行调用，使用tomcat等容器部署的場景，建议通过扩展ContextListener等自行调用以下代码实现优雅停机：1ProtocolConfig.destroyAll(); 主机绑定缺省主机 IP 查找顺序： 通过 LocalHost.getLocalHost() 获取本机地址。 如果是 127.* 等 loopback 地址，则扫描各网卡，获取网卡 IP。 注册的地址如果获取不正确，比如需要注册公网地址，可以：可以在 /etc/hosts 中加入：机器名 公网 IP，比如：test1 205.182.23.201在 dubbo.xml 中加入主机地址的配置：1&lt;dubbo:protocol host="205.182.23.201"&gt; 或在 dubbo.properties 中加入主机地址的配置：1dubbo.protocol.host=205.182.23.201 端口配置: 日志dubbo内置 log4j、slf4j、jcl、jdk 这些日志框架的适配，也可以通过以下方式显示配置日志输出策略：123456命令行java -Ddubbo.application.logger=log4j在 dubbo.properties 中指定dubbo.application.logger=log4j在 dubbo.xml 中配置&lt;dubbo:application logger=&quot;log4j&quot; /&gt; 访问日志如果你想记录每一次请求信息，可开启访问日志，类似于apache的访问日志。注意：此日志量比较大，请注意磁盘容量。1234将访问日志输出到当前应用的log4j日志：&lt;dubbo:protocol accesslog="true" /&gt;将访问日志输出到指定文件：&lt;dubbo:protocol accesslog="http://10.20.160.198/wiki/display/dubbo/foo/bar.log" /&gt; 服务容器服务容器是一个 standalone 的启动程序,服务容器只是一个简单的 Main 方法，并加载一个简单的 Spring 容器，用于暴露服务。容器类型Spring Container自动加载 META-INF/spring 目录下的所有 Spring 配置。配置 spring 配置加载位置：1dubbo.spring.config=classpath*:META-INF/spring/*.xml Jetty Container启动一个内嵌 Jetty，用于汇报状态。123dubbo.jetty.port=8080：配置 jetty 启动端口dubbo.jetty.directory=/foo/bar：配置可通过 jetty 直接访问的目录，用于存放静态文件dubbo.jetty.page=log,status,system：配置显示的页面，缺省加载所有页面 Log4j Container自动配置 log4j 的配置，在多进程启动时，自动给日志文件按进程分目录。123dubbo.log4j.file=/foo/bar.log：配置日志文件路径dubbo.log4j.level=WARN：配置日志级别dubbo.log4j.subdirectory=20880：配置日志子目录，用于多进程启动，避免冲突 容器启动 缺省只加载 springjava com.alibaba.dubbo.container.Main 通过 main 函数参数传入要加载的容器java com.alibaba.dubbo.container.Main spring jetty log4j 通过 JVM 启动参数传入要加载的容器java com.alibaba.dubbo.container.Main -Ddubbo.container=spring,jetty,log4j 通过 classpath 下的 dubbo.properties 配置传入要加载的容器dubbo.container=spring,jetty,log4j ReferenceConfig 缓存ReferenceConfig 实例很重，封装了与注册中心的连接以及与提供者的连接，需要缓存。否则重复生成 ReferenceConfig 可能造成性能问题并且会有内存和连接泄漏。在 API 方式编程时，容易忽略此问题。因此，自 2.4.0 版本开始， dubbo 提供了简单的工具类 ReferenceConfigCache用于缓存 ReferenceConfig 实例。12345678910ReferenceConfig&lt;XxxService&gt; reference = new ReferenceConfig&lt;XxxService&gt;();reference.setInterface(XxxService.class);reference.setVersion("1.0.0");......ReferenceConfigCache cache = ReferenceConfigCache.getCache();// cache.get方法中会缓存 Reference对象，并且调用ReferenceConfig.get方法启动ReferenceConfigXxxService xxxService = cache.get(reference);// 注意！ Cache会持有ReferenceConfig，不要在外部再调用ReferenceConfig的destroy方法，导致Cache内的ReferenceConfig失效！// 使用xxxService对象xxxService.sayHello(); 消除 Cache 中的 ReferenceConfig，将销毁 ReferenceConfig 并释放对应的资源。12ReferenceConfigCache cache = ReferenceConfigCache.getCache();cache.destroy(reference); 缺省 ReferenceConfigCache 把相同服务 Group、接口、版本的 ReferenceConfig 认为是相同，缓存一份。即以服务 Group、接口、版本为缓存的 Key。可以修改这个策略:12KeyGenerator keyGenerator = new ...ReferenceConfigCache cache = ReferenceConfigCache.getCache(keyGenerator ); 分布式事务(目前未实现)分布式事务基于 JTA/XA 规范实现两阶段提交： 线程栈自动dump当业务线程池满时，我们需要知道线程都在等待哪些资源、条件，以找到系统的瓶颈点或异常点。dubbo通过Jstack自动导出线程堆栈来保留现场，方便排查问题默认策略:导出路径，user.home标识的用户主目录导出间隔，最短间隔允许每隔10分钟导出一次 指定导出路径：123&lt;dubbo:application ...&gt; &lt;dubbo:parameter key=&quot;dump.directory&quot; value=&quot;/tmp&quot; /&gt;&lt;/dubbo:application&gt; Netty4支持dubbo 2.5.6版本新增了对netty4通信模块的支持，启用方式如下provider端：123&lt;dubbo:protocol server=&quot;netty4&quot; /&gt;或&lt;dubbo:provider server=&quot;netty4&quot; /&gt; consumer端：1&lt;dubbo:consumer client=&quot;netty4&quot; /&gt; schema 配置参考手册API参考手册：略dubbo支持的配置方式:属性配置，注解配置，API配置，xml配置。所有配置项分为三大类： 服务发现：表示该配置项用于服务的注册与发现，目的是让消费方找到提供方。 服务治理：表示该配置项用于治理服务间的关系，或为开发测试提供便利条件。 性能调优：表示该配置项用于调优性能，不同的选项对性能会产生影响。 所有配置最终都将转换为URL表示，并由服务提供方生成，经注册中心传递给消费方，各属性对应 URL 的参数。提示： XML Schema: http://code.alibabatech.com/schema/dubbo/dubbo.xsd ↩ 注意：只有 group，interface，version 是服务的匹配条件，三者决定是不是同一个服务，其它配置项均为调优和治理参数。 URL 格式：protocol://username:password@host:port/path?key=value&amp;key=value 具体配置项与参数请参考文档 协议参考手册dubbo支持很多协议如:dubbo,rmi,hessian,http,webservice,thrift,redis. 官方推荐使用dubbo协议Dubbo 缺省协议采用单一长连接和 NIO 异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。反之，Dubbo 缺省协议不适合传送大数据量的服务，比如传文件，传视频等，除非请求量很低。 Transporter: mina, netty, grizzy Serialization: dubbo, hessian2, java, json Dispatcher: all, direct, message, execution, connection ThreadPool: fixed, cached 缺省协议，使用基于 mina 1.1.7 和 hessian 3.2.1 的 tbremoting 交互。 连接个数：单连接 连接方式：长连接 传输协议：TCP 传输方式：NIO 异步传输 序列化：Hessian 二进制序列化 适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供者，尽量不要用 dubbo 协议传输大文件或超大字符串。 适用场景：常规远程服务方法调用 约束 参数及返回值需实现 Serializable 接口 参数及返回值不能自定义实现 List, Map, Number, Date, Calendar 等接口，只能用 JDK 自带的实现，因为 hessian 会做特殊处理，自定义实现类中的属性值都会丢失。 Hessian 序列化，只传成员属性值和值的类型，不传方法或静态变量 服务器端和客户端对领域对象并不需要完全一致，而是按照最大匹配原则。 注册中心参考手册dubbo支持Zookeeper,Multicast,Redis,Simple等作为注册中心，但推荐使用Zookeeper.zookeeper 注册中心: 流程说明： 服务提供者启动时: 向 /dubbo/com.foo.BarService/providers 目录下写入自己的 URL 地址 服务消费者启动时: 订阅 /dubbo/com.foo.BarService/providers 目录下的提供者 URL 地址。并向 /dubbo/com.foo.BarService/consumers 目录下写入自己的 URL 地址 监控中心启动时: 订阅 /dubbo/com.foo.BarService 目录下的所有提供者和消费者 URL 地址。在 provider 和 consumer 中增加 zookeeper 客户端 jar 包依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.3.3&lt;/version&gt;&lt;/dependency&gt; Dubbo 支持 zkclient 和 curator 两种 Zookeeper 客户端实现,从 2.2.0 版本开始缺省为 zkclient 实现.1&lt;dubbo:registry ... client=&quot;zkclient&quot; /&gt; 12345&lt;dependency&gt; &lt;groupId&gt;com.github.sgroschupf&lt;/groupId&gt; &lt;artifactId&gt;zkclient&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt;&lt;/dependency&gt; Zookeeper 单机配置:123&lt;dubbo:registry address="zookeeper://10.20.153.10:2181" /&gt;或：&lt;dubbo:registry protocol="zookeeper" address="10.20.153.10:2181" /&gt; Zookeeper 集群配置：123&lt;dubbo:registry address="zookeeper://10.20.153.10:2181?backup=10.20.153.11:2181,10.20.153.12:2181" /&gt;或：&lt;dubbo:registry protocol="zookeeper" address="10.20.153.10:2181,10.20.153.11:2181,10.20.153.12:2181" /&gt; Telnet管理命令旧有方式：(下面会提到新方式，2.5.8之后的方式为新方式)使用telnet localhost 20880进入交互shell后，可使用如下命令进行交互：1234567891011121314151617181920212223242526272829303132333435363738394041ls: 显示服务列表ls -l: 显示服务详细信息列表ls XxxService: 显示服务的方法列表ls -l XxxService: 显示服务的方法详细信息列表ps: 显示服务端口列表ps -l: 显示服务地址列表ps 20880: 显示端口上的连接信息ps -l 20880: 显示端口上的连接详细信息cd XxxService: 改变缺省服务，当设置了缺省服务，凡是需要输入服务名作为参数的命令，都可以省略服务参数cd /: 取消缺省服务pwd: 显示当前缺省服务trace XxxService: 跟踪 1 次服务任意方法的调用情况trace XxxService 10: 跟踪 10 次服务任意方法的调用情况trace XxxService xxxMethod: 跟踪 1 次服务方法的调用情况trace XxxService xxxMethod 10: 跟踪 10 次服务方法的调用情况count XxxService: 统计 1 次服务任意方法的调用情况count XxxService 10: 统计 10 次服务任意方法的调用情况count XxxService xxxMethod: 统计 1 次服务方法的调用情况count XxxService xxxMethod 10: 统计 10 次服务方法的调用情况invoke XxxService.xxxMethod(&#123;&quot;prop&quot;: &quot;value&quot;&#125;): 调用服务的方法invoke xxxMethod(&#123;&quot;prop&quot;: &quot;value&quot;&#125;): 调用服务的方法(自动查找包含此方法的服务)status: 显示汇总状态，该状态将汇总所有资源的状态，当全部 OK 时则显示 OK，只要有一个 ERROR 则显示 ERROR，只要有一个 WARN 则显示 WARNstatus -l: 显示状态列表log debug: 修改 dubbo logger 的日志级别log 100: 查看 file logger 的最后 100 字符的日志help: 显示 telnet 命帮助信息help xxx: 显示xxx命令的详细帮助信息clear: 清除屏幕上的内容clear 100: 清除屏幕上的指定行数的内容exit: 退出当前 telnet 命令行 dubbo 2.5.8 新版本重构了 telnet 模块，提供了新的 telnet 命令支持。端口新版本的 telnet 端口 与 dubbo 协议的端口是不同的端口，默认为 22222，可通过配置文件dubbo.properties或JVM参数修改:dubbo.qos.port=33333安全默认情况下，dubbo 接收任何主机发起的命令，可通过配置文件dubbo.properties或jvm参数修改:dubbo.qos.accept.foreign.ip=false telnet 与 http 协议telnet 模块现在同时支持 http 协议和 telnet 协议，方便各种情况的使用12➜ ~ telnet localhost 22222➜ ~ curl &quot;localhost:22222/ls?arg1=xxx&amp;arg2=xxxx&quot; Online 上线服务命令1234567//上线所有服务dubbo&gt;onlineOK//根据正则，上线部分服务dubbo&gt;online com.*OK Offline 下线服务命令1234567//下线所有服务dubbo&gt;offlineOK//根据正则，上线部分服务dubbo&gt;offline com.*OK 服务化最佳实践分包:建议将服务接口，服务模型，服务异常等均放在 API 包中，因为服务模型及异常也是 API 的一部分，同时，这样做也符合分包原则：重用发布等价原则(REP)，共同重用原则(CRP)。 粒度:服务接口尽可能大粒度，每个服务方法应代表一个功能，而不是某功能的一个步骤，否则将面临分布式事务问题，Dubbo 暂未提供分布式事务支持。服务接口建议以业务场景为单位划分，并对相近业务做抽象，防止接口数量爆炸。不建议使用过于抽象的通用接口，如：Map query(Map)，这样的接口没有明确语义，会给后期维护带来不便。 版本:每个接口都应定义版本号，为后续不兼容升级提供可能，如：1&lt;dubbo:service interface="com.xxx.XxxService" version="1.0" /&gt;。 当不兼容时，先升级一半提供者为新版本，再将消费者全部升为新版本，然后将剩下的一半提供者升为新版本。 兼容性:服务接口增加方法，或服务模型增加字段，可向后兼容，删除方法或删除字段，将不兼容，枚举类型新增字段也不兼容，需通过变更版本号升级。各协议的兼容性不同，参见： 服务协议 序列化:服务参数及返回值建议使用 POJO 对象，即通过 setter, getter 方法表示属性的对象。服务参数及返回值不建议使用接口，因为数据模型抽象的意义不大，并且序列化需要接口实现类的元信息，并不能起到隐藏实现的意图。服务参数及返回值都必需是 byValue 的，而不能是 byReference 的，消费方和提供方的参数或返回值引用并不是同一个，只是值相同，Dubbo 不支持引用远程对象。 异常:建议使用异常汇报错误，而不是返回错误码，异常信息能携带更多信息，以及语义更友好。如果担心性能问题，在必要时，可以通过 override 掉异常类的 fillInStackTrace() 方法为空方法，使其不拷贝栈信息。服务提供方不应将 DAO 或 SQL 等异常抛给消费方，应在服务实现中对消费方不关心的异常进行包装，否则可能出现消费方无法反序列化相应异常。 调用:不要只是因为是 Dubbo 调用，而把调用 try…catch 起来。try…catch 应该加上合适的回滚边界上。对于输入参数的校验逻辑在 Provider 端要有。如有性能上的考虑，服务实现者可以考虑在 API 包上加上服务 Stub 类来完成检验。 推荐配置方式在 Provider 上尽量多配置 Consumer 端属性: 作服务的提供者，比服务使用方更清楚服务性能参数，如调用的超时时间，合理的重试次数，等等 在 Provider 配置后，Consumer 不配置则会使用 Provider 的配置值。否则，Consumer 会使用 Consumer 端的全局设置，这对于 Provider 不可控的，并且往往是不合理的 Provider 上尽量多配置 Consumer 端的属性，让 Provider 实现者一开始就思考 Provider 服务特点、服务质量的问题。 12345678&lt;dubbo:service interface="com.alibaba.hello.api.HelloService" version="1.0.0" ref="helloService" timeout="300" retry="2" loadbalance="random" actives="0"/&gt;&lt;dubbo:service interface="com.alibaba.hello.api.WorldService" version="1.0.0" ref="helloService" timeout="300" retry="2" loadbalance="random" actives="0" &gt; &lt;dubbo:method name="findAllPerson" timeout="10000" retries="9" loadbalance="leastactive" actives="5" /&gt;&lt;dubbo:service/&gt; 在 Provider 上可以配置的 Consumer 端属性有： timeout 方法调用超时 retries 失败重试次数，缺省是 2 loadbalance 负载均衡算法 ，缺省是随机 random。还可以有轮询 roundrobin、最不活跃优先 leastactive actives 消费者端，最大并发调用限制，即当 Consumer 对一个服务的并发调用到上限后，新调用会 Wait 直到超时. Provider 上配置合理的 Provider 端属性12345&lt;dubbo:protocol threads="200" /&gt; &lt;dubbo:service interface="com.alibaba.hello.api.HelloService" version="1.0.0" ref="helloService" executes="200" &gt; &lt;dubbo:method name="findAllPerson" executes="50" /&gt;&lt;/dubbo:service&gt; Provider 上可以配置的 Provider 端属性有： threads 服务线程池大小 executes 一个服务提供者并行执行请求上限，即当 Provider 对一个服务的并发调用到上限后，新调用会 Wait，这个时候 Consumer可能会超时。 配置 Dubbo 缓存文件提供者列表缓存文件：1&lt;dubbo:registry file=”$&#123;user.home&#125;/output/dubbo.cache” /&gt; 注意： 文件的路径，应用可以根据需要调整，保证这个文件不会在发布过程中被清除。 如果有多个应用进程注意不要使用同一个文件，避免内容被覆盖。 这个文件会缓存注册中心的列表和服务提供者列表。有了这项配置后，当应用重启过程中，Dubbo 注册中心不可用时则应用会从这个缓存文件读取服务提供者列表的信息，进一步保证应用可靠性。 监控配置使用固定端口暴露服务，而不要使用随机端口这样在注册中心推送有延迟的情况下，消费者通过缓存列表也能调用到原地址，保证调用成功。使用 Dragoon 的 http 监控项监控注册中心上服务提供方服务提供方，使用 Dragoon 的 telnet 或 shell 监控项监控服务提供者端口状态：echo status | nc -i 1 20880 | grep OK | wc -l，其中的 20880 为服务端口服务消费方，通过将服务强制转型为 EchoService，并调用 $echo() 测试该服务的提供者是可用如 assertEqauls(“OK”, ((EchoService)memberService).$echo(“OK”)); 不要使用 dubbo.properties 文件配置，推荐使用对应 XML 配置 dubbo管理手册1、示列提供者，消费者安装2、Zookeeper安装3、管理控制台安装管理控制台为内部裁剪版本，开源部分主要包含：路由规则，动态配置，服务降级，访问控制，权重调整，负载均衡，等管理功能。dubbo-admin dubbo-dev手册 config 配置层：对外配置接口，以 ServiceConfig, ReferenceConfig 为中心，可以直接初始化配置类，也可以通过 spring 解析配置生成配置类 proxy 服务代理层：服务接口透明代理，生成服务的客户端 Stub 和服务器端 Skeleton, 以 ServiceProxy 为中心，扩展接口为 ProxyFactory registry 注册中心层：封装服务地址的注册与发现，以服务 URL 为中心，扩展接口为 RegistryFactory, Registry, RegistryService cluster 路由层：封装多个提供者的路由及负载均衡，并桥接注册中心，以 Invoker 为中心，扩展接口为 Cluster, Directory, Router, LoadBalance monitor 监控层：RPC 调用次数和调用时间监控，以 Statistics 为中心，扩展接口为 MonitorFactory, Monitor, MonitorService protocol 远程调用层：封将 RPC 调用，以 Invocation, Result 为中心，扩展接口为 Protocol, Invoker, Exporter exchange 信息交换层：封装请求响应模式，同步转异步，以 Request, Response 为中心，扩展接口为 Exchanger, ExchangeChannel, ExchangeClient, ExchangeServer transport 网络传输层：抽象 mina 和 netty 为统一接口，以 Message 为中心，扩展接口为 Channel, Transporter, Client, Server, Codec serialize 数据序列化层：可复用的一些工具，扩展接口为 Serialization, ObjectInput, ObjectOutput, ThreadPool 在 Dubbo 的核心领域模型中： Protocol 是服务域，它是 Invoker 暴露和引用的主功能入口，它负责 Invoker 的生命周期管理。 Invoker 是实体域，它是 Dubbo 的核心模型，其它模型都向它靠扰，或转换成它，它代表一个可执行体，可向它发起 invoke 调用，它有可能是一个本地的实现，也可能是一个远程的实现，也可能一个集群实现。 Invocation 是会话域，它持有调用过程中的变量，比如方法名，参数等。 基本设计原则 采用 Microkernel + Plugin 模式，Microkernel 只负责组装 Plugin，Dubbo 自身的功能也是通过扩展点实现的，也就是 Dubbo 的所有功能点都可被用户自定义扩展所替换。 采用 URL 作为配置信息的统一格式，所有扩展点都通过传递 URL 携带配置信息。 更具体的请看官方dev手册。 扩展实现之负载均衡由于dubbo采用 Microkernel + Plugin 模式设计原则。dubbo提供了基本上所有可以扩展的扩展点，基本上满足需求。 Dubbo 的扩展点加载从 JDK 标准的 SPI (Service Provider Interface) 扩展点发现机制加强而来。Dubbo 改进了 JDK 标准的 SPI 的以下问题： JDK 标准的 SPI 会一次性实例化扩展点所有实现，如果有扩展实现初始化很耗时，但如果没用上也加载，会很浪费资源。 如果扩展点加载失败，连扩展点的名称都拿不到了。比如：JDK 标准的 ScriptEngine，通过 getName() 获取脚本类型的名称，但如果 RubyScriptEngine 因为所依赖的 jruby.jar 不存在，导致 RubyScriptEngine 类加载失败，这个失败原因被吃掉了，和 ruby 对应不起来，当用户执行 ruby 脚本时，会报不支持 ruby，而不是真正失败的原因。 增加了对扩展点 IoC 和 AOP 的支持，一个扩展点可以直接 setter 注入其它扩展点。 约定：在扩展类的 jar 包内，放置扩展点配置文件 META-INF/dubbo/接口全限定名，内容为：配置名=扩展实现类全限定名，多个实现类用换行符分隔。 以负载均衡扩展点为例：扩展接口：com.alibaba.dubbo.rpc.cluster.LoadBalance默认实现： com.alibaba.dubbo.rpc.cluster.loadbalance.RandomLoadBalance com.alibaba.dubbo.rpc.cluster.loadbalance.RoundRobinLoadBalance com.alibaba.dubbo.rpc.cluster.loadbalance.LeastActiveLoadBalance1、实现接口12345public class XxxLoadBalance implements LoadBalance &#123; public &lt;T&gt; Invoker&lt;T&gt; select(List&lt;Invoker&lt;T&gt;&gt; invokers, Invocation invocation) throws RpcException &#123; // ... &#125;&#125; 2、在项目的META-INF/dubbo/com.alibaba.dubbo.rpc.cluster.LoadBalance：文件中添加1xxx=com.xxx.XxxLoadBalance 3、使用123&lt;dubbo:protocol loadbalance=&quot;xxx&quot; /&gt;&lt;!-- 缺省值设置，当&lt;dubbo:protocol&gt;没有配置loadbalance时，使用此配置 --&gt;&lt;dubbo:provider loadbalance=&quot;xxx&quot; /&gt; 设计原则本文仅列举部分原则。 1、采用 Microkernel + Plugin 模式Microkernel 只负责组装 Plugin，Dubbo 自身的功能也是通过扩展点实现的，也就是 Dubbo 的所有功能点都可被用户自定义扩展所替换。 2、API 与 SPI 分离框架或组件通常有两类客户，一个是使用者，一个是扩展者。API (Application Programming Interface) 是给使用者用的，而 SPI (Service Provide Interface) 是给扩展者用的。在设计时，尽量把它们隔离开，而不要混在一起。也就是说，使用者是看不到扩展者写的实现的。 3、在重要的过程上设置拦截接口、重要的状态的变更发送事件并留出监听接口 4、最少概念，一致性概念模型]]></content>
      <categories>
        <category>读书笔记</category>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
        <tag>microservice</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构之DevOps]]></title>
    <url>%2F2017%2F12%2F21%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E4%B9%8BDevOps%2F</url>
    <content type="text"><![CDATA[CloudNative12因素应用:为了解决传统应用升级缓慢、架构臃肿、不能快速迭代、故障不能快速定位、问题无法快速解决等问题，云原生这一概念横空出世。云原生可以改进应用开发的效率，改变企业的组织结构，甚至会在文化层面上直接影响一个公司的决策。 另外，云原生也很好地解释了云上运行的应用应该具备什么样的架构特性——敏捷性、可扩展性、故障可恢复性。综上所述，云原生应用应该具备以下几个关键词： 敏捷 可靠 高弹性 易扩展 故障隔离保护 不中断业务持续更新 KubernetesKubernetes(k8s)是Google开源的容器集群管理系统，其提供应用部署、维护、 扩展机制等功能，利用Kubernetes能方便地管理跨机器运行容器化的应用，其主要功能如下：1) 使用Docker对应用程序包装(package)、实例化(instantiate)、运行(run)。2) 以集群的方式运行、管理跨机器的容器。3) 解决Docker跨机器容器之间的通讯问题。4) Kubernetes的自我修复机制使得容器集群总是运行在用户期望的状态。 Kubernetes用户可以通过编写一个yaml或者json格式的配置文件，也可以通过工具/代码生成或直接请求kubernetes API创建应用，该配置文件中包含了用户想要应用程序保持的状态，不论整个kubernetes集群中的个别主机发生什么问题，都不会影响应用程序的状态，你还可以通过改变该配置文件或请求kubernetes API来改变应用程序的状态。 Kubernetes的key Concepts:1、Pod: 最小化的部署单元。that can be created, scheduled, and managed. It’s a logical collection of containers that belong to an application.通常Pod里的容器运行相同的应用。同一个Pod里的容器共享同一个网络命名空间，可以使用localhost互相通信。 如果Pod是短暂的，那么我怎么才能持久化容器数据使其能够跨重启而存在呢？ 是的，Kubernetes支持卷的概念，因此可以使用持久化的卷类型。 是否手动创建Pod，如果想要创建同一个容器的多份拷贝，需要一个个分别创建出来么？可以手动创建单个Pod，但是也可以使用Replication Controller使用Pod模板创建出多份拷贝，下文会详细介绍。 如果Pod是短暂的，那么重启时IP地址可能会改变，那么怎么才能从前端容器正确可靠地指向后台容器呢？这时可以使用Service，每个资源都可以用一个配置文件来定义， For example, a Couchbase pod can be defined with the following .yaml file:：1234567891011121314apiVersion: v1kind: Pod# labels attached to this Podmetadata: name: couchbase-pod labels: name: couchbase-podspec: containers: - name: couchbase # Docker image that will run in this Pod image: couchbase ports: - containerPort: 8091 2、Label：如上面的metadata.labels，一般用于标识。Labels是Service和Replication Controller运行的基础，为了将访问Service的请求转发给后端提供服务的多个容器，正是通过标识容器的labels来选择正确的容器。同样，Replication Controller也使用labels来管理通过pod 模板创建的一组容器。 3、Replica Sets(副本集)：A replica set ensures that a specified number of pod replicas are running on worker nodes at any one time. It allows both up- and down-scaling the number of replicas. It also ensures recreation of a pod when the worker node reboots or otherwise fails.如下例子，指定两个容器实例：123456789101112131415161718192021222324252627apiVersion: extensions/v1beta1kind: ReplicaSetmetadata: name: couchbase-rsspec: # Two replicas of the Pod to be created replicas: 2 # Identifies the label key and value on the Pod that # this Replica Set is responsible for managing selector: matchLabels: app: couchbase-rs-pod matchExpressions: - &#123;key: tier, operator: In, values: [&quot;backend&quot;]&#125; template: metadata: labels: # label key and value on the pod. # These must match the selector above. app: couchbase-rs-pod tier: backend spec: containers: - name: couchbase image: couchbase ports: - containerPort: 8091 Replication Controller主要有如下用法：1) Rescheduling如上所述，Replication Controller会确保Kubernetes集群中指定的pod副本(replicas)在运行， 即使在节点出错时。2) Scaling通过修改Replication Controller的副本(replicas)数量来水平扩展或者缩小运行的pods。3) Rolling updatesReplication Controller的设计原则使得可以一个一个地替换pods来rolling updates服务。4) Multiple release tracks如果需要在系统中运行multiple release的服务，Replication Controller使用labels来区分multiple release tracks。 4、Service：每个Pod被分配一个ip地址，如果Pod死了，会被重新创建，但此时ip会发生改变，为了避免这种情况对某些依赖ip容器如数据库访问造成影响，可用Service, 分配到service的ip不会变化。Service是定义一系列Pod以及访问这些Pod的策略的一层抽象。Service通过Label找到Pod组。A Service defines a logical set of Pods and a policy by which to access them。是真实应用服务的抽象，每一个服务后面都有很多对应的容器来支持，通过Proxy的port和服务selector决定服务请求传递给后端提供服务的容器，对外表现为一个单一访问接口，外部不需要了解后端如何运行，这给扩展或维护后端带来很大的好处。 现在，假定有2个后台Pod，并且定义后台Service的名称为‘backend-service’，lable选择器为（tier=backend, app=myapp）。backend-service 的Service会完成如下两件重要的事情：会为Service创建一个本地集群的DNS入口，因此前端Pod只需要DNS查找主机名为 ‘backend-service’，就能够解析出前端应用程序可用的IP地址。现在前端已经得到了后台服务的IP地址，但是它应该访问2个后台Pod的哪一个呢？Service在这2个后台Pod之间提供透明的负载均衡，会将请求分发给其中的任意一个（如下面的动画所示）。通过每个Node上运行的代理（kube-proxy）完成。有一个特别类型的Kubernetes Service，称为’LoadBalancer’，作为外部负载均衡器使用，在一定数量的Pod之间均衡流量。比如，对于负载均衡Web流量很有用。 5、Volumes：A Volume is a directory on disk or in another container.由于Pod里面允许的容器在重启后，里面的数据会清除，用Volume可以避免这个问题，用来保存需要持久化的数据。 建议参考:http://kubernetes.iohttps://dzone.com/refcardz/kubernetes-essentialshttp://www.infoq.com/cn/articles/netflix-oss-spring-cloud-kuberneteshttps://rootsongjc.gitbooks.io/kubernetes-handbook/content/https://www.ibm.com/developerworks/cn/opensource/os-kubernetes-developer-guide/index.html Kubernetes ArchitectureCluster：分为Master Nodes，Worker Nodes. A Kubernetes cluster is a set of physical or virtual machines and other infrastructure resources that are used to run your applications. The machines that manage the cluster are called Master Nodes and the machines that run the containers are called Worker Nodes. NodeA Node is a physical or virtual machine. It has the necessary services to run application containers.A Worker Node runs tasks as delegated by the master. Each Worker Node can run multiple pods. Kubelet：Kubelet is a service running on each Node that manages containers and is managed by the master.Kubelet is a Kubernetes-internal concept and generally does not require direct manipulation. 持续集成与发布应用构建和发布流程说明： 用户向Gitlab提交代码，代码中必须包含Dockerfile 将代码提交到远程仓库 用户在发布应用时需要填写git仓库地址和分支、服务类型、服务名称、资源数量、实例个数，确定后触发Jenkins自动构建 Jenkins的CI流水线自动编译代码并打包成docker镜像推送到Harbor镜像仓库 Jenkins的CI流水线中包括了自定义脚本，根据我们已准备好的kubernetes的YAML模板，将其中的变量替换成用户输入的选项 生成应用的kubernetes YAML配置文件 更新Ingress的配置，根据新部署的应用的名称，在ingress的配置文件中增加一条路由信息 更新PowerDNS，向其中插入一条DNS记录，IP地址是边缘节点的IP地址。关于边缘节点，请查看边缘节点配置 Jenkins调用kubernetes的API，部署应用 DevOps真正践行DevOps，让开发人员在掌握自己的开发和测试环境，让环境一致，让开发效率提升，让运维没有堆积如山的tickets，让监控更加精准，从kubernetes平台开始。行动指南： 根据环境（比如开发、测试、生产）划分namespace，也可以根据项目来划分 再为每个用户划分一个namespace、创建一个serviceaccount和kubeconfig文件，不同namespace间的资源隔离，目前不隔离网络，不同namespace间的服务可以互相访问 创建yaml模板，降低编写kubernetes yaml文件编写难度 在kubectl命令上再封装一层，增加用户身份设置和环境初始化操作，简化kubectl命令和常用功能 管理员通过dashboard查看不同namespace的状态，也可以使用它来使操作更便捷 所有应用的日志统一收集到ElasticSearch中，统一日志访问入口 可以通过Grafana查看所有namespace中的应用的状态和kubernetes集群本身的状态 需要持久化的数据保存在分布式存储中，例如GlusterFS或Ceph中 与SpringCloud对比微服务关注方面有：配置管理、服务发现与负载平衡、弹性和失败冗余、API管理、安全服务、中央集中日志、集中测量、分布式跟踪、调度部署和自动扩展。 根据这些观点，得出Spring Cloud和Kubernetes两个平台映射：1.配置管理：配置服务器、Consul和Netflix Archaius（Spring Cloud）；Kubernetes ConfigMap&amp;Secrets ;2.服务发现: Netflix Eureka,Hashicorp Consul(Spring Cloud);Kubernetes Service&amp;Ingress Resource;3.负载平衡：Netflix Ribbon（Spring Cloud）；Kubernetes Service4.API网关：Netflix Zuul（SpringCloud）；Kubernetes Service&amp;Ingress Resource5.安全服务：SpringCloud Security6.中央集中日志：ELK Stack（LogStash）；EFKstack（Fluentd）。7.集中测量：Netflix Spectator&amp; Atlas；Heapster、Prometheus、Grafana。8.分布式跟踪：SpringCloud Sleuth，Zipkin；OpenTracing、Zipkin9.弹性和失败冗余：Netflix Hystrix、Turbine&amp;Ribbon；Kubernetes Health Check&amp;resource isolation10.自动扩展self Healing：Spring Cloud无；Kubernetes Health Check、SelfHealing、Autoscaling11.打包 部署和调度部署：Spring Boot；Docker／Rkt、Kubernetes Scheduler&amp;Deployment12.任务工作管理：Spring Batch；Kubernetes Jobs&amp;Scheduled Jobs13.单个应用：Spring Cloud Cluster ；Kubernetes Pods Spring Cloud有一套丰富的集成良好的Java库，作为应用程序栈一部分解决所有运行时问题。 因此，微服务本身通过库和运行时作为代理来执行客户端服务发现，负载平衡，配置更新，度量跟踪等。诸如单例集群服务和批处理作业的模式也在JVM中进行管理。 Kubernetes是多语言的，不仅针对Java平台，并以通用的方式为所有语言解决分布式计算的挑战。 它提供应用程序栈外部的配置管理，服务发现，负载平衡，跟踪，度量，单例，平台调度作业等平台级别功能。 该应用系统不需要任何库或代理程序用于客户端逻辑，它可以用任何语言编写。这两个平台是互补的，并且可以结合在一起，创造一个更加强大的解决方案,例如，Spring Boot提供了用于构建单个JAR应用程序包的Maven插件。结合Docker和Kubernetes的声明性部署和调度功能，使微服务运行变得轻而易举。 类似地，Spring Cloud具有应用程序库，用于使用Hystrix（断路器）和Ribbon（用于负载平衡）创建弹性的，容错的微服务。 但是单单这是不够的，当它与Kubernetes的健康检查，进程重新启动和自动扩展功能相结合时，微服务成为一个真正的抗脆弱的系统。]]></content>
      <categories>
        <category>MicroService</category>
        <category>DevOps</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>msa</tag>
        <tag>架构</tag>
        <tag>DevOps</tag>
        <tag>Docker</tag>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构之SpringCloud全家桶]]></title>
    <url>%2F2017%2F12%2F21%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E4%B9%8BSpringCloud%E5%85%A8%E5%AE%B6%E6%A1%B6%2F</url>
    <content type="text"><![CDATA[What’s SpringCloud?SpringCloud是基于微服务理念，并在基于Netflix OSS的基础上开发的一套全生态的微服务治理框架。模块众多:如Eureka服务发现与注册(CAP中的AP模式)，Hystrix断路器，Sleuth分布式链路追踪，Zuul统一api网关，Config配置中心，Ribbon客户端负载均衡，Sidecar异构系统集成等等。SpringCloud中的微服务通常需要运行在Docker容器中，而Docker是一个类似于虚拟机的虚拟化主机环境，提供独立、快速发布、运行的能力。Kubernetes是Google开源的容器集群管理系统，其提供应用部署、维护、 扩展机制等功能，利用Kubernetes能方便地管理跨机器运行容器化的应用。值得一提的是SpringCloud是基于SpringBoot的. SpringCloud vs Dubbo我们选取几个指标进行比较：开发团队背景，社区活跃度，平台架构(功能齐全度，通信协议，文档质量)，技术发展，及国内外应用现状 1、开发团队背景：Alibaba VS Spring Source + Pivotal + Netfix阿里巴巴:BAT三座大山之一，是中国最大的互联网电商平台，旗下还有个全国最大的云服务提供品牌的阿里云。技术实力也是非常雄厚的:除了为Apache捐过几个顶级项目外，还有很多国内特别流行的开源库，如Dubbo,Druid,Fastjson,Weex,Antd等，同时还持续为Linux内核贡献代码。SpringSource:JavaEE事实标准的建立和主导者Pivotal:由EMC和VMware联合成立的公司.是一家下一代云计算和大数据应用相结合的公司.其下有Cloud Foundry公有云，Redis,SpringSource,RabbitMQ等多个超牛逼开源产品。目前归属于Dell(EMC已被Dell收购)。Netfix:是一家在线影片租赁提供商，主要提供Netflix超大数量的DVD并免费递送。Netflix也是被业界称为拥有最优秀的微服务实践经验的公司。 总之，都很牛逼，分个高低就很难了。不过有一点是比较明确的，阿里巴巴虽然开源了很多产品，但其在国内开源界名声不是特别好，因为其开源的产品大都是以KPI驱动的，所以导致很难有长期的维护。而国外的开源氛围和精神就很好。 2、社区活跃度：Dubbo与2013年左右停止维护，2017年下半年重启维护。也就是说，社区基本上从火爆到清冷，现在重启维护，社区活跃度也令人堪忧，很多公司都已经或正在考虑迁移到SpringCloud。SpringCloud:有SpringSource的社区优势和可信度，社区也是非常活跃的.举例来说，但就spring-cloud-netflix这核心模块，就有2000多次commit，123个contributors.而dubbo这么多年来总共才2000次commit，45个contributors. 3、平台架构Dubbo算是Spring Cloud的一个子集好了,大致相当于Spring Cloud里的 Eureka + Feign + 1/2Hystrix先对比下功能: Dubbo是一套RPC的半完善解决方案，配套的软件基础设施不全，好处是编码环节基本没有侵入性。问题定位、熔断和监控方面的问题让人没有那么的放心。Dubbo实践通常以ZooKeeper为注册中心。针对分布式领域著名的CAP理论（C——数据一致性，A——服务可用性，P——服务对网络分区故障的容错性），Zookeeper 保证的是CP ，但对于服务发现而言，可用性比数据一致性更加重要 ，而 Eureka 设计则遵循AP原则 。 RPC vs RESTRPC优点：性能比REST高。微服务提倡轻量级通信协议，并能够跨语言，而RESTful符合这一点，RPC有点违背此理念，比较难以实现异构系统。RPC服务提供方与调用方接口依赖方式太强。而微服务强调松耦合，微服务间独立开发，互不影响。而REST接口相比RPC更为轻量化，服务提供方和调用方的依赖只是依靠一纸契约，不存在代码级别的强依赖。当然REST接口也有痛点，因为接口定义过轻，很容易导致定义文档与实际实现不一致导致服务集成时的问题，但是该问题很好解决，只需要通过每个服务整合swagger，让每个服务的代码与文档一体化，就能解决。(如果还是会存在问题，可以采用SpringCloud基于CDC理念开发的Contract模块)。 文档质量：都很全。 4、技术发展：由于Dubbo期间停止维护了几年，现在重启维护，但前景并不明朗，说不定哪天又会停止维护。SpringCloud目前正处于告诉发展期，而且Spring生态开源精神是不容置疑的。总之，Dubbo以前确实很牛逼，但SpringCloud代表未来。 5、国内外应用现状：Dubbo在国内一直是主流，但由于其开源态度问题，已经或正在流水很多开发者或公司。比如阿里巴巴内部已经不再采用Dubbo了，而且之前网易基于Dubbo开发了Dubbox,但网易也已经开始转向SpringCloud阵营了。虽然有流失，但流行程度仍然很高，就类似于xp。国外采用情况不详。SpringCloud国外是主流，国内也正在普及，应用案例如：网易、华为、饿了么、DaoCloud、拍拍贷、新浪等待。 SpringCloud Config随着线上项目变的日益庞大，每个项目都散落着各种配置文件，如果采用分布式的开发模式，需要的配置文件随着服务增加而不断增多。某一个基础服务信息变更，都会引起一系列的更新和重启，运维苦不堪言也容易出错。配置中心便是解决此类问题的灵丹妙药。 配置中心提供的核心功能： 集中管理各环境的配置文件 配置文件修改之后，可以实时快速的生效 可以进行版本管理 Spring Cloud Config项目是一个解决分布式系统的配置管理方案。它包含了Client和Server两个部分，server提供配置文件的存储、以接口的形式将配置文件的内容提供出去，client通过接口获取数据、并依据此数据初始化自己的应用。Spring cloud使用git或svn存放配置文件，默认情况下使用git。仓库中的配置文件会被转换成web接口，访问可以参照以下的规则：12345/&#123;application&#125;/&#123;profile&#125;[/&#123;label&#125;]/&#123;application&#125;-&#123;profile&#125;.yml/&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.yml/&#123;application&#125;-&#123;profile&#125;.properties/&#123;label&#125;/&#123;application&#125;-&#123;profile&#125;.properties spring.application.name：对应{application}部分spring.cloud.config.profile：对应{profile}部分spring.cloud.config.label：对应git的分支。 RefreshSpring Cloud Config分服务端和客户端，服务端负责将git（svn）中存储的配置文件发布成REST接口，客户端可以从服务端REST接口获取配置。但客户端并不能主动感知到配置的变化，从而主动去获取新的配置。客户端如何去主动获取新的配置信息呢，springcloud已经给我们提供了解决方案，每个客户端通过POST方法触发各自的/refresh。curl -X POST http://localhost:8002/refresh每次手动刷新客户端也很麻烦，有没有什么办法只要提交代码就自动调用客户端来更新呢，github/gitlab的webhook是一个好的办法。但是当客户端越来越多的时候hook支持的已经不够优雅，另外每次增加客户端都需要改动hook也是不现实的。使用Spring Cloud Bus可以完美解决这一问题。 所以最终方案：SpringCloud Config+SpringCloud Bus+Gitlab WebHook优雅地解决刷新问题： 这时Spring Cloud Bus做配置更新步骤如下:1、提交代码触发WebHooK post请求给/bus/refresh2、server端接收到请求并发送给Spring Cloud Bus3、Spring Cloud bus接到消息并通知给其它客户端4、其它客户端接收到通知，请求Server端获取最新配置5、全部客户端均获取到最新的配置6、注解为@RefreshScope的类中的@Value都将得到刷新。1234567891011@RestController@RefreshScopepublic class ConfigClientController &#123; @Value("$&#123;profile&#125;") private String profile; @GetMapping("/profile") public String hello() &#123; return this.profile; &#125;&#125; 配置:bootstrap.yml,而非application.yml12345678910111213spring: application: name: microservice-foo # 对应config server所获取的配置文件的&#123;application&#125; cloud: config: uri: http://localhost:8080/ profile: dev # profile对应config server所获取的配置文件中的&#123;profile&#125; label: master # 指定Git仓库的分支，对应config server所获取的配置文件的&#123;label&#125; rabbitmq: # SpringCloud Bus需要用到AMQP来广播消息 host: localhost port: 5672 username: guest password: guest 那么，可不可以支持局部刷新?某些场景下（例如灰度发布），我们可能只想刷新部分微服务的配置，此时可通过/bus/refresh端点的destination参数来定位要刷新的应用程序。例如：/bus/refresh?destination=customers:8000，这样消息总线上的微服务实例就会根据destination参数的值来判断是否需要要刷新。其中，customers:8000指的是各个微服务的ApplicationContext ID。destination参数也可以用来定位特定的微服务。例如：/bus/refresh?destination=customers:**，这样就可以触发customers微服务所有实例的配置刷新。 SpringCloud Config还支持的功能： 属性自动加密解密：当属性名是:{cipher}开头时，如{cipher}*，就代表它是加密过的。config server在获取该属性发送到客户端之前是会进行解密。1234spring: datasource: username: dbuser password: &apos;&#123;cipher&#125;FKSAJDFGYOS8F7GLHAKERGFHLSAJ&apos; 前提条件：1、添加Spring Security RSA依赖，2、确保JCE在JDK中；3、确保有效的公钥在应用中。同时config server提供方便的/encrypt and /decrypt端点，1234$ curl localhost:8888/encrypt -d mysecret682bc583f4641835fa2db009355293665d2647dade3375c0ee201de2a49f7bda$ curl localhost:8888/decrypt -d 682bc583f4641835fa2db009355293665d2647dade3375c0ee201de2a49f7bdamysecret SpringCloud Netflix EurekaEureka是Netflix开源的一款提供服务注册和发现的产品，它提供了完整的Service Registry和Service Discovery实现。类似与ZooKeeper。通过服务中心来获取服务你不需要关注你调用的项目IP地址，由几台服务器组成，每次直接去服务中心获取可以使用的服务去调用即可。由于各种服务都注册到了服务中心，就有了去做很多高级功能条件。比如几台服务提供相同服务来做均衡负载；监控服务器调用成功率来做熔断，移除服务列表中的故障点；监控服务调用时间来对不同的服务器设置不同的权重等等。 CAP理论：在分布式系统领域有个著名的CAP(Consistency、Availablity和Partition Tolerance)定理：C——数据一致性，A——服务可用性，P——服务对网络分区故障的容错性。这三个特性在任何分布式系统中不能同时满足，最多同时满足两个。 ZoopKeeper是基于CP原则构建的:Zookeeper保证的是CP，即任何时刻对Zookeeper的访问请求能得到一致的数据结果，同时系统对网络分割具备容错性，但是它不能保证每次服务请求的可用性。从实际情况来分析，在使用Zookeeper获取服务列表时，如果zookeeper正在选主，或者Zookeeper集群中半数以上机器不可用，那么将就无法获得数据了。ZK有一个Leader，而且在Leader无法使用的时候通过Paxos（ZAB）算法选举出一个新的Leader。这个Leader的目的就是保证写信息的时候只向这个Leader写入，Leader会同步信息到Followers。这个过程就可以保证数据的一致性。但是对于服务发现场景来说，情况就不太一样了：针对同一个服务，即使注册中心的不同节点保存的服务提供者信息不尽相同，也并不会造成灾难性的后果。因为对于服务消费者来说，能消费才是最重要的——拿到可能不正确的服务实例信息后尝试消费一下，也好过因为无法获取实例信息而不去消费。所以，对于服务发现而言，可用性比数据一致性更加重要——AP胜过CP。 Eureka是基于AP原则构建的:对比下ZK，Eureka不用选举一个Leader。每个Eureka服务器单独保存服务注册地址，Eureka也没有Leader的概念。这也因此产生了无法保证每个Eureka服务器都保存一致数据的特性。当Eureka与注册者心跳无法保持的时候，依然保存注册列表的信息很长一段时间。当然了，客户端中必须用有效的机制屏蔽坏掉的服务器，这个Spring Cloud中的体现是Ribbon。 Eureka因为没有选举过程来选举Leader，因此写的信息可以独立进行。因此有可能出现数据信息不一致的情况。但是当网络出现问题的时候，每台服务器也可以完成独立的服务。当然了，一些客户端的负载平衡和Fail Over机制需要来辅助完成额外的功能。 关于一致性：一致性是指从系统外部读取系统内部的数据时，在一定约束条件下相同，即数据变动在系统内部各节点应该是同步的。根据一致性的强弱程度不同，可以将一致性级别分为如下几种： 强一致性（strong consistency）。任何时刻，任何用户都能读取到最近一次成功更新的数据。 单调一致性（monotonic consistency）。任何时刻，任何用户一旦读到某个数据在某次更新后的值，那么就不会再读到比这个值更旧的值。也就是说，可获取的数据顺序必是单调递增的。 会话一致性（session consistency）。任何用户在某次会话中，一旦读到某个数据在某次更新后的值，那么在本次会话中就不会再读到比这值更旧的值 会话一致性是在单调一致性的基础上进一步放松约束，只保证单个用户单个会话内的单调性，在不同用户或同一用户不同会话间则没有保障。示例case：php的session概念。 最终一致性（eventual consistency）。用户只能读到某次更新后的值，但系统保证数据将最终达到完全一致的状态，只是所需时间不能保障。(幂等) 弱一致性（weak consistency）。用户无法在确定时间内读到最新更新的值。 Eureka高可用集群Eureka Server可以运行多个实例来构建集群，解决单点问题，但不同于ZooKeeper的选举leader的过程，Eureka Server采用的是Peer to Peer对等通信。这是一种去中心化的架构，无master/slave区分，每一个Peer都是对等的。在这种架构中，节点通过彼此互相注册来提高可用性，每个节点需要添加一个或多个有效的serviceUrl指向其他节点。每个节点都可被视为其他节点的副本。 如果某台Eureka Server宕机，Eureka Client的请求会自动切换到新的Eureka Server节点，当宕机的服务器重新恢复后，Eureka会再次将其纳入到服务器集群管理之中。当节点开始接受客户端请求时，所有的操作都会进行replicateToPeer（节点间复制）操作，将请求复制到其他Eureka Server当前所知的所有节点中。 一个新的Eureka Server节点启动后，会首先尝试从邻近节点获取所有实例注册表信息，完成初始化。并且会通过心跳续约的方式定期更新。默认配置下，如果Eureka Server在一定时间内没有接收到某个服务实例的心跳，Eureka Server将会注销该实例（默认为90秒，通过eureka.instance.lease-expiration-duration-in-seconds配置）。当Eureka Server节点在短时间内丢失过多的心跳时（比如发生了网络分区故障），那么这个节点就会进入自我保护模式 Eureka架构Eureka包含两个组件：Eureka Server 和 Eureka Client，它们的作用如下：Eureka Client是一个Java客户端，用于简化与Eureka Server的交互；Eureka Server提供服务发现的能力，各个微服务启动时，会通过Eureka Client向Eureka Server进行注册自己的信息，Eureka Server会存储该服务的信息；微服务启动后，会周期性地向Eureka Server发送心跳（默认周期为30秒）以续约自己的信息。如果Eureka Server在一定时间内没有接收到某个微服务节点的心跳，Eureka Server将会注销该微服务节点（默认90秒）；每个Eureka Server同时也是Eureka Client，多个Eureka Server之间通过复制的方式完成服务注册表的同步；Eureka Client会缓存Eureka Server中的信息。即使所有的Eureka Server节点都宕掉，服务消费者依然可以使用缓存中的信息找到服务提供者。综上，Eureka通过心跳检测、健康检查和客户端缓存等机制，提高了系统的灵活性、可伸缩性和可用性。 监控界面Eureka为我们提供了监控界面来监控服务的监控程度。 Eureka的自我保护模式如果在Eureka Server的首页看到以下这段提示，则说明Eureka已经进入了保护模式。 EMERGENCY! EUREKA MAY BE INCORRECTLY CLAIMING INSTANCES ARE UP WHEN THEY’RE NOT. RENEWALS ARE LESSER THAN THRESHOLD AND HENCE THE INSTANCES ARE NOT BEING EXPIRED JUST TO BE SAFE.保护模式主要用于一组客户端和Eureka Server之间存在网络分区场景下的保护。一旦进入保护模式，Eureka Server将会尝试保护其服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。详见：https://github.com/Netflix/eureka/wiki/Understanding-Eureka-Peer-to-Peer-Communication默认情况下，如果Eureka Server在一定时间内没有接收到某个微服务实例的心跳，Eureka Server将会注销该实例（默认90秒）。但是当网络分区故障发生时，微服务与Eureka Server之间无法正常通信，以上行为可能变得非常危险了——因为微服务本身其实是健康的，此时本不应该注销这个微服务。Eureka通过“自我保护模式”来解决这个问题——当Eureka Server节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。一旦进入该模式，Eureka Server就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。当网络故障恢复后，该Eureka Server节点会自动退出自我保护模式。综上，自我保护模式是一种应对网络异常的安全保护措施。它的架构哲学是宁可同时保留所有微服务（健康的微服务和不健康的微服务都会保留），也不盲目注销任何健康的微服务。使用自我保护模式，可以让Eureka集群更加的健壮、稳定。在Spring Cloud中，可以使用eureka.server.enable-self-preservation = false 禁用自我保护模式。 开发、测试时如何解决Eureka注册服务慢的问题?使用配置项：eureka.instance.leaseRenewalIntervalInSeconds作为实例还涉及到与注册中心的周期性心跳，默认持续时间为30秒（通过serviceUrl）。在实例、服务器、客户端都在本地缓存中具有相同的元数据之前，服务不可用于客户端发现（所以可能需要3次心跳）。你可以使用eureka.instance.leaseRenewalIntervalInSeconds 配置，这将加快客户端连接到其他服务的过程。在生产中，最好坚持使用默认值，因为在服务器内部有一些计算，他们对续约做出假设。 如何解决Eureka Server不踢出已关停的节点的问题在开发过程中，我们常常希望Eureka Server能够迅速有效地踢出已关停的节点，但是新手由于Eureka自我保护模式，以及心跳周期长的原因，常常会遇到Eureka Server不踢出已关停的节点的问题。解决方法如下： (1) Eureka Server端：配置关闭自我保护，并按需配置Eureka Server清理无效节点的时间间隔。eureka.server.enable-self-preservation # 设为false，关闭自我保护eureka.server.eviction-interval-timer-in-ms # 清理间隔（单位毫秒，默认是60*1000）(2) Eureka Client端：配置开启健康检查，并按需配置续约更新时间和到期时间。eureka.client.healthcheck.enabled # 开启健康检查（需要spring-boot-starter-actuator依赖）eureka.instance.lease-renewal-interval-in-seconds # 续约更新时间间隔（默认30秒）eureka.instance.lease-expiration-duration-in-seconds # 续约到期时间（默认90秒）注意：更改Eureka更新频率将打破服务器的自我保护功能，生产环境下不建议自定义这些配置。详见：https://github.com/spring-cloud/spring-cloud-netflix/issues/373 注意点：eureka.client.healthcheck.enabled=true配置项必须设置在application.yml中eureka.client.healthcheck.enabled=true 只应该在application.yml中设置。如果设置在bootstrap.yml中将会导致一些不良的副作用，例如在Eureka中注册的应用名称是UNKNOWN等。 SpringCloud Netflix RibbonSpring Cloud Ribbon是基于Netflix Ribbon实现的一套客户端负载均衡的工具。它是一个基于HTTP和TCP的客户端负载均衡器。它可以通过在客户端中配置ribbonServerList或者使用Eureka服务发现来设置服务端列表去轮询访问以达到均衡负载的作用。目前已经被集合进Feign中了，平常我们直接使用Feign就可以了。它主要包括六个组件： ServerList，负载均衡使用的服务器列表。这个列表会缓存在负载均衡器中，并定期更新。当Ribbon与Eureka结合使用时，ServerList的实现类就是DiscoveryEnabledNIWSServerList，它会保存Eureka Server中注册的服务实例表。 ServerListFilter，服务器列表过滤器。这是一个接口，主要用于对Service Consumer获取到的服务器列表进行预过滤，过滤的结果也是ServerList。Ribbon提供了多种过滤器的实现。 IPing，探测服务实例是否存活的策略。 IRule，负载均衡策略，其实现类表述的策略包括：轮询、随机、根据响应时间加权等，其类结构如下图所示。 我们也可以自己定义负载均衡策略，比如我们就利用自己实现的策略，实现了服务的版本控制和直连配置。实现好之后，将实现类重新注入到Ribbon中即可。 ILoadBalancer，负载均衡器。这也是一个接口，Ribbon为其提供了多个实现，比如ZoneAwareLoadBalancer。而上层代码通过调用其API进行服务调用的负载均衡选择。一般ILoadBalancer的实现类中会引用一个IRule。 RestClient，服务调用器。顾名思义，这就是负载均衡后，Ribbon向Service Provider发起REST请求的工具。 Ribbon工作时会做四件事情： 优先选择在同一个Zone且负载较少的Eureka Server； 定期从Eureka更新并过滤服务实例列表； 根据用户指定的策略，在从Server取到的服务注册列表中选择一个实例的地址； 通过RestClient进行服务调用。 SpringCloud Netflix FeignSpring Cloud Feign是一套基于Netflix Feign实现的声明式服务调用客户端。它使得编写Web服务客户端变得更加简单。类似于RPC调用。我们只需要通过创建接口并用注解来配置它既可完成对Web服务接口的绑定。它具备可插拔的注解支持，包括Feign注解、JAX-RS注解。它也支持可插拔的编码器和解码器。Spring Cloud Feign还扩展了对Spring MVC注解的支持，同时还整合了Ribbon和Eureka来提供均衡负载的HTTP客户端实现。 由于Feign是基于Ribbon实现的，所以它自带了客户端负载均衡功能，也可以通过Ribbon的IRule进行策略扩展。另外，Feign还整合的Hystrix来实现服务的容错保护，不过Feign的Hystrix默认是关闭的。你可以通过配置来开启1feign.hystrix.enabled=true Feign具有如下特性： 可插拔的注解支持，包括Feign注解和JAX-RS注解、Spring MVC注解 支持可插拔的HTTP编码器和解码器 支持Hystrix和它的Fallback 支持Ribbon的负载均衡 支持HTTP请求和响应的压缩 123456789101112131415161718192021222324252627282930@FeignClient("stores")public interface StoreClient &#123; @RequestMapping(method = RequestMethod.GET, value = "/stores") List&lt;Store&gt; getStores(); @RequestMapping(method = RequestMethod.POST, value = "/stores/&#123;storeId&#125;", consumes = "application/json") Store update(@PathVariable("storeId") Long storeId, Store store);&#125;/*** Feign fallback支持*/@FeignClient(name = "microservice-provider-user", fallback = FeignClientFallback.class)public interface UserFeignClient &#123; @RequestMapping(value = "/&#123;id&#125;", method = RequestMethod.GET) public User findById(@PathVariable("id") Long id);&#125;/** * 回退类FeignClientFallback需实现Feign Client接口 */@Componentclass FeignClientFallback implements UserFeignClient &#123; @Override public User findById(Long id) &#123; User user = new User(); user.setId(-1L); user.setUsername("默认用户"); return user; &#125;&#125; 12345678910@RestControllerpublic class MovieController &#123; @Autowired private UserFeignClient userFeignClient; @GetMapping("/user/&#123;id&#125;") public User findById(@PathVariable Long id) &#123; return this.userFeignClient.findById(id); &#125;&#125; Q&amp;A如何使用Feign构造多参数的请求1、原生参数+@RequestParam2、map +@RequestParam3、post + @ResponseBody Spring Cloud中，Feign和Ribbon在整合了Hystrix后，可能会出现首次调用失败的问题，要如何解决该问题呢？Hystrix默认的超时时间是1秒，如果超过这个时间尚未响应，将会进入fallback代码。而首次请求往往会比较慢（因为Spring的懒加载机制，要实例化一些类），这个响应时间可能就大于1秒了。解决方案有三种。方法一：hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds: 5000该配置是让Hystrix的超时时间改为5秒方法二：hystrix.command.default.execution.timeout.enabled: false该配置，用于禁用Hystrix的超时时间方法三：feign.hystrix.enabled: false该配置，用于索性禁用feign的hystrix。该做法除非一些特殊场景，不推荐使用。 SpringCloud Netflix Hystrix雪崩效应:在微服务架构中通常会有多个服务层调用，基础服务的故障可能会导致级联故障，进而造成整个系统不可用的情况，这种现象被称为服务雪崩效应。服务雪崩效应是一种因“服务提供者”的不可用导致“服务消费者”的不可用,并将不可用逐渐放大的过程。如果下图所示：A作为服务提供者，B为A的服务消费者，C和D是B的服务消费者。A不可用(网络延迟，服务出现不稳定，响应缓慢或者掉调用超时，宕机等)引起了B的不可用(系统的线程和可用连接数用完)，并将不可用像滚雪球一样放大到C和D时。导致新的请求进不来，服务僵死，这便是故障传递。最终形成雪崩效应/多米诺骨牌效应。使得整个集群都不能对外提供服务。 断路器/熔断器（CircuitBreaker）断路器的原理很简单，如同电力过载保护器。它可以实现快速失败(fast-fail)，如果它在一段时间内侦测到许多类似的错误，会强迫其以后的多个调用快速失败，不再访问远程服务器，从而防止应用程序不断地尝试执行可能会失败的操作，使得应用程序继续执行而不用等待修正错误，或者浪费CPU时间去等到长时间的超时产生。断路器也可以使应用程序能够诊断错误是否已经修正，如果已经修正，应用程序会再次尝试调用操作。Hystrix对其实现分为三部分:熔断，fallback，资源隔离 1、熔断： 这里涉及到断路器的三个重要参数：快照时间窗、请求总数下限、错误百分比下限。这个参数的作用分别是： 快照时间窗：断路器确定是否打开需要统计一些请求和错误数据，而统计的时间范围就是快照时间窗，默认为最近的10秒。 请求总数下限：在快照时间窗内，必须满足请求总数下限才有资格根据熔断。默认为20，意味着在10秒内，如果该hystrix命令的调用此时不足20次，即时所有的请求都超时或其他原因失败，断路器都不会打开。 错误百分比下限：当请求总数在快照时间窗内超过了下限，比如发生了30次调用，如果在这30次调用中，有16次发生了超时异常，也就是超过50%的错误百分比，在默认设定50%下限情况下，这时候就会将断路器打开。 当断路器打开，对主逻辑进行熔断之后，hystrix会启动一个休眠时间窗，在这个时间窗内，降级逻辑是临时的成为主逻辑，当休眠时间窗到期，断路器将进入半开状态，释放一次请求到原来的主逻辑上，如果此次请求正常返回，那么断路器将继续闭合，主逻辑恢复，如果这次请求依然有问题，断路器继续进入打开状态，休眠时间窗重新计时。 2、FallbackFallback相当于是降级操作(前面Feign时已有提到). 如对于查询操作, 我们可以实现一个fallback方法, 当请求后端服务出现异常的时候, 可以使用fallback方法返回的值. fallback方法的返回值一般是设置的默认值或者来自缓存. 3、资源隔离：两种模式:多个线程池隔离模式和信号量隔离模式 在Hystrix中, 主要通过线程池来实现资源隔离. 通常在使用的时候我们会根据调用的远程服务划分出多个线程池. 例如调用产品服务的Command放入A线程池, 调用账户服务的Command放入B线程池. 这样做的主要优点是运行环境被隔离开了. 这样就算调用服务的代码存在bug或者由于其他原因导致自己所在线程池被耗尽时, 不会对系统的其他服务造成影响.但是带来的代价就是维护多个线程池会对系统带来额外的性能开销.如果是对性能有严格要求而且确信自己调用服务的客户端代码不会出问题的话, 可以使用Hystrix的信号模式(Semaphores)来隔离资源.与线程隔离最大不同在于执行依赖代码的线程依然是请求线程（该线程需要通过信号申请）,如果客户端是可信的且可以快速返回，可以使用信号隔离替换线程隔离,降低开销. 因为熔断只是作用在服务调用这一端，而Feign已经依赖了Hystrix。故而配置好相关策略后，仅仅使用Feign和书写fallback类或方法即可。 (前面Feign时已提及) 隔离分析：http://hot66hot.iteye.com/blog/2155036 Hystrix-dashboard &amp; TurbineHystrix-dashboard是一款针对Hystrix进行实时监控的工具，通过Hystrix Dashboard我们可以在直观地看到各Hystrix Command的请求响应时间, 请求成功率等数据。但是只使用Hystrix Dashboard的话, 你只能看到单个应用内的服务信息, 这明显不够. 我们需要一个工具能让我们汇总系统内多个服务的数据并显示到Hystrix Dashboard上, 这个工具就是Turbine. Turbine是聚合服务器发送事件流数据的一个工具，用来监控集群下hystrix的metrics情况。 Delay：该参数用来控制服务器上轮询监控信息的延迟时间，默认为2000毫秒，我们可以通过配置该属性来降低客户端的网络和CPU消耗。们可以在监控信息的左上部分找到两个重要的图形信息：一个实心圆和一条曲线。实心圆：共有两种含义。它通过颜色的变化代表了实例的健康程度，如下图所示，它的健康度从绿色、黄色、橙色、红色递减。该实心圆除了颜色的变化之外，它的大小也会根据实例的请求流量(request traffic)发生变化，流量越大该实心圆就越大。所以通过该实心圆的展示，我们就可以在大量的实例中快速的发现故障实例和高压力实例。曲线：用来记录2分钟内流量的相对变化，我们可以通过它来观察到流量的上升和下降趋势。 SpringCloud SleuthSpring Cloud Sleuth 主要功能就是在分布式系统中提供追踪解决方案.spring cloud sleuth可以结合zipkin，将信息发送到zipkin，利用zipkin的存储来存储信息，利用zipkin ui来展示数据。同时也可以只是简单的将数据记在日志中。 提供链路追踪。通过sleuth可以很清楚的看出一个请求都经过了哪些服务。可以很方便的理清服务间的调用关系。 可视化错误。对于程序未捕捉的异常，可以在zipkin界面上看到。 分析耗时。通过sleuth可以很方便的看出每个采样请求的耗时，分析出哪些服务调用比较耗时。当服务调用的耗时随着请求量的增大而增大时，也可以对服务的扩容提供一定的提醒作用。 优化链路。对于频繁地调用一个服务，或者并行地调用等，可以针对业务做一些优化措施。 服务追踪分析:微服务架构上通过业务来划分服务的，通过REST调用，对外暴露的一个接口，可能需要很多个服务协同才能完成这个接口功能，如果链路上任何一个服务出现问题或者网络超时，都会形成导致接口调用失败。随着业务的不断扩张，服务之间互相调用会越来越复杂。 术语:Span：基本工作单元，span通过一个64位ID唯一标识，trace以另一个64位ID表示，span还有其他数据信息，比如摘要、时间戳事件、键值对注释、span的ID、以及span父ID等span在不断的启动和停止，同时记录了时间信息，当你创建了一个span，你必须在未来的某个时刻停止它。初始化span称为”root span”,该span的id和trace的id相等。Trace：一系列spans组成的一个树状结构(root span为根共享),trace也用一个64位的id唯一标识，trace中的所有span都共享该trace id。Annotation：用来及时记录一个事件的存在，一些核心annotations用来定义一个请求的开始和结束 cs - Client Sent -客户端发起一个请求，这个annotion描述了这个span的开始 sr - Server Received -服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳，便可得到网络延迟 ss - Server Sent -注解表明请求处理的完成(当请求返回客户端)，如果ss减去sr时间戳，便可得到服务端需要的处理请求所需的时间 cr - Client Received -表明span的结束，客户端成功接收到服务端的回复，如果cr减去cs时间戳，便可得到客户端从服务端获取回复的所有所需时间 这个过程中span的父子关系： Sleuth可以配合ELK使用，来查询分析跟踪日志。sleuth可以配合Zipkin使用。Zipkin时Twitter开源的分布式跟踪系统，主要功能是收集系统的时序数据，从而跟踪微服务架构的系统延时等问题。Zipkin还提供了一个非常与友好的界面，来帮助分析跟踪数据。 ZipKin还可以分析微服务间的依赖关系 还可以看Span的详细分析界面： 改进：解耦微服务与zipkinserversleuth+sleuth-zipkin-stream+MQ+Elasticsearch(此处作为存储,默认zipkin的数据是放在内存中的) SpringCloud Netflix Zuul提示:最近新出了一个SpringCloud-gateway，值得关注，不过问题还挺多的。 外部的应用如何来访问内部各种各样的微服务呢？在微服务架构中，后端服务往往不直接开放给调用端，而是通过一个API网关根据请求的url，路由到相应的服务。当添加API网关后，在第三方调用端和服务提供方之间就创建了一面墙，这面墙直接与调用方通信进行权限控制后将请求均衡分发给后台服务端。有了api gateway之后，一些与业务关系并不大的通用处理逻辑可以从api gateway中剥离出来，api gateway仅仅负责服务的编排与结果的组装。 为什么需要API Gateway?1、简化客户端调用复杂度2、数据裁剪以及聚合通常而言不同的客户端对于显示时对于数据的需求是不一致的，比如手机端或者Web端又或者在低延迟的网络环境或者高延迟的网络环境。因此为了优化客户端的使用体验，API Gateway可以对通用性的响应数据进行裁剪以适应不同客户端的使用需求。同时还可以将多个API调用逻辑进行聚合，从而减少客户端的请求数，优化客户端用户体验.3、多渠道支持当然我们还可以针对不同的渠道和客户端提供不同的API Gateway,对于该模式的使用由另外一个大家熟知的方式叫Backend for front-end(BFF), 在Backend for front-end模式当中，我们可以针对不同的客户端分别创建其BFF.4、遗留系统的微服务化改造 zuul 是netflix开源的一个API Gateway 服务器, 本质上是一个web servlet应用。Zuul 是提供动态路由，监控，弹性，安全等边缘功能的框架。Zuul的主要功能是路由转发和过滤器。路由功能是微服务的一部分，比如／api/user转发到到user服务，/api/shop转发到到shop服务。zuul默认和Ribbon结合实现了负载均衡的功能。 Zuul可以通过过滤机制，从而实现以下各项功能： 验证与安全保障(默认未提供): 识别面向各类资源的验证要求并拒绝那些与要求不符的请求。 审查与监控: 在边缘位置追踪有意义数据及统计结果，从而为我们带来准确的生产状态结论。 动态路由(默认未提供): 以动态方式根据需要将请求路由至不同后端集群处。 压力测试(默认未提供): 逐渐增加指向集群的负载流量，从而计算性能水平。 负载分配: 为每一种负载类型分配对应容量，并弃用超出限定值的请求。 静态响应处理: 在边缘位置直接建立部分响应，从而避免其流入内部集群。默认情况下，Zuul会代理所有注册到Eureka Server的微服务，并且Zuul的路由规则如下：http://ZUUL_HOST:ZUUL_PORT/serviceId/** 会被转发到特定的微服务下.但支持的路由规则非常丰富。 Zuul存在的问题，包括：性能问题：没有Nginx牛逼，所以一般结合Nginx来使用。WebSocket的支持问题： Zuul中并不直接提供对WebSocket的支持，需要添加额外的过滤器实现对WebSocket的支持；为了解决以上问题，可以通过在Zuul前端部署Nginx实现对Zuul实例的反向代理，同时适当的通过添加Cache以及请求压缩减少对后端Zuul实例的压力。 注意:Zuul通过Eureka是支持集群高可用模式的。 Q&amp;A:1、会话保持问题通过跟踪一个HTTP请求经过Zuul到具体服务，再到返回结果的全过程。我们很容易就能发现，在传递的过程中，HTTP请求头信息中的Cookie和Authorization都没有被正确地传递给具体服务，所以最终导致会话状态没有得到保持的现象。那么这些信息是在哪里丢失的呢？解决该问题的思路也很简单，我们只需要通过设置sensitiveHeaders即可，设置方法分为两种：全局设置：zuul.sensitive-headers=指定路由设置：zuul.routes.&lt; routeName&gt;.sensitive-headers=zuul.routes.&lt; routeName&gt;.custom-sensitive-headers=true 2、重定向问题在使用Spring Cloud Zuul对接Web网站的时候，处理完了会话控制问题之后。往往我们还会碰到如下图所示的问题，我们在浏览器中通过Zuul发起了登录请求，该请求会被路由到某WebSite服务，该服务在完成了登录处理之后，会进行重定向到某个主页或欢迎页面。此时，仔细的开发者会发现，在登录完成之后，我们浏览器中URL的HOST部分发生的改变，该地址变成了具体WebSite服务的地址了。解决：zuul.add-host-header=true。 SpringCloud StreamSpring Cloud Stream提供了很多抽象和基础组件来简化消息驱动型微服务应用。包含以下内容： 绑定broker抽象 持久化发布／订阅支持 消费者组支持 分区支持（Partitioning Support） Spring Cloud Stream提供对 Kafka, Rabbit MQ的绑定实现。Spring Cloud Stream 最大的方便之处，莫过于抽象了事件驱动的一些概念，对于消息中间件的进一步封装，可以做到代码层面对中间件的无感知，甚至于动态的切换中间件，切换topic。使得微服务开发的高度解耦，服务可以关注更多自己的业务流程。 引用了发布-订阅、消费组、分区的三个核心概念。 1、发布-订阅模式：当一条消息被投递到消息中间件之后，所有订阅次消息的Consumer都可以消费到这条消息。RabbitMQ里是通过Topic的Exchange来实现的。Kafka里是通过Topic会被每个ConsumerGroup消费一次这个特性决定的。 2、消费组：在微服务架构中，每个服务节点为了实现高可用和负载均衡实际上都会部署多个实例，如果只有发布-订阅模式就会导致同一个功能的服务节点会重复消费。为了解决这个问题，Spring Cloud Stream中提供了消费组的概念。通过spring.cloud.stream.bindings.input.group属性为应用指定一个组名，MQ中的一条消息只能被同一个组名中的一个实例消费。（每个微服务结点启动后会默认获得一个独立的组）。 需要注意的是：每个发送到消费组的数据，仅由消费组中的一个消费者处理。 3、主题分区：Stream提供对一个应用多个实例情况下的数据分区支持，在这种场景下，the broker topic会被视为拥有很多分区的结构，这样确保含有特定标识的数据总是会被同一个消费者实例消费。不管中间件是否原生支持，由于Stream的通用抽象，都可以使用这一特性。 SpringCloud BusSpring cloud bus通过轻量消息代理连接各个分布的节点。这会用在广播状态的变化（例如配置变化）或者其他的消息指令。Spring bus的一个核心思想是通过分布式的启动器对spring boot应用进行扩展，可以用来建立一个多个应用之间的通信频道。目前唯一实现的方式是用AMQP消息代理作为通道(因为它依赖于SpringCloud Stream)。利用bus的机制可以做很多的事情，其中配置中心客户端刷新就是典型的应用场景之一，我们用一张图来描述bus在配置中心使用的机制。 跟踪总线事件一些场景下，我们可能希望知道Spring Cloud Bus事件传播的细节。此时，我们可以跟踪总线事件（RemoteApplicationEvent的子类都是总线事件）。跟踪总线事件非常简单，只需设置spring.cloud.bus.trace.enabled=true，这样在/bus/refresh端点被请求后，访问/trace端点就可获得 SpringCloud Sidecar关于Sidecar模式，在微服务架构演进一文中已有介绍，这里不重复了。Sidecar是作为一个代理的服务来间接性的让其他语言可以使用Eureka等相关组件。通过与Zuul的来进行路由的映射，从而可以做到服务的获取，然后可以使用Ribbon，Feign对服务进行消费，以及对Config Server的间接性调用。非jvm需要应该实现一个健康检查，Sidecar能够以此来报告给Eureka注册中心该应用是up还是down状态。做完了我们可以通过服务名称访问异构语言的接口，走网关，直接通过服务名称访问都是可以的。 建议参考：http://www.cnblogs.com/YrlixJoe/p/7506222.htmlhttps://www.cnblogs.com/YrlixJoe/p/7509655.htmlhttps://www.cnblogs.com/waterlufei/p/7145746.html OAuth2+JWT与身份验证在微服务演进一文中已有提到一些实现机制。这里仅提供图 概念：OAUTH2:Spring Cloud可以使用OAUTH2(Spring Security OAuth2)来实现多个微服务的统一认证授权通过向OAUTH2服务进行集中认证和授权，获得access_token而这个token是受其他微服务信任的，在后续的访问中都把access_token带过去，从而实现了微服务的统一认证授权。 OAUth2OAuth 2 在整个流程中有四种角色: 资源拥有者(Resource Owner) - 这里是Tom 资源服务器(Resource Server) - 这里是Facebook 授权服务器(Authorization Server) - 这里当然还是Facebook，因为Facebook有相关数据 客户端(Client) - 这里是某App任何类型的应用都提供用户登录，登录结果是一个Access Token，所有的之后的API调用都将这个Access Token加入HTTP请求头中，被调用服务去授权服务器验证Access Token并获取该Token可访问的权限信息。这样一来，所有服务的访问都会请求另外的服务来完成鉴权。 JWT:JSON Web TokenJWT是一种安全标准。基本思路就是用户提供用户名和密码给认证服务器，服务器验证用户提交信息信息的合法性；如果验证成功，会产生并返回一个Token，用户可以使用这个token访问服务器上受保护的资源。感觉这2种好像没多大区别呀，其实是有区别的：OAuth2是一种授权框架 ，JWT是一种认证协议无论使用哪种方式切记用HTTPS来保证数据的安全性。 四种面向微服务系统的身份验证方案：在传统的单体架构中，单个服务保存所有的用户数据，可以校验用户，并在认证成功后创建HTTP会话。在微服务架构中：方案1：单点登录（SSO）方案。方案2：分布式会话方案。 但微服务中提倡stateless,所以考虑采用单点登录方案OAUTH2+JWT实现。基于token的鉴权机制类似于http协议也是无状态的，它不需要在服务端去保留用户的认证信息或者会话信息。这就意味着基于token认证机制的应用不需要去考虑用户在哪一台服务器登录了，这就为应用的扩展提供了便利。 流程上是这样的： 用户使用用户名密码来请求服务器 服务器进行验证用户的信息 服务器通过验证发送给用户一个token 客户端存储token，并在每次请求时附送上这个token值 服务端验证token值，并返回数据 这个token必须要在每次请求时传递给服务端，它应该保存在请求头里， 另外，服务端要支持CORS(跨来源资源共享)策略，一般我们在服务端这么做就可以了Access-Control-Allow-Origin: *。 JWT是由三段信息构成的，将这三段信息文本用.链接一起就构成了Jwt字符串。就像这样:eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 第一部分我们称它为头部（header),第二部分我们称其为载荷（payload, 类似于飞机上承载的物品)，第三部分是签证（signature).1、headerjwt的头部承载两部分信息：声明类型，这里是jwt声明加密的算法 通常直接使用 HMAC SHA256完整的头部就像下面这样的JSON：1234&#123; &apos;typ&apos;: &apos;JWT&apos;, &apos;alg&apos;: &apos;HS256&apos;&#125; 然后将头部进行base64编码,构成了第一部分.eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9 2、playload载荷就是存放有效信息的地方。这些有效信息包含三个部分 标准中注册的声明 公共的声明 私有的声明 标准中注册的声明 (建议但不强制使用) ： iss: jwt签发者 sub: jwt所面向的用户 aud: 接收jwt的一方 exp: jwt的过期时间，这个过期时间必须要大于签发时间 nbf: 定义在什么时间之前，该jwt都是不可用的. iat: jwt的签发时间 jti: jwt的唯一身份标识，主要用来作为一次性token,从而回避重放攻击。 公共的声明 ：公共的声明可以添加任何的信息，一般添加用户的相关信息或其他业务需要的必要信息.但不建议添加敏感信息，因为该部分在客户端可解密. 私有的声明 ：私有声明是提供者和消费者所共同定义的声明，一般不建议存放敏感信息，因为base64是对称解密的，意味着该部分信息可以归类为明文信息。 3、signaturejwt的第三部分是一个签证信息，这个签证信息由三部分组成： header (base64后的) payload (base64后的) secret注意：secret是保存在服务器端的，jwt的签发生成也是在服务器端的，secret就是用来进行jwt的签发和jwt的验证，所以，它就是你服务端的私钥。(可以采用对称或非对称RSA加密) 如何应用一般是在请求头里加入Authorization，并加上Bearer标注：12345fetch('api/user/1', &#123; headers: &#123; 'Authorization': 'Bearer ' + token &#125;&#125;) 服务端会验证token，如果验证通过就会返回相应的资源。整个流程就是这样的: 优点 因为json的通用性，所以JWT是可以进行跨语言支持的，像JAVA,JavaScript,NodeJS,PHP等很多语言都可以使用。 因为有了payload部分，所以JWT可以在自身存储一些其他业务逻辑所必要的非敏感信息。 便于传输，jwt的构成非常简单，字节占用很小，所以它是非常便于传输的。 相比于session，它无需保存在服务器，所以不占用服务器内存开销, 它易于应用的扩展 无状态、可拓展性强：比如有3台机器（A、B、C）组成服务器集群，若session存在机器A上，session只能保存在其中一台服务器，此时你便不能访问机器B、C，因为B、C上没有存放该Session，而使用token就能够验证用户请求合法性，并且我再加几台机器也没事，所以可拓展性好就是这个意思。 前后端分离，支持跨域访问 安全相关 不应该在jwt的payload部分存放敏感信息，因为该部分是客户端可解密的部分。 保护好secret私钥，该私钥非常重要。 如果可以，请使用https协议 JWT缺点：一旦拿到token， 可用它访问服务器，直到过期，中间服务器无法控制它，如是它失效(（自己放出去的token，含着泪也要用到底。)（有解决方案： 在 token 中保存信息，可添加额外的验证，如加一个 flag， 把数据库对应的flag失效，来控制token有效性，但这又相当于在内存中保存session id了）。token 的过期时间设置很关键，一般把它设到凌晨少人访问时失效，以免用户使用过程中失效而丢失数据。 总结：使用spring-cloud-security-oauth2来实现oauth server和resource server，oauth Server和resource Server分开，oauth Server和resource Server使用了jwt的方式进行了实现。认证服务器生成jwt格式的token，并不对其进行持久化，然后资源服务器使用密钥进行校验token。 Service接口约束-Swagger2Swagger2，它可以轻松的整合到Spring Boot中，并与Spring MVC程序配合组织出强大RESTful API文档。它既可以减少我们创建文档的工作量，同时说明内容又整合入实现代码中，让维护文档和修改代码整合为一体，可以让我们在修改代码逻辑的同时方便的修改文档说明。另外Swagger2也提供了强大的页面测试功能来调试每个RESTful API。具体效果如下图所示: 与Eureka Server集成整和Eureka和Swagger2，当点击链接的时候，直接跳转到Swagger2的UI界面，怎么做了？在服务配置中添加配置以上面注册到Eureka Server上的服务RDCLOUD-JPA为例，在该服务的配置文件中加上如下配置即可：eureka.instance.status-page-url=http://localhost:${server.port}/swagger-ui.html # ${server.port}为该服务的端口号(不知道是否支持ServiceId的形式，待验证) 集成监控上面我们提到了SpringBoot有Actuator监控,配合SpringBoot Amin可以界面化Eureka也有监控监控Hystix有DashBoardSleuth有Zipkin UI界面 那么如何集成起来监控呢？两种方案：1、Spring Boot Amin集成2、Atlas+Spectator+Grafana 1、Spring Boot Amin特点：特点： Show health status Show details, like JVM &amp; memory metrics Counter &amp; gauge metrics Datasource metrics - Cache metrics Show build-info number Follow and download logfile View jvm system- &amp; environment-properties Support for Spring Cloud’s postable /env- &amp;/refresh-endpoint Easy loglevel management (currently for Logback only) Interact with JMX-beans View thread dump View HTTP traces Hystrix-Dashboard integration Download heapdump Notification on status change (via mail, Slack, Hipchat, …) Event journal of status changes (non persistent) 建议参考：https://juejin.im/entry/593e447e61ff4b006c9be676https://github.com/codecentric/spring-boot-adminhttp://cxytiandi.com/blog/detail/12880 2、Atlas+Spectator+Grafana搭建实时监控平台建议参考https://my.oschina.net/u/2408085/blog/733900https://medium.com/@brunosimioni/near-real-time-monitoring-charts-with-spring-boot-actuator-jolokia-and-grafana-1ce267c50bcchttps://segmentfault.com/a/1190000008527553]]></content>
      <categories>
        <category>MicroService</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>msa</tag>
        <tag>SpringCloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务架构之服务架构演进]]></title>
    <url>%2F2017%2F12%2F21%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E4%B9%8B%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%2F</url>
    <content type="text"><![CDATA[微服务MicroService(MSA),在近几年被炒得火热,但,What’s MicroService? 我相信是很多人都想问的，难道就是小服务?先别急，其实，微服务并没有明确的定义，它只是一种架构风格(Restful不也是一种架构风格不)，我们先看看传统单体与微服务架构的区别图示： 微服务架构特性MartinFlower大神给出了关于微服务特性很好的归纳：大神原文见此. In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. 总之，微服务架构风格，是一种将一哥单一应用拆分为多个微小服务的方式，每个微小服务在独立的进程中运行(注:服务之间由原先的进程内通信，编程进程间甚至跨网络跨主机间的通信)并使用轻量级的通信协议(如HTTP Restful API).这些服务围绕业务能力而组织，并通过自动化部署方式单独部署。这小微服务只需要很弱的集中管理(甚至不需要),同时可以使用不用的编程语言和不用的数据库技术。 根据MartinFowler的分析，微服务架构有以下的一些通用特性：1、通过服务实现应用的组件化(Componentization via Services)：微服务架构中将组件定义为可被独立替换和升级的软件单元，在应用架构设计中通过将整体应用切分成可独立部署及升级的微服务方式进行组件化设计。2、围绕业务能力组织服务(Organized around Business Capabilities)：微服务架构采取以业务能力为出发点组织服务的策略，因此微服务团队的组织结构必须是跨功能的（如：既管应用，也管数据库）、强搭配的DevOps开发运维一体化团队，通常这些团队不会太大（如：亚马逊的“Two pizzateam”- 不超过12人）。3、产品而非项目模式(Productsnot Projects)：传统的应用模式是一个团队以项目模式开发完整的应用，开发完成后就交付给运维团队负责维护；微服务架构则倡导一个团队应该如开发产品般负责一个“微服务”完整的生命周期，倡导”谁开发，谁运维(you build, you run it)“的开发运维一体化方法。4、智能端点与管道扁平化/弱通信(Smartendpoints and dumb pipes)：微服务架构主张将组件间通讯的相关业务逻辑/智能放在组件端点侧而非放在通讯组件中，通讯机制或组件应该尽量简单及松耦合。RESTful HTTP协议和仅提供消息路由功能的轻量级异步机制是微服务架构中最常用的通讯机制。5、“去中心化”治理(Decentralized Governance)：整体式应用往往倾向于采用单一技术平台，微服务架构则鼓励使用合适的工具完成各自的任务，每个微服务可以考虑选用最佳工具完成(如不同的编程语言)。微服务的技术标准倾向于寻找其他开发者已成功验证解决类似问题的技术。6、“去中心化”数据管理(Decentralized Data Management)：微服务架构倡导采用多样性持久化(PolyglotPersistence)的方法，让每个微服务管理其自有数据库，并允许不同微服务采用不同的数据持久化技术。7、基础设施自动化(Infrastructure Automation)：云化及自动化部署等技术极大地降低了微服务构建、部署和运维的难度，通过应用持续集成和持续交付等方法有助于达到加速推出市场的目的。8、故障处理设计(Design for failure)：微服务架构所带来的一个后果是必须考虑每个服务的失败容错机制。因此，微服务非常重视建立架构及业务相关指标的实时监控和日志机制。9、演进式的设计(Evolutionary Design)：微服务应用更注重快速更新，因此系统的计会随时间不断变化及演进。微服务的设计受业务功能的生命周期等因素影响。如某应用是整体式应用，但逐渐朝微应用架构方向演进，整体式应用仍是核心，但新功能将使用应用所提供的API构建。再如在某微服务应用中，可替代性模块化设计的基本原则，在实施后发现某两个微服务经常必须同时更新，则这很可能意味着应将其合并为一个微服务。个人认为比较关键的四个词:you build,you run it,Decentralized,Automation,Design for failure. 理论基础-康威定律(Conway’s Law) Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure. – Melvyn Conway, 1967 翻译起来就是:系统架构等同于组织的沟通结构。组织架构会在潜移默化中约束软件系统架构的形态。提示：违背康威定律，非常容易出现系统设计盲区，出现 “两不管” 互相推脱的局面，当业务边界与组织架构冲突时，从业内的实践经验来看，宁愿选择更加符合组织架构的服务拆分边界。这也是一种符合康威定律的做法。 关于康威定律，阿里云的一篇文章解释的比较详细，见此值得一提的是，这是1967年提出的，而微服务是2014年左右开始火起来的，至少在中国是这样。 单体式架构环境下的康威定律：微服务架构下的康威定律: 微服务架构 vs. SOA同：两者都是架构风格范畴，都强调服务化。异：但其关注领域与涉及范围不同。SOA更关注企业规模范围，微服务架构则更关注应用规模范围。微服务组件 vs. 服务组件 – 两者都是描述业务功能的具体实现，其区别在于粒度不同，此外还有在可管理性、灵活性上的差异。通信协议上：微服务强调轻量级协议及弱通信，SOA有ESB这个重量级成员.微服务强调每个服务单独部署与独立的进程，Devops，同时与Docker容器技术结合地非常好，这是SOA所不要求的。网上有个比较好的表格，如下： 微服务的优缺点微服务架构的优点： 每个服务都比较简单，只关注于一个业务功能。 微服务架构方式是松耦合的，可以提供更高的灵活性。 微服务可通过最佳及最合适的不同的编程语言与工具进行开发，能够做到有的放矢地解决针对性问题。 每个微服务可由不同团队独立开发，互不影响，加快推出市场的速度。 微服务架构是持续交付(CD)的巨大推动力，允许在频繁发布不同服务的同时保持系统其他部分的可用性和稳定性。 微服务架构的缺点： 运维开销及成本增加：整体应用可能只需部署至一小片应用服务区集群，而微服务架构可能变成需要构建/测试/部署/运行数十个独立的服务，并可能需要支持多种语言和环境。这导致一个整体式系统如果由20个微服务组成，可能需要40~60个进程。 必须有坚实的DevOps开发运维一体化技能：开发人员需要熟知运维与投产环境，开发人员也需要掌握必要的数据存储技术如NoSQL，具有较强DevOps技能的人员比较稀缺，会带来招聘人才方面的挑战。 隐式接口及接口匹配问题：把系统分为多个协作组件后会产生新的接口，这意味着简单的交叉变化可能需要改变许多组件，并需协调一起发布。在实际环境中，一个新品发布可能被迫同时发布大量服务，由于集成点的大量增加，微服务架构会有更高的发布风险。 代码重复：某些底层功能需要被多个服务所用，为了避免将“同步耦合引入到系统中”，有时需要向不同服务添加一些代码，这就会导致代码重复。 分布式系统的复杂性：作为一种分布式系统，微服务引入了复杂性和其他若干问题，例如网络延迟、容错性、消息序列化、不可靠的网络、异步机制、版本化、差异化的工作负载等，开发人员需要考虑以上的分布式系统问题。 异步机制：微服务往往使用异步编程、消息与并行机制，如果应用存在跨微服务的事务性处理，其实现机制会变得复杂化。另外一个关于微服务的挑战来自于分区的数据库架构。商业交易中同时给多个业务分主体更新消息很普遍。这种交易对于单体式应用来说很容易，因为只有一个数据库。在微服务架构应用中，需要更新不同服务所使用的不同的数据库。使用分布式交易并不一定是好的选择，不仅仅是因为CAP理论，还因为今天高扩展性的NoSQL数据库和消息传递中间件并不支持这一需求。最终你不得不使用一个最终一致性(冥等)的方法，从而对开发者提出了更高的要求和挑战。 可测性的挑战：在动态环境下服务间的交互会产生非常微妙的行为，难以可视化及全面测试。经典微服务往往不太重视测试，更多的是通过监控发现生产环境的异常，进而快速回滚或采取其他必要的行动。但对于特别在意风险规避监管或投产环境错误会产生显著影响的场景下需要特别注意。以上某些缺点不是不可降服的，实践中可以通过某些方式解决。 微服务实践参考Kasun的两篇博文，可以有个较好的理解，地址传送：http://kasunpanorama.blogspot.com/2015/11/microservices-in-practice.htmlhttp://kasunpanorama.blogspot.com/2016/02/microservices-enterprise-integration.html?view=sidebar其中关于单体应用(Monolithic application)，及SOA的部分，我就不多说了。主要说说MSA的。 Microservices Architecture：1、a suite of small and independent services that are running in its own process, developed and deployed independently.2、Microservices is not just about splitting the services available in monolith into independent services.The business capabilities can be implemented as fully independent, fine-grained and self contained (micro)services. Guidelines for Designing microservices:(微服务设计原则) Single Responsibility Principle(SRP): Having a limited and a focused business scope for a microservice helps us to meet the agility in development and delivery of services. During the designing phase of the microservices, we should find their boundaries and align them with the business capabilities (also known as bounded context in Domain-Driven-Design). Make sure the microservices design ensures the agile/independent development and deployment of the service. Our focus should be on the scope of the microservice, but not about making the the service smaller. The (right)size of the service should be the required size to facilitate a given business capability. Unlike service in SOA, a given microservice should have a very few operations/functionalities and simple message format. It is often a good practice to start with relatively broad service boundaries to begin with, refactoring to smaller ones (based on business requirements) as time goes on. 微服务中的消息传递(Messaging in Microservices)单体应用一般采用进程内函数或方法调用，SOA中，在web service层次解耦了消息传递，同时支持不同的协议，如HTTP,JMS,WebServices，但采用比较复杂的消息模式，及通过ESB来总管消息而微服务采用简单和轻量级的消息传递格式和协议,一般而言就是HTTP Restful API+JSON/XML1、Synchronous Messaging - REST, Thrift2、Asynchronous Messaging - AMQP, STOMP, MQTT3、Message Formats - JSON, XML, Thrift, ProtoBuf, Avro4、Service Contracts- Defining the service interfaces(服务端接口约束) - Swagger, RAML, Thrift IDL,在SpringCloud的架构中，习惯性采用Swagger2生成接口文档，可以选择SpringCloud Contract来实现强制约定和CDC. Integrating Microservices (Inter-service/process Communication)：in order to realize a business use case, it is required to have the communication structures between different microservices/processes. 1、Point-to-point style - Invoking services directly缺点：非功能性需求如:用户认证，限流，监控，需要在每个微服务层去实现。导致重复实现(当然可以做成公共依赖，但仍然不够优雅，也没法控制好)同时也满足不了微服务要求的高伸缩性要求。 2、API-Gateway style将非功能性需求如:用户认证，限流，监控等，放在api-gateway中来实现。提供统一的抽象。并对外可灵活提供不同接入方式。注意： If a microservice wants to consume another microservice that also needs to be done through the API-GW. (貌似SpringCloud中的Zuul并不是这么干的)缺点：较直接调用，多了一层代理转发，不过一般这不会称为什么问题。 3、Message Broker style:利于解耦和快速响应 去中心化(Decentralized)1、Decentralized Data Management左边是单体应用，右边是微服务应用 数据去中心化的要点: Each microservice can have a private database to persist the data that requires to implement the business functionality offered from it. A given microservice can only access the dedicated private database but not the databases of other microservices. In some business scenarios, you might have to update several database for a single transaction. In such scenarios, the databases of other microservices should be updated through its service API only (not allowed to access the database directly) The de-centralized data management give you the fully decoupled microservices and the liberty of choosing disparate data management techniques (SQL or NoSQL etc.) 2、Decentralized Governance：In general ‘governance’ means establishing and enforcing how people and solutions work together to achieve organizational objectives.(“治理”意味着建立执行者和解决方案如何协同工作以实现组织目标)在微服务架构中，微服务的是完全独立并与其他微服务解耦的，实现上也采用不同的技术和平台。所以不需要定义一个统一的标准来约束服务的设计和开发。(主要针对技术选项，比如某些公司就一套SSH框架解决所有问题.) we can summarize the decentralized governance capabilities of Microservices as follows.： In microservices architecture there is no requirement to have centralized design-time governance. Microservices can make their own decisions about its design and implementation. Microservices architecture foster the sharing of common/reusable services. Some of the run-time governances aspects such as SLAs, throttling, monitoring, common security requirements and service discovery may be implemented at API-GW level. Service Registry and Service Discovery服务注册，一般启动时注册，退出时注销。服务发现：有两种形式，客户端发现和服务端发现。1、客户端发现：2、服务端发现： 注：这里的客户端与服务端都是相对而言的。 Deployment要求： 每个服务独立部署. 确保可以在服务级别达到伸缩性(a given service may get more traffic than other services). 快速构建和部署服务. Failure in one microservice must not affect any of the other services. Docker (an open source engine that lets developers and system administrators deploy self-sufficient application containers in Linux environments) provides a great way to deploy microservices addressing the above requirements. 关键步骤如下： Package the microservice as a (Docker) container image. Deploy each service instance as a container. Scaling is done based on changing the number of container instances. Building, deploying and starting microservice will be much faster as we are using docker containers (which is much faster than a regular VM) Kubernetes is extending Docker’s capabilities by allowing to manage a cluster of Linux containers as a single system, managing and running Docker containers across multiple hosts, offering co-location of containers, service discovery and replication control. As you can see, most of these features are essential in our microservices context too. Hence using Kubernetes (on top of Docker) for microservices deployment has become an extremely powerful approach, specially for large scale microservices deployments.(注意点:K8s与msa的一些功能有重叠，实践时需要进行取舍) Security OAuth2 - Is an access delegation protocol. The client authenticates with authorization server and get an opaque token which is known as ‘Access token‘. Access token has zero information about the user/client. It only has a reference to the user information that can only be retrieved by the Authorization server. Hence this is known as a ‘by-reference token‘ and it is safe to use this token even in the public network/internet. OpenID Connect behaves similar to OAuth but in addition to the Access token, the authorization server issues an ID token which contains information about the user. This is often implement by a JWT (JSON Web Token) and that is signed by authorization server. So, this ensures the trust between the authorization server and the client. JWT token is therefore known as a ‘By-value token‘ as it contains the information of the user and obviously it is not safe to use it outside the internal network. OAuth2与OpenID的异同点:同，都有一个Token，都有认证server,异，前者token不包含用户信息，后者包含。实践中一般两种结合。对外用by-reference token,对内用JWT格式的By-value token. 实现微服务安全的步骤要点: Leave authentication to OAuth and the OpenID Connect server(Authorization Server), so that microservices successfully provide access given someone has the right to use the data. Use the API-GW style, in which there is a single entry point for all the client request. Client connects to authorization server and obtains the Access Token (by-reference token).Then send the access token to the API-GW along with the request. Token Translation at the Gateway - API-GW extracts the access token and send it to the authorization server to retrieve the JWT (by value-token). Then GW passes this JWT along with the request to the microservices layer. JWTs contains the necessary information to help in storing user sessions etc. If each service can understand a JSON web token, then you have distributed your identity mechanism which is allowing you to transport identity throughout your system. At each microservice layer, we can have a component that process the JWT, which is a quite trivial implementation. 其实上面的实现就是一种statless或sessionless模式。已经看不到传统http session及cookie发挥作用了。什么是JWT?后续文章再讲。 Transactions在分布式系统中要实现事务是非常复杂的。微服务架构风格提倡transaction-less between services。如果确实需要事务，可以采用补偿操作(compensating operation) The key idea is, a given microservice is based on the single responsibility principle and if a given microservice failed to execute a given operation, we can consider that as a failure of that entire microservice. Then all the other (upstream) operations has to be undone by invoking the respective compensating operation of those microservice Design for Failures原则(基于雪崩效应，多米诺股牌效应的考虑，fast-fail,failover等)An unavailable or unresponsive microservice should not bring the whole microservices-based application down. Thus microservices should be fault tolerant, be able to recover when that is possible and the client has to handle it gracefully.Also, since services can fail at any time, it’s important to be able to detect(real-time monitoring) the failures quickly and, if possible, automatically restore the services. 微服务中的几种处理错误的模式：1、Circuit Breaker(断路器)MatinFlower大神也有篇文章介绍,见此When you are doing an external call to a microservice, you configure a fault monitor component with each invocation and when the failures reach a certain threshold then that component stops any further invocations of the service (trips the circuit). After certain number of requests in open state (which you can configure), change the circuit back to close state.This pattern is quite useful to avoid unnecessary resource consumption, request delay due to timeouts and also give us to chance to monitor the system (based on the active open circuits states). 2、Bulkhead(隔离)As microservice application comprises of number of microservices, the failures of one part of the microservices-based application should not affect the rest of the application. Bulkhead pattern is about isolating different parts of your application so that a failure of a service in such part of the application does not affect any of the other services. 3、Timeout 很多情况下，一般把这些实现在网关层次(SpringCloud Zuul也支持，但每个微服务也支持) 微服务在企业中的实践这里所说的时，已有旧系统的情况下。 Orchestration(业务流程/编排)1、Orchestration at Microservices Layer 2、Orchestration at the Gateway Layer 3、基于AMQP或Kafka的异步消息 微服务部署模式主要参考地址：https://martinfowler.com/bliki/BlueGreenDeployment.htmlhttp://blog.csdn.net/zyqduron/article/details/59507525 1、Blue/Green Deployment（蓝绿部署）(1) 部署版本1的应用（一开始的状态）所有外部请求的流量都打到这个版本上。(2) 部署版本2的应用版本2的代码与版本1不同(新功能、Bug修复等)。(3) 将流量从版本1切换到版本2。(4) 如版本2测试正常，就删除版本1正在使用的资源（例如实例），从此正式用版本2。从过程不难发现，在部署的过程中，我们的应用始终在线。并且，新版本上线的过程中，并没有修改老版本的任何内容，在部署期间，老版本的状态不受影响。这样风险很小，并且，只要老版本的资源不被删除，理论上，我们可以在任何时间回滚到老版本。 2、Rolling update（滚动发布）滚动发布，一般是取出一个或者多个服务器停止服务，执行更新，并重新将其投入使用。周而复始，直到集群中所有的实例都更新成新版本。这种部署方式相对于蓝绿部署，更加节约资源——它不需要运行两个集群、两倍的实例数。我们可以部分部署，例如每次只取出集群的20%进行升级。 这种方式也有很多缺点，例如：(1) 没有一个确定OK的环境。使用蓝绿部署，我们能够清晰地知道老版本是OK的，而使用滚动发布，我们无法确定。(2) 修改了现有的环境。(3) 如果需要回滚，很困难。举个例子，在某一次发布中，我们需要更新100个实例，每次更新10个实例，每次部署需要5分钟。当滚动发布到第80个实例时，发现了问题，需要回滚。此时，脾气不好的程序猿很可能想掀桌子，因为回滚是一个痛苦，并且漫长的过程。(4) 有的时候，我们还可能对系统进行动态伸缩，如果部署期间，系统自动扩容/缩容了，我们还需判断到底哪个节点使用的是哪个代码。 3、灰度发布/金丝雀部署/AB test灰度发布是在原有版本可用的情况下，同时部署一个新版本应用作为“金丝雀”（金丝雀对瓦斯极敏感，矿井工人携带金丝雀，以便及时发发现危险），测试新版本的性能和表现，以保障整体系统稳定的情况下，尽早发现、调整问题。灰度发布是指在黑与白之间，能够平滑过渡的一种发布方式。AB test就是一种灰度发布方式，让一部分用户继续用A，一部分用户开始用B，如果用户对B没有什么反对意见，那么逐步扩大范围，把所有用户都迁移到B上面来。灰度发布可以保证整体系统的稳定，在初始灰度的时候就可以发现、调整问题，以保证其影响度。很多人把灰度发布与蓝绿部署混为一谈，笔者认为，与灰度发布最类似的应该是金丝雀部署。 注意：概念上来说，A/B测试是用来测试应用功能表现的方法，例如可用性、受欢迎程度、可见性等等 灰度发布／金丝雀发布由以下几个步骤组成： 准备好部署各个阶段的工件，包括：构建工件，测试脚本，配置文件和部署清单文件。 从负载均衡列表中移除掉“金丝雀”服务器。 升级“金丝雀”应用（排掉原有流量并进行部署）。 对应用进行自动化测试。 将“金丝雀”服务器重新添加到负载均衡列表中（连通性和健康检查）。 如果“金丝雀”在线使用测试成功，升级剩余的其他服务器。（否则就回滚） 总结(1) 蓝绿部署：不停止老版本，额外搞一套新版本，等测试发现新版本OK后，删除老版本。(2) 滚动发布：按批次停止老版本实例，启动新版本实例。(3) 灰度发布/金丝雀部署：不停止老版本，额外搞一套新版本，常常按照用户设置路由权重，例如90%的用户维持使用老版本，10%的用户尝鲜新版本。不同版本应用共存，经常与A/B测试一起使用，用于测试选择多种方案。 通过docker和kubernetes，我们可以很简单的实现蓝绿部署、A/B测试、灰度发布.（这点需待后续实践中体会） Service Mesh(服务网格)首先摆出参考文章地址:http://philcalcado.com/2017/08/03/pattern_service_mesh.htmlhttp://geek.csdn.net/news/detail/239953http://blog.csdn.net/zvayivqt0ufji/article/details/78351355https://istio.io/docs/welcome/index.htmlhttps://istio.io/docs/welcome/index.html 首先我们先看看计算机网络的发展： 第一批基于微服务构建的系统遵循了与前几代联网计算机类似的策略。这意味着落实上述需求的责任落在了编写服务的工程师身上。但是，尽管数十年前开发的TCP/IP协议栈和通用网络模型仍然是计算机之间相互通讯的有力工具，但更复杂的架构引入了另一个层面的要求，这再次需要由在这方面工作的工程师来实现。而不是应用工程师。 类似于我们在网络协议栈中看到的那样，大规模分布式服务所需的功能应该放到底层的平台中。人们使用高级协议（如HTTP）编写非常复杂的应用程序和服务，甚至无需考虑TCP是如何控制网络上的数据包的。这种情况就是微服务所需要的，那些从事服务开发工作的工程师可以专注于业务逻辑的开发，从而避免浪费时间去编写自己的服务基础设施代码或管理整个系统的库和框架。 不幸的是，通过改变网络协议栈来添加这个层并不是一个可行的任务。许多人的解决方案是通过一组代理来实现。这个的想法是，服务不会直接连接到它的下游，而是让所有的流量都将通过一个小小的软件来透明地添加所需功能。sidecars就是这样一种模式。在2013年，Airbnb写了一篇有关Synapse和Nerve的文章，这是“边三轮”的一个开源实现。一年后，Netflix推出了Prana，专门用于让非JVM应用程序从他们的NetflixOSS生态系统中受益(SpringCloud sidecar的前身)。虽然有这么几个开源的代理实现，但它们往往被设计为需要与特定的基础架构组件配合使用。例如，在服务发现方面，Airbnb的Nerve和Synapse假设了服务是在Zookeeper中注册，而对于Prana，则应该使用Netflix自己的Eureka服务注册表 。随着微服务架构的日益普及，我们最近看到了一波新的代理浪潮，它们足以灵活地适应不同的基础设施组件和偏好。 架构图:服务网格是用于处理服务到服务通信的专用基础设施层。它负责通过复杂的服务拓扑来可靠地传递请求。实际上，服务网格通常被实现为与应用程序代码一起部署的轻量级网络代理矩阵，并且它不会被应用程序所感知。随着微服务部署被迁移到更为复杂的运行时中去，如Kubernetes和Mesos，人们开始使用一些平台上的工具来实现网格网络这一想法。他们实现的网络正从互相之间隔离的独立代理，转移到一个合适的并且有点集中的控制面上来。 图中应用作为服务的发起方，只需要用最简单的方式将请求发送给本地的服务网格代理，然后网格代理会进行后续的操作，如服务发现，负载均衡，最后将请求转发给目标服务。当有大量服务相互调用时，它们之间的服务调用关系就会形成网格，来看看这个鸟瞰图，实际的服务流量仍然直接从代理流向代理，但是控制面知道每个代理实例。控制面使得代理能够实现诸如访问控制和度量收集这样的功能，但这需要它们之间进行合作： 最近公布的Istio项目https://istio.io/是这类系统中最著名的例子。服务网格是用于处理服务到服务通信的“专用基础设施层”。它通过这些代理来管理复杂的服务拓扑，可靠地传递服务之间的请求。 从某种程度上说，这些代理接管了应用程序的网络通信层。 值得一提的是：Service Mesh概念是今年才开始提出来的，而相关项目Istio是2017.5.才启动的。目前最新版本0.4. 什么是Service Mesh 一种基础设施层服务，服务间的通信通过service mesh进行 可靠地传输复杂拓扑中服务的请求，将它们变成现代的云原生服务 一种网络代理的实现，通常与业务服务部署在一起，业务服务不感知 一种网络模型，在TCP/IP之上的抽象层，TCP/IP负责将字节码可靠地在网络节点间传递，Service mesh则复杂将服务间的协议请求可靠地在服务间进行传输。它们不关心传输的内容 TCP/IP仅仅负责传输，但Service mesh可对运行时进行控制，使服务变得可监控，可管理。 为什么使用Service Mesh 无需考虑每种语言都要解决的问题 对业务代码0侵入，开发者无需关心分布式架构带来的复杂性以及引入的技术问题 对于不适合改造的老旧单体应用，提供了一种接入分布式环境的方式 微服务化的进程通常不是一蹴而就的，很多应用选择了演进的方式，就是将单体应用一部分一部分地进行拆分。而在这个过程中，使用Service Mesh就可以很好地保证未拆分的应用与已经拆分出来的微服务之间的互通和统一治理 开发出的应用既是云原生的又具有云独立性，不将业务代码与任何框架，平台或者服务绑定 Service mesh解决不了的问题 无分布式事务方案 Service Mesh组件代理请求转发，会在一定程度上降低系统通信性能 没有Event Driven的框架 侵入式框架以源码和业务代码结合，有较强定制和扩展能力，Service mesh相对不易定制扩展 在运行时，依赖单独的Service Mesh代理，多了一个故障点。整个系统的运行和运维也强依赖于Service Mesh组件的能力 Service Mesh新生代IstioIstio是Service Mesh的一种实现，由Google，IBM和Lyft于2017.5开始主导开发，目前最新版本0.4官方定义:Istio：一个连接，管理和保护微服务的开放平台。Istio提供一种简单的方式来建立已部署的服务的网络，具备负载均衡，服务到服务认证，监控等等功能，而不需要改动任何服务代码。简单的说，有了Istio，你的服务就不再需要任何微服务开发框架（典型如Spring Cloud，Dubbo），也不再需要自己动手实现各种复杂的服务治理的功能（很多是Spring Cloud和Dubbo也不能提供的，需要自己动手）。只要服务的客户端和服务器可以进行简单的直接网络访问，就可以通过将网络层委托给Istio，从而获得一系列的完备功能。可以近似的理解为：Istio = 微服务框架 + 服务治理 Istio的关键功能: HTTP/1.1，HTTP/2，gRPC和TCP流量的自动区域感知负载平衡和故障切换。 通过丰富的路由规则，容错和故障注入，对流行为的细粒度控制。 支持访问控制，速率限制和配额的可插拔策略层和配置API。 集群内所有流量的自动量度，日志和跟踪，包括集群入口和出口。 安全的服务到服务身份验证，在集群中的服务之间具有强大的身份标识。 为什么要使用Istio?微服务的两面性:虽然微服务对开发进行了简化，通过将复杂系统切分为若干个微服务来分解和降低复杂度，使得这些微服务易于被小型的开发团队所理解和维护。但是，复杂度并非从此消失。微服务拆分之后，单个微服务的复杂度大幅降低，但是由于系统被从一个单体拆分为几十甚至更多的微服务， 就带来了另外一个复杂度：微服务的连接、管理和监控。试想， 对于一个大型系统， 需要对多达上百个甚至上千个微服务的管理、部署、版本控制、安全、故障转移、策略执行、遥测和监控等，谈何容易。更不要说更复杂的运维需求，例如A/B测试，金丝雀发布，限流，访问控制和端到端认证。开发人员和运维人员在单体应用程序向分布式微服务架构的转型中， 不得不面临上述挑战。 Istio在服务网络中统一提供了许多关键功能： 流量管理：控制服务之间的流量和API调用的流向，使得调用更可靠，并使网络在恶劣情况下更加健壮。 可观察性：了解服务之间的依赖关系，以及它们之间流量的本质和流向，从而提供快速识别问题的能力。 策略执行：将组织策略应用于服务之间的互动，确保访问策略得以执行，资源在消费者之间良好分配。策略的更改是通过配置网格而不是修改应用程序代码。 服务身份和安全：为网格中的服务提供可验证身份，并提供保护服务流量的能力，使其可以在不同可信度的网络上流转。 平台支持：Istio旨在在各种环境中运行，包括跨云， 预置，Kubernetes，Mesos等。最初专注于Kubernetes，但很快将支持其他环境。 集成和定制：策略执行组件可以扩展和定制，以便与现有的ACL，日志，监控，配额，审核等解决方案集成。这些功能极大的减少了应用程序代码，底层平台和策略之间的耦合，使微服务更容易实现。 要体会Istio的好处，不妨设想一下，在平时理解的微服务开发过程中，在没有Istio这样的服务网格的情况下，要如何开发我们的应用程序，才可以做到前面列出的这些丰富多彩的功能? 这数以几十记的各种特性，如何才可以加入到应用程序? 无外乎，找个Spring Cloud或者Dubbo的成熟框架，直接搞定服务注册，服务发现，负载均衡，熔断等基础功能。然后自己开发服务路由等高级功能， 接入Zipkin等Apm做全链路监控，自己做加密、认证、授权。 想办法搞定灰度方案，用Redis等实现限速、配额。 诸如此类，一大堆的事情， 都需要自己做，无论是找开源项目还是自己操刀，最后整出一个带有一大堆功能的应用程序，上线部署。然后给个配置说明到运维，告诉他说如何需要灰度，要如何如何， 如果要限速，配置哪里哪里。 这里就有一个很严肃的问题， 给每个业务程序的开发人员: 你到底想往你的业务程序里面塞多少管理和运维的功能? 就算你hold的住技术和时间，你有能力一个一个的满足各种运维和管理的需求吗？ 当你发现你开始疲于响应各种非功能性的需求时，就该开始反省了: 我们开发的是业务程序，它的核心价值在业务逻辑的处理和实现，将如此之多的时间精力花费在这些非业务功能上， 这真的合理吗? 而且即使是在实现层面，微服务实施时，最重要的是如何划分微服务，如何制定接口协议，你该如何分配你有限的时间和资源？Istio 超越 spring cloud和dubbo 等传统开发框架之处， 就在于不仅仅带来了远超这些框架所能提供的功能， 而且也不需要应用程序为此做大量的改动， 开发人员也不必为上面的功能实现进行大量的知识储备。 整体架构Istio服务网格逻辑上分为数据面板和控制面板。数据面板由一组智能代理（Envoy）组成，代理部署为Sidecar，调解和控制微服务之间所有的网络通信。控制面板负责管理和配置代理来路由流量，以及在运行时执行策略。 Istio 中的主要模块 Envoy/Mixer/Pilot/Auth。Envoy在其中扮演的负责搬砖的民工角色，而指挥Envoy工作的民工头就是Pilot模块。Pilot负责收集和验证配置并将其传播到各种Istio组件。它从Mixer和Envoy中抽取环境特定的实现细节，为他们提供独立于底层平台的用户服务的抽象表示。此外，流量管理规则（即通用4层规则和7层HTTP/gRPC路由规则）可以在运行时通过Pilot进行编程。每个Envoy实例根据其从Pilot获得的信息以及其负载均衡池中的其他实例的定期健康检查来维护 负载均衡信息，从而允许其在目标实例之间智能分配流量，同时遵循其指定的路由规则。Pilot负责在Istio服务网格中部署的Envoy实例的生命周期。 EnvoryIstio 使用Envoy代理的扩展版本，Envoy是以C++开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Istio利用了Envoy的许多内置功能，例如动态服务发现，负载均衡，TLS termination，HTTP/2&amp;gRPC代理，熔断器，健康检查，基于百分比流量拆分的分段推出，故障注入和丰富的metrics。Envoy实现了过滤和路由、服务发现、健康检查，提供了具有弹性的负载均衡。它在安全上支持TLS，在通信方面支持gRPC。概括说，Envoy提供的是服务间网络通讯的能力、以及网络通讯直接相关的功能： 服务发现：从Pilot得到服务发现信息 过滤 负载均衡 健康检查 执行路由规则(Rule): 规则来自Polit,包括路由和目的地策略 加密和认证: TLS certs来自 Istio-Auth此外, Envoy 也吐出各种数据给Mixer: Metrics Logging Distribution Trace: 目前支持 Zipkin 总结: Envoy是Istio中负责”干活”的模块,如果将整个Istio体系比喻为一个施工队,那么 Envoy 就是最底层负责搬砖的民工，所有体力活都由Envoy完成。所有需要控制，决策，管理的功能都是其他模块来负责，然后配置给Envoy。 Envoy的竞争者：基于Scala的Linkerd。 Linkerd的功能和定位和Envoy非常相似，而且就在今年上半年成功进入CNCF。而在 Istio 推出之后，Linkerd做了一个很有意思的动作：Linkerd推出了和Istio的集成，实际为替换Envoy作为Istio的数据面板，和Istio的控制面板对接。 目前，Nginx推出了自己的服务网格产品Nginmesh，功能类似，比较有意思的地方是Ngxinmesh一出来就直接宣布要和Istio集成，替换Envoy。 Pilot流量管理，Istio最核心的功能是流量管理，前面我们看到的数据面板，由Envoy组成的服务网格，将整个服务间通讯和入口/出口请求都承载于其上。使用Istio的流量管理模型，本质上将流量和基础设施扩展解耦，让运维人员通过Pilot指定它们希望流量遵循什么规则，而不是哪些特定的pod/VM应该接收流量。对这段话的理解, 可以看下图：假定我们原有服务B，部署在Pod1/2/3上，现在我们部署一个新版本在Pod4在，希望实现切5%的流量到新版本。我们还可以玩的更炫一点, 比如根据请求的内容来源将流量发送到特定版本：每个Envoy实例根据其从Pilot获得的信息以及其负载均衡池中的其他实例的定期健康检查来维护 负载均衡信息，从而允许其在目标实例之间智能分配流量，同时遵循其指定的路由规则。Pilot负责在Istio服务网格中部署的Envoy实例的生命周期。Envoy API负责和Envoy的通讯, 主要是发送服务发现信息和流量控制规则给EnvoyEnvoy提供服务发现，负载均衡池和路由表的动态更新的API。这些API将Istio和Envoy的实现解耦。Polit定了一个抽象模型，以从特定平台细节中解耦，为跨平台提供基础Platform Adapter则是这个抽象模型的现实实现版本, 用于对接外部的不同平台最后是 Rules API，提供接口给外部调用以管理Pilot，包括命令行工具Istioctl以及未来可能出现的第三方管理界面Pilot架构中, 最重要的是Abstract Model和Platform Adapter，我们详细介绍。Abstract Model：是对服务网格中”服务”的规范表示, 即定义在istio中什么是服务，这个规范独立于底层平台。Platform Adapter：这里有各种平台的实现，目前主要是Kubernetes，另外0.2版本的代码中出现了Consul和Eureka。 基于上述的架构设计，Pilot提供以下重要功能： 请求路由 服务发现和负载均衡 故障处理 故障注入 规则配置 Mixer功能概括：Mixer负责在服务网格上执行访问控制和使用策略，并收集Envoy代理和其他服务的遥测数据。我们的系统通常会基于大量的基础设施而构建，这些基础设施的后端服务为业务服务提供各种支持功能。包括访问控制系统，遥测捕获系统，配额执行系统，计费系统等。在传统设计中, 服务直接与这些后端系统集成，容易产生硬耦合。在Istio中，为了避免应用程序的微服务和基础设施的后端服务之间的耦合，提供了 Mixer 作为两者的通用中介层：Mixer 设计将策略决策从应用层移出并用配置替代，并在运维人员控制下。应用程序代码不再将应用程序代码与特定后端集成在一起，而是与Mixer进行相当简单的集成，然后 Mixer 负责与后端系统连接。Mixer 提供三个核心功能： 前提条件检查。允许服务在响应来自服务消费者的传入请求之前验证一些前提条件。前提条件包括认证，黑白名单，ACL检查等等。 配额管理。使服务能够在多个维度上分配和释放配额。典型例子如限速。 遥测报告。使服务能够上报日志和监控。 Istio-AuthIstio-Auth提供强大的服务到服务和终端用户认证，使用交互TLS，内置身份和凭据管理。它可用于升级服务网格中的未加密流量，并为运维人员提供基于服务身份而不是网络控制实施策略的能力。Istio的未来版本将增加细粒度的访问控制和审计，以使用各种访问控制机制（包括基于属性和角色的访问控制以及授权钩子）来控制和监视访问您的服务，API或资源的人员。其中包括三个组件：身份，密钥管理和通信安全。在这个例子中, 服务A以服务帐户“foo”运行, 服务B以服务帐户“bar”运行, 他们之间的通讯原来是没有加密的. 但是Istio在不修改代码的情况, 依托Envoy形成的服务网格, 直接在客户端Envoy和服务器端Envoy之间进行通讯加密。目前在Kubernetes上运行的 Istio，使用Kubernetes service account/服务帐户来识别运行该服务的人员。未来则规划有非常多的特性： 细粒度授权和审核 安全Istio组件（Mixer, Pilot等） 集群间服务到服务认证 使用JWT/OAuth2/OpenID_Connect终端到服务的认证 支持GCP服务帐户和AWS服务帐户 非http流量（MySql，Redis等）支持 Unix域套接字，用于服务和Envoy之间的本地通信 中间代理支持 可插拔密钥管理组件需要提醒的是：这些功能都是不改动业务应用代码的前提下实现的。 Benefits of IstioFleet-wide Visibility: Failures happen, and operators need tools to stay on top of the health of clusters and their graphs of microservices. Istio produces detailed monitoring data about application and network behaviors that is rendered using Prometheus &amp; Grafana, and can be easily extended to send metrics and logs to any collection, aggregation and querying system. Istio enables analysis of performance hotspots and diagnosis(性能热点和诊断) of distributed failure modes with Zipkin tracing. 存在问题Istio毕竟目前才是0.4 release版本，毕竟5月份才出来，因此还是存在大量的问题，集中表现为： 仅对k8s有较好的支持，其他的支持还在完善 性能较低,后续版本有待提高(Envoy每次访问请求Mixer API导致性能下降) 很多功能尚未完成]]></content>
      <categories>
        <category>MicroService</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>msa</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[历史博文迁移]Java8新特性学习笔记]]></title>
    <url>%2F2017%2F12%2F20%2F%E5%8E%86%E5%8F%B2%E5%8D%9A%E6%96%87%E8%BF%81%E7%A7%BB-Java8%E6%96%B0%E7%89%B9%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[原地址在我的segmentfault专栏里:https://segmentfault.com/a/1190000010137914,现进行部分博文迁移. 从Java8发布到现在有好几年了,而Java9也提上发布日程了(没记错的话好像就是这个月2017年7月，也许会再度跳票吧，不过没关系，稳定大于一切，稳定了再发布也行)，现在才开始去真正学习，说来也是惭愧。虽然目前工作环境仍然以Java6为主，不过Java8目前已是大势所趋了。Java8带来了许多令人激动的新特性，如lambda表达式，StreamsAPI与并行集合计算，新的时间日期API(借鉴joda-time)，字节码支持保存方法参数名(对于框架开发真的是非常赞的一个特性),Optional类解决空指针问题(虽然Guava中早就有这个了)等。 lambda表达式语法: v-&gt;System.out.println(v) (v)-&gt;System.out.println(v) (String v)-&gt;System.out.println(v) (v)-&gt;{System.out.println(v);return v+1;} 12List&lt;Integer&gt; numbers = Arrays.asList(1, 2, 3, 4, 5, 6);numbers.forEach((Integer value) -&gt; System.out.println(value)); 注意：lambda表达式内如果引用了外部的局部变量，那么这个局部变量必须是final的，如果不是，编译器会自动给加上final。比如下面这段是错误的12345//这是错误的int num = 2;Function&lt;Integer, Integer&gt; stringConverter = (from) -&gt; from * num;num++;//会报错，因为此时num已经是final声明的了System.out.println(stringConverter.apply(3)); 函数式接口： 仅有一个抽象方法的接口(注：Java8之后接口也可以有非抽象方法，所以此处强调只有一个抽象方法的接口) 可选注解：@FunctionalInterface ， 作用：编译器会检查，javadoc文档中会特别说明。 这里需要强调的是，函数式接口只能有一个抽象方法，而不是指只能有一个方法。这分两点来说明。首先，在Java 8中，接口运行存在实例方法（见默认方法一节），其次任何被java.lang.Object实现的方法，都不能视为抽象方法，因此，下面的NonFunc接口不是函数式接口，因为equals()方法在java.lang.Object中已经实现。 标准函数式接口新的 java.util.function 包定义旨在使用 lambdas 的广泛函数式接口。这些接口分为几大类： Function：接受一个参数，基于参数值返回结果 Predicate：接受一个参数，基于参数值返回一个布尔值 BiFunction：接受两个参数，基于参数值返回结果 Supplier：不接受参数，返回一个结果 Consumer：接受一个参数，无结果 (void) 123interface NonFunc &#123;boolean equals(Object obj);&#125; 同理，下面实现的IntHandler接口符合函数式接口要求，虽然看起来它不像，但实际上它是一个完全符合规范的函数式接口。12345@FunctionalInterfacepublic static interface IntHandler&#123; void handle(int i); boolean equals(Object obj);&#125; 接口默认方法在Java 8之前的版本，接口只能包含抽象方法，Java 8 对接口做了进一步的增强。在接口中可以添加使用 default 关键字修饰的非抽象方法。还可以在接口中定义静态方法。如今，接口看上去与抽象类的功能越来越类似了。这一改进使得Java 8拥有了类似于多继承的能力。一个对象实例，将拥有来自于多个不同接口的实例方法。比如，对于接口IHorse，实现如下：123456public interface IHorse&#123; void eat(); default void run()&#123; System.out.println(“hourse run”); &#125;&#125; 在Java 8中，使用default关键字，可以在接口内定义实例方法。注意，这个方法是并非抽象方法，而是拥有特定逻辑的具体实例方法。 所有的动物都能自由呼吸，所以，这里可以再定义一个IAnimal接口，它也包含一个默认方法breath()。12345public interface IAnimal &#123; default void breath()&#123; System.out.println(“breath”); &#125;&#125; 骡是马和驴的杂交物种，因此骡（Mule）可以实现为IHorse，同时骡也是动物，因此有：1234567891011public class Mule implements IHorse,IAnimal&#123; @Override public void eat() &#123; System.out.println(“Mule eat”); &#125; public static void main(String[] args) &#123; Mule m=new Mule(); m.run(); m.breath(); &#125;&#125; 注意上述代码中Mule实例同时拥有来自不同接口的实现方法。在这Java 8之前是做不到的。从某种程度上说，这种模式可以弥补Java单一继承的一些不便。但同时也要知道，它也将遇到和多继承相同的问题，如果IDonkey也存在一个默认的run()方法，那么同时实现它们的Mule，就会不知所措，因为它不知道应该以哪个方法为准。此时，由于IHorse和IDonkey拥有相同的默认实例方法，故编译器会抛出一个错误：Duplicate default methods named run with the parameters () and () are inherited from the types IDonkey and IHorse 接口默认实现对于整个函数式编程的流式表达式非常重要。比如，大家熟悉的java.util.Comparator接口，它在JDK 1.2时就已经被引入。在Java 8中，Comparator接口新增了若干个默认方法，用于多个比较器的整合。其中一个常用的默认如下：12345678default Comparator&lt;T&gt; thenComparing(Comparator&lt;? super T&gt; other) &#123; Objects.requireNonNull(other); return (Comparator&lt;T&gt; &amp; Serializable) (c1, c2) -&gt; &#123; int res = compare(c1, c2); return (res != 0) ? res : other.compare(c1, c2); &#125;;&#125; 有个这个默认方法，在进行排序时，我们就可以非常方便得进行元素的多条件排序，比如，如下代码构造一个比较器，它先按照字符串长度排序，继而按照大小写不敏感的字母顺序排序。12Comparator&lt;String&gt; cmp = Comparator.comparingInt(String::length).thenComparing(String.CASE_INSENSITIVE_ORDER); 接口静态方法：在接口中，还允许定义静态的方法。接口中的静态方法可以直接用接口来调用。例如，下面接口中定义了一个静态方法 find，该方法可以直接用 StaticFunInterface .find() 来调用。1234567891011public interface StaticFunInterface &#123;public static int find()&#123;return 1;&#125;&#125;public class TestStaticFun &#123;public static void main(String[] args)&#123;//接口中定义了静态方法 find 直接被调用StaticFunInterface.fine();&#125;&#125; 说明：虽然我知道了default方法是为了便于集合接口(新的StreamsAPI)向后兼容而设计的，但是这个接口静态方法暂时还没体会其作用，可能得接触多了才会明白吧。 方法引用 静态方法引用：ClassName::methodName 实例上的实例方法引用：instanceReference::methodName (这里还可以使用this) 超类上的实例方法引用：super::methodName 类型上的实例方法引用：ClassName::methodName 构造方法引用：Class::new 数组构造方法引用：TypeName[]::new 123456789public class InstanceMethodRef &#123; public static void main(String[] args) &#123; List&lt;User&gt; users=new ArrayList&lt;User&gt;(); for(int i=1;i&lt;10;i++)&#123; users.add(new User(i,”billy”+Integer.toString(i))); &#125; users.stream().map(User::getName).forEach(System.out::println); &#125;&#125; 注意几点：对于第一个方法引用User::getName，表示User类的实例方法。在执行时，Java会自动识别流中的元素（这里指User实例）是作为调用目标还是调用方法的参数。一般来说，如果使用的是静态方法，或者调用目标明确，那么流内的元素会自动作为参数使用。如果函数引用表示实例方法，并且不存在调用目标，那么流内元素就会自动作为调用目标。因此，如果一个类中存在同名的实例方法和静态函数，那么编译器就会感到很困惑，因为此时，它不知道应该使用哪个方法进行调用。 Stream API打开 Collection Api可以看到多了一个 stream() default 方法:123default Stream&lt;E&gt; stream() &#123; return StreamSupport.stream(spliterator(), false);&#125; Stream 允许以声明方式处理集合等可以转换为 Stream 的数据, 他有很多特点: 内部迭代 :与原有的 Iterator 不同, Stream 将迭代操作(类似 for / for-each )全部固化到了Api内部实现, 用户只需传入表达计算逻辑的 lambda 表达式(可以理解为 Supplier 、 Function 这些的 @FunctionalInterface 的实现), Stream 便会自动迭代- 数据触发计算逻辑并生成结果. 内部迭代主要解决了两方面的问题: 避免集合处理时的套路和晦涩 ; 便于库内部实现的多核并行优化 . 流水线 :很多 Stream 操作会再返回一个 Stream , 这样多个操作就可以链接起来, 形成一个大的流水线, 使其看起来像是 对数据源进行数据库式查询 , 这也就让自动优化成为可能, 如 隐式并行 . 隐式并行 :如将 .stream() 替换为 .parallelStream() , Stream 则会自动启用Fork/Join框架, 并行执行各条流水线, 并最终自动将结果进行合并. 延迟计算 :由于 Stream 大部分的操作(如 filter() 、 generate() 、 map() …)都是接受一段 lambda 表达式, 逻辑类似接口实现(可以看成是 回调 ), 因此代码并不是立即执行的, 除非流水线上触发一个终端操作, 否则中间操作不会执行任何处理. 短路求值 :有些操作不需要处理整个流就能够拿到结果, 很多像 anyMatch() 、 allMatch() 、 limit() , 只要找到一个元素他们的工作就可以结束, 也就没有必要执行后面的操作, 因此如果后面有大量耗时的操作, 此举可大大节省性能. Stream 构成一个流管道(Stream pipeline)通常由3部分构成: 数据源(Source) -&gt; 中间操作/转换(Transforming) -&gt; 终端操作/执行(Operations) : Stream 由数据源生成, 经由中间操作串联起来的一条流水线的转换, 最后由终端操作触发执行拿到结果. 1、数据源-Stream生成除了前面介绍过的 collection.stream() , 流的生成方式多种多样, 可简单概括为3类: 通用流 、 数值流 、 其他 , 其中以 通用流最为常用, 数值流是Java为 int 、 long 、 double 三种数值类型防 拆装箱 成本所做的优化:A、通用流 Arrays.stream(T[] array) Stream.empty() Stream.generate(Supplier s) 返回无限无序流，其中每个元素由Supplier生成. Stream.iterate(T seed, UnaryOperator f) 返回无限有序流。 Stream.of(T… values) Stream.concat(Stream&lt;? extends T&gt; a, Stream&lt;? extends T&gt; b) 创建一个懒惰连接的流，其元素是第一个流的所有元素，后跟第二个流的所有元素. StreamSupport.stream(Spliterator spliterator, boolean parallel) 从Spliterator创建一个新的顺序流或并行流。. B、数值流 Arrays.stream(Xxx[] array) Returns a sequential Int/Long/DoubleStream with the specified array as its source. XxxStream.empty() Returns an empty sequential Int/Long/DoubleStream . XxxStream.generate(XxxSupplier s) Returns an infinite sequential unordered stream where each element is generated by the provided Int/Long/DoubleSupplier . XxxStream.iterate(Xxx seed, XxxUnaryOperator f) Returns an infinite sequential ordered Int/Long/DoubleStream like as Stream.iterate(T seed, UnaryOperator f) XxxStream.of(Xxx… values) Returns a sequential ordered stream whose elements are the specified values. XxxStream.concat(XxxStream a, XxxStream b) Creates a lazily concatenated stream whose elements are all the elements of the first stream followed by all the elements of the second stream. Int/LongStream.range(startInclusive, endExclusive) Returns a sequential ordered Int/LongStream from startInclusive (inclusive) to endExclusive (exclusive) by an incremental step of 1. Int/LongStream.rangeClosed(startInclusive, endInclusive) Returns a sequential ordered Int/LongStream from startInclusive (inclusive) to endInclusive (inclusive) by an incremental step of 1. C、其他C.1、I/O Stream BufferedReader.lines() C.2、File Stream Files.lines(Path path) Files.find(Path start, int maxDepth, BiPredicate matcher, FileVisitOption… options) DirectoryStream newDirectoryStream(Path dir) Files.walk(Path start, FileVisitOption… options) C.3、Jar JarFile.stream() C.4、Random Random.ints() Random.longs() Random.doubles() C.5、Pattern splitAsStream(CharSequence input) … 另外, 三种数值流之间, 以及数值流与通用流之间都可以相互转换: 数值流转换: doubleStream.mapToInt(DoubleToIntFunction mapper) 、 intStream.asLongStream() … 数值流转通用流: longStream.boxed() 、 intStream.mapToObj(IntFunction&lt;? extends U&gt; mapper) … 通用流转数值流: stream.flatMapToInt(Function&lt;? super T,? extends IntStream&gt; mapper) 、 stream.mapToDouble(ToDoubleFunction&lt;? super T&gt; mapper) … 中间操作-Stream转换所有的中间操作都会返回另一个 Stream , 这让多个操作可以链接起来组成中间操作链, 从而形成一条流水线, 因此它的特点就是前面提到的 延迟执行 : 触发流水线上触发一个终端操作, 否则中间操作不执行任何处理. filter(Predicate&lt;? super T&gt; predicate) distinct() Returns a stream consisting of the distinct elements (according to Object.equals(Object) ) of this stream. limit(long maxSize) skip(long n) sorted(Comparator&lt;? super T&gt; comparator) map(Function&lt;? super T,? extends R&gt; mapper) Returns a stream consisting of the results of applying the given function to the elements of this stream. flatMap(Function&lt;? super T,? extends Stream&lt;? extends R&gt;&gt; mapper) Returns a stream consisting of the results of replacing each element of this stream with the contents of a mapped stream produced by applying the - provided mapping function to each element. peek(Consumer&lt;? super T&gt; action) Returns a stream consisting of the elements of this stream, additionally performing the provided action on each element as elements are consumed from the resulting stream. 这里着重讲解下 flatMap()假设我们有这样一个字符串list: List strs = Arrays.asList(“hello”, “alibaba”, “world”); 如何列出里面各不相同的字符呢?12Stream&lt;Stream&lt;String&gt;&gt; streamStream = strs.stream() .map(str -&gt; Arrays.stream(str.split(""))); 我们将 String 分解成 String[] 后再由 Arrays.stream() 将 String[] 映射成 Stream , 但这个结果是我们不想看到的: 我们明明想要的是 Stream 却得到的是 Stream]]></content>
      <categories>
        <category>读书笔记</category>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java8</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[历史博文迁移]Spring-Boot学习笔记]]></title>
    <url>%2F2017%2F12%2F20%2F%E5%8E%86%E5%8F%B2%E5%8D%9A%E6%96%87%E8%BF%81%E7%A7%BB-Spring-Boot%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[原地址在我的segmentfault专栏里:https://segmentfault.com/a/1190000009888085,现进行部分博文迁移. Spring-Boot 1.5 学习笔记使用Spring Boot很容易创建一个独立运行（运行jar,内嵌Servlet容器）、准生产级别的基于Spring框架的项目，使用Spring Boot你可以不用或者只需要很少的Spring配置。 Spring将很多魔法带入了Spring应用程序的开发之中，其中最重要的是以下四个核心。 自动配置：针对很多Spring应用程序常见的应用功能，Spring Boot能自动提供相关配置 起步依赖：告诉Spring Boot需要什么功能，它就能引入需要的库。 命令行界面：这是Spring Boot的可选特性，借此你只需写代码就能完成完整的应用程序，无需传统项目构建。 Actuator：让你能够深入运行中的Spring Boot应用程序，一探究竟。 Java版本：推荐使用java8 构建一个Sping Boot的Maven项目，强烈推荐Spring Initializr,它从本质上来说就是一个Web应用程序，它能为你生成Spring Boot项目结构。Spring Initializr有几种用法： 1、通过Web界面使用，访问：http://start.spring.io/ 2、通过ide相关插件创建 @SpringBootApplication是Sprnig Boot项目的核心注解，主要目的是开启自动配置。使用命令 mvn spring-boot:run”在命令行启动该应用 配置文件spring-boot配置文件application.properties支持的属性列表：http://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html1、直接在要使用的地方通过注解@Value(value=”${configName}”)就可以绑定到你想要的属性上面。2、在application.properties中的各个参数之间也可以直接引用来使用。 123com.name=&quot;111&quot;com.want=&quot;222&quot;com.dudu.yearhope=$&#123;com.name&#125;-$&#123;com.want&#125; 3、有时候属性太多了，一个个绑定到属性字段上太累，官方提倡绑定一个对象的bean，这里我们建一个ConfigBean.java类，顶部需要使用注解@ConfigurationProperties(prefix = “com.xxx”)来指明使用哪个.这点可以参考：org.springframework.boot.autoconfigure.jdbc.DataSourceProperties类的写法 这里配置完还需要在spring Boot入口类加上@EnableConfigurationProperties并指明要加载哪个bean比如：@EnableConfigurationProperties(DataSourceProperties.class) 4、有时候我们不希望把所有配置都放在application.properties里面，这时候我们可以另外定义一个，如test.properties,路径跟也放在src/main/resources下面。我们新建一个bean类,如下： 12345678@Configuration@ConfigurationProperties(prefix = &quot;com.md&quot;) @PropertySource(&quot;classpath:test.properties&quot;)public class ConfigTestBean &#123; private String name; private String want; // 省略getter和setter&#125; 5、随机值配置配置文件中${random} 可以用来生成各种不同类型的随机值，从而简化了代码生成的麻烦，例如 生成 int 值、long 值或者 string 字符串。 1234dudu.number=$&#123;random.int&#125;dudu.uuid=$&#123;random.uuid&#125;dudu.number.less.than.ten=$&#123;random.int(10)&#125;dudu.number.in.range=$&#123;random.int[1024,65536]&#125; 6、外部配置-命令行参数配置如java -jar xx.jar –server.port=9090其中server.port是application.properties里面的选项 7、Profile-多环境配置在Spring Boot中多环境配置文件名需要满足application-{profile}.properties的格式，其中{profile}对应你的环境标识，比如：application-dev.properties：开发环境application-prod.properties：生产环境想要使用对应的环境，有两种方式 只需要在application.properties中使用spring.profiles.active属性来设置，值对应上面提到的{profile}，这里就是指dev、prod这2个。 当然你也可以用命令行启动的时候带上参数：java -jar xxx.jar –spring.profiles.active=dev 还可以像这样设置SPRING_PROFILES_ACTIVE环境变量：$ export SPRING_PROFILES_ACTIVE=production 在代码里，我们还可以直接用@Profile注解来进行配置如下：12345678910111213141516171819202122/** * 测试数据库 */@Component@Profile(&quot;testdb&quot;)public class TestDBConnector implements DBConnector &#123; @Override public void configure() &#123; System.out.println(&quot;testdb&quot;); &#125;&#125;/** * 生产数据库 */@Component@Profile(&quot;devdb&quot;)public class DevDBConnector implements DBConnector &#123; @Override public void configure() &#123; System.out.println(&quot;devdb&quot;); &#125;&#125; 通过在配置文件激活具体使用哪个实现类spring.profiles.active=testdb 启动原理解析123456@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; @SpringBootApplication与@EnableAutoConfiguration@SpringBootApplication背后：打开源码看看，有三个Annotation原注解： @Configuration（@SpringBootConfiguration点开查看发现里面还是应用了@Configuration） @EnableAutoConfiguration @ComponentScan(如果不指定basePackages等属性，则默认Spring框架实现会从声明@ComponentScan所在类的package进行扫描。) @EnableAutoConfiguration这个Annotation最为重要(点开源码看看),Spring框架提供的各种名字为@Enable开头的Annotation定义？比如@EnableScheduling、@EnableCaching、@EnableMBeanExport等，@EnableAutoConfiguration的理念和做事方式其实一脉相承，简单概括一下就是，借助@Import的支持，收集和注册特定场景相关的bean定义。 @EnableScheduling是通过@Import将Spring调度框架相关的bean定义都加载到IoC容器。 @EnableMBeanExport是通过@Import将JMX相关的bean定义加载到IoC容器。 而@EnableAutoConfiguration也是借助@Import的帮助，将所有符合自动配置条件的bean定义加载到IoC容器，仅此而已！ 借助于Spring框架原有的一个工具类：SpringFactoriesLoader的支持，@EnableAutoConfiguration可以智能的自动配置SpringFactoriesLoader属于Spring框架私有的一种扩展方案，其主要功能就是从指定的配置文件META-INF/spring.factories加载配置。@EnableAutoConfiguration自动配置的魔法骑士是从classpath中搜寻所有的META-INF/spring.factories配置文件，并将其中org.springframework.boot.autoconfigure.EnableutoConfiguration对应的配置项(主要在SpringBoot的autoconfigure依赖包中)通过反射（Java Refletion）实例化为对应的标注了@Configuration的JavaConfig形式的IoC容器配置类，然后汇总为一个并加载到IoC容器。 SpringApplication执行流程1） 如果使用的是SpringApplication.run静态方法，那么，这个方法里面首先要创建一个SpringApplication实例，在初始化的时候，它会提前做几件事情： 根据classpath里面是否存在某个特征类（org.springframework.web.context.ConfigurableWebApplicationContext）来决定是否应该创建一个为Web应用使用的ApplicationContext类型。 使用SpringFactoriesLoader在应用的classpath中查找并加载所有可用的ApplicationContextInitializer,ApplicationListener 2）执行run方法的逻辑，首先遍历执行所有通过SpringFactoriesLoader可以查找到并加载的SpringApplicationRunListener。调用它们的started()方法。然后创建并配置当前Spring Boot应用将要使用的Environment（包括配置要使用的PropertySource以及Profile）。然后遍历调用所有SpringApplicationRunListener的environmentPrepared()的方法。之后，如果SpringApplication的showBanner属性被设置为true，则打印banner。 3） 根据用户是否明确设置了applicationContextClass类型以及初始化阶段的推断结果，决定该为当前SpringBoot应用创建什么类型的ApplicationContext并创建完成，然后根据条件决定是否添加ShutdownHook，决定是否使用自定义的BeanNameGenerator，决定是否使用自定义的ResourceLoader，当然，最重要的，将之前准备好的Environment设置给创建好的ApplicationContext使用。 4） ApplicationContext创建好之后，遍历调用先前找到的ApplicationContextInitializer的initialize（applicationContext）方法来对已经创建好的ApplicationContext进行进一步的处理。5） 遍历调用所有SpringApplicationRunListener的contextPrepared()方法。6） 最核心的一步，将之前通过@EnableAutoConfiguration获取的所有配置以及其他形式的IoC容器配置加载到已经准备完毕的ApplicationContext。7） 遍历调用所有SpringApplicationRunListener的contextLoaded()方法。8） 调用ApplicationContext的refresh()方法，完成IoC容器可用的最后一道工序。9） 查找当前ApplicationContext中是否注册有CommandLineRunner，如果有，则遍历执行它们。10） 正常情况下，遍历执行SpringApplicationRunListener的finished()方法. 模板引擎Spring Boot为Spring MVC提供适用于多数应用的自动配置功能。在Spring默认基础上，自动配置添加了以下特性： 引入ContentNegotiatingViewResolver和BeanNameViewResolver beans。 对静态资源的支持，包括对WebJars的支持。 自动注册Converter，GenericConverter，Formatter beans。 对HttpMessageConverters的支持。 自动注册MessageCodeResolver。 对静态index.html的支持。 对自定义Favicon的支持。 如果想全面控制Spring MVC，你可以添加自己的@Configuration，并使用@EnableWebMvc对其注解。如果想保留Spring Boot MVC的特性，并只是添加其他的MVC配置(拦截器，formatters，视图控制器等)，你可以添加自己的WebMvcConfigurerAdapter类型的@Bean（不使用@EnableWebMvc注解）例如：配置一个拦截器12345678910111213@Configurationpublic class WebConfiguration extends WebMvcConfigurerAdapter &#123; @Bean public RemoteIpFilter remoteIpFilter() &#123; return new RemoteIpFilter(); &#125; @Bean public LocaleChangeInterceptor localeChangeInterceptor() &#123; return new LocaleChangeInterceptor(); &#125; @Override public void addInterceptors(InterceptorRegistry registry &#123; registry.addInterceptor(localeChangeInterceptor()); &#125;&#125; Spring Boot 默认为我们提供了静态资源处理，使用 WebMvcAutoConfiguration 中的配置各种属性。建议大家使用Spring Boot的默认配置方式，提供的静态资源映射如下: classpath:/META-INF/resources classpath:/resources classpath:/static classpath:/public 这使用了Spring MVC的ResourceHttpRequestHandler. Spring Boot支持多种模版引擎包括： FreeMarker Groovy Thymeleaf(官方推荐) Mustache JSP技术Spring Boot官方是不推荐的，原因有三： tomcat只支持war的打包方式，不支持可执行的jar。 Jetty 嵌套的容器不支持jsp 创建自定义error.jsp页面不会覆盖错误处理的默认视图，而应该使用自定义错误页面 当你使用上述模板引擎中的任何一个，它们默认的模板配置路径为：src/main/resources/templates。当然也可以修改这个路径 配置错误页面Spring Boot自动配置的默认错误处理器会查找名为error的视图，如果找不到就用默认的白标错误视图，如图3-1所示。因此，最简单的方法就是创建一个自定义视图，让解析出的视图名为error。这一点归根到底取决于错误视图解析时的视图解析器。  实现了Spring的View接口的Bean，其 ID为error（由Spring的BeanNameViewResolver所解析）。  如果配置了Thymeleaf，则有名为error.html的Thymeleaf模板。  如果配置了FreeMarker，则有名为error.ftl的FreeMarker模板。  如果配置了Velocity，则有名为error.vm的Velocity模板。  如果是用JSP视图，则有名为error.jsp的JSP模板。 Spring Boot会为错误视图提供如下错误属性。  timestamp：错误发生的时间。  status：HTTP状态码。  error：错误原因。  exception：异常的类名。  message：异常消息（如果这个错误是由异常引起的）。  errors：BindingResult异常里的各种错误（如果这个错误是由异常引起的）。  trace：异常跟踪信息（如果这个错误是由异常引起的）。  path：错误发生时请求的URL路径。 默认日志logback配置spring-boot-starter-logging根据不同的日志系统，你可以按如下规则组织配置文件名，就能被正确加载： Logback：logback-spring.xml, logback-spring.groovy, logback.xml Log4j：log4j-spring.properties, log4j-spring.xml, log4j.properties, log4j.xml Log4j2：log4j2-spring.xml, log4j2.xml 如果你不想用logback.xml作为Logback配置的名字，可以通过logging.config属性指定自定义的名字：logging.config=classpath:logging-config.xml 多环境日志输出据不同环境（prod:生产环境，test:测试环境，dev:开发环境）来定义不同的日志输出，在 logback-spring.xml中使用 springProfile 节点来定义，方法如下：文件名称不是logback.xml，想使用spring扩展profile支持，要以logback-spring.xml命名12345678&lt;!-- 测试环境+开发环境. 多个使用逗号隔开. --&gt;&lt;springProfile name=&quot;test,dev&quot;&gt; &lt;logger name=&quot;com.dudu.controller&quot; level=&quot;info&quot; /&gt;&lt;/springProfile&gt;&lt;!-- 生产环境. --&gt;&lt;springProfile name=&quot;prod&quot;&gt; &lt;logger name=&quot;com.dudu.controller&quot; level=&quot;ERROR&quot; /&gt;&lt;/springProfile&gt; 可以启动服务的时候指定 profile （如不指定使用默认），如指定prod 的方式为：java -jar xxx.jar –spring.profiles.active=prod 数据源与事务配置1、使用普通jdbc12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-jdbc&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt; application.properties中配置数据源信息。1234spring.datasource.url = jdbc:mysql://localhost:3306/spring?useUnicode=true&amp;characterEncoding=utf-8spring.datasource.username = rootspring.datasource.password = rootspring.datasource.driver-class-name = com.mysql.jdbc.Driver 自定义数据源12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.0.19&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314151617181920212223242526272829@SpringBootApplicationpublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125; @Autowired private Environment env; //destroy-method=&quot;close&quot;的作用是当数据库连接不使用的时候,就把该连接重新放到数据池中,方便下次使用调用. @Bean(destroyMethod = &quot;close&quot;) public DataSource dataSource() &#123; DruidDataSource dataSource = new DruidDataSource(); dataSource.setUrl(env.getProperty(&quot;spring.datasource.url&quot;)); dataSource.setUsername(env.getProperty(&quot;spring.datasource.username&quot;));//用户名 dataSource.setPassword(env.getProperty(&quot;spring.datasource.password&quot;));//密码 dataSource.setDriverClassName(env.getProperty(&quot;spring.datasource.driver-class-name&quot;)); dataSource.setInitialSize(2);//初始化时建立物理连接的个数 dataSource.setMaxActive(20);//最大连接池数量 dataSource.setMinIdle(0);//最小连接池数量 dataSource.setMaxWait(60000);//获取连接时最大等待时间，单位毫秒。 dataSource.setValidationQuery(&quot;SELECT 1&quot;);//用来检测连接是否有效的sql dataSource.setTestOnBorrow(false);//申请连接时执行validationQuery检测连接是否有效 dataSource.setTestWhileIdle(true);//建议配置为true，不影响性能，并且保证安全性。 dataSource.setPoolPreparedStatements(false);//是否缓存preparedStatement，也就是PSCache return dataSource; &#125;&#125; 覆盖Spring Boot 配置Spring Boot自动配置自带了很多配置类，每一个都能运用在你的应用程序里。它们都使用了Spring 4.0的条件化配置，可以在运行时判断这个配置是该被运用，还是该被忽略。如:12345@Bean@ConditionalOnMissingBean(JdbcOperations.class)public JdbcTemplate jdbcTemplate() &#123;return new JdbcTemplate(this.dataSource);&#125; 条件注解有如下： @ConditionalOnBean 配置了某个特定Bean @ConditionalOnMissingBean 没有配置特定的Bean @ConditionalOnClass Classpath里有指定的类 @ConditionalOnMissingClass Classpath里缺少指定的类 @ConditionalOnExpression 给定的Spring Expression Language（SpEL）表达式计算结果为true @ConditionalOnJava Java的版本匹配特定值或者一个范围值 @ConditionalOnJndi 参数中给定的JNDI位置必须存在一个，如果没有给参数，则要有JNDI InitialContext @ConditionalOnProperty 指定的配置属性要有一个明确的值 @ConditionalOnResource Classpath里有指定的资源 @ConditionalOnWebApplication 这是一个Web应用程序 @ConditionalOnNotWebApplication 这不是一个Web应用程序 单元测试12345678@RunWith(SpringRunner.class)@SpringBootTest(classes = App.class)public MyTest&#123; @Test public void test1()&#123; &#125;&#125; 测试Web 应用程序1、普通测试 123456789101112131415161718192021@RunWith(SpringRunner.class)@SpringBootTest(classes = App.class)@WebAppConfigurationpublic class MockMvcWebTests &#123; @Autowired private WebApplicationContext webContext; private MockMvc mockMvc; @Before public void setupMockMvc() &#123; mockMvc = MockMvcBuilders.webAppContextSetup(webContext).build(); &#125; @Test public void homePage() throws Exception &#123; mockMvc.perform(MockMvcRequestBuilders.get("/readingList")) .andExpect(MockMvcResultMatchers.status().isOk()) .andExpect(MockMvcResultMatchers.view().name("readingList")) .andExpect(MockMvcResultMatchers.model().attributeExists("books")) .andExpect(MockMvcResultMatchers.model().attribute("books", Matchers.is(Matchers.empty()))); &#125;&#125; 首先向/readingList发起一个GET请求，接下来希望该请求处理成功（isOk()会判断HTTP 200响应码），并且视图的逻辑名称为readingList。测试还要断定模型包含一个名为books的属性，该属性是一个空集合。所有的断言都很直观。 2、测试运行中的应用程序在测试类上添加@Web-IntegrationTest注解，可以声明你不仅希望Spring Boot为测试创建应用程序上下文，还要启动一个嵌入式的Servlet容器。一旦应用程序运行在嵌入式容器里，你就可以发起真实的HTTP请求，断言结果了。这里采用@WebIntegration-Test，在服务器里启动了应用程序, 以Spring的RestTemplate对应用程序发起HTTP请求。Spring-boot 1.4以后推荐采用SpringBootTest(webEnvironment=WebEnvironment.DEFINED_PORT) (or RANDOM_PORT)来代替@WebIntegrationTest,而此类在1.5已经被移除了。https://github.com/spring-projects/spring-boot/wiki/Spring-Boot-1.4-Release-Notes 12345678910111213141516171819@RunWith(SpringRunner.class)@SpringBootTest(classes = App.class,webEnvironment=WebEnvironment.RANDOM_PORT)public class WebInRuntimeTest &#123; @Value("$&#123;local.server.port&#125;") private int port; @Autowired private TestRestTemplate template; @Test public void test1()&#123;// Assert.state(true,"测试成功"); System.out.println(port); ResponseEntity&lt;String&gt; response=template.getForEntity("/test/111", String.class); System.out.println(response.getStatusCodeValue()); System.out.println(response.getHeaders()); System.out.println(response.getBody()); Assert.hasText("111","true"); &#125;&#125; 用随机端口启动服务器,@WebIntegrationTest的value属性接受一个String数组，数组中的每项都是键值对，形如name=value，用来设置测试中使用的属性。要设置server.port，你可以这样做：@WebIntegrationTest(“server.port:0”) //使用0表示端口号随机，也可以具体指定如8888这样的固定端口.或者直接这样@WebIntegrationTest(randomPort=true) Spring Boot将local.server.port的值设置为了选中的端口。我们只需使用Spring的@Value注解将其注入即可：12@Value("$&#123;local.server.port&#125;")private int port; 3、使用Selenium 测试HTML 页面12345678910111213141516171819202122232425262728@RunWith(SpringRunner.class)@SpringBootTest(classes = App.class)@WebIntegrationTest(randomPort=true)public class ServerWebTests &#123; private static FirefoxDriver browser; @Value("$&#123;local.server.port&#125;") private int port; @BeforeClass public static void openBrowser() &#123; browser = new FirefoxDriver(); browser.manage().timeouts().implicitlyWait(10, TimeUnit.SECONDS); &#125; @Test public void addBookToEmptyList() &#123; String baseUrl = "http://localhost:" + port; browser.get(baseUrl); assertEquals("You have no books in your book list",browser.findElementByTagName("div").getText()); browser.findElementByName("title").sendKeys("BOOK TITLE"); browser.findElementByTagName("form").submit(); WebElement dl = browser.findElementByCssSelector("dt.bookHeadline"); assertEquals("BOOK TITLE by BOOK AUTHOR (ISBN: 1234567890)",dl.getText()); &#125; @AfterClass public static void closeBrowser() &#123; browser.quit(); &#125;&#125; 外部tomcat部署war配置当我们不想使用内嵌tomcat部署时，我们也可以使用外部tomcat，并打包成war：1、继承SpringBootServletInitializer外部容器部署的话，就不能依赖于Application的main函数了，而是要以类似于web.xml文件配置的方式来启动Spring应用上下文，此时我们需要在启动类中继承SpringBootServletInitializer并实现configure方法： 123456public class Application extends SpringBootServletInitializer &#123; @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) &#123; return application.sources(Application.class); &#125;&#125; 这个类的作用与在web.xml中配置负责初始化Spring应用上下文的监听器作用类似，只不过在这里不需要编写额外的XML文件了。 SpringBootServletInitializer是一个支持Spring WebApplicationInitializer实现。除了配置Spring的Dispatcher-Servlet，还会在Spring应用程序上下文里查找Filter、Servlet或ServletContextInitializer类型的Bean，把它们绑定到Servlet容器里。 还有一点值得注意：就算我们在构建的是WAR文件，这个文件仍旧可以脱离应用服务器直接运行。如果你没有删除Application里的main()方法，构建过程生成的WAR文件仍可直接运行，一如可执行的JAR文件：$ java -jar readinglist-0.0.1-SNAPSHOT.war这样一来，同一个部署产物就能有两种部署方式了！ 2、pom.xml修改tomcat相关的配置如果要将最终的打包形式改为war的话，还需要对pom.xml文件进行修改，因为spring-boot-starter-web中包含内嵌的tomcat容器，所以直接部署在外部容器会冲突报错。所以需要进行依赖排除123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 开发常用的热部署方式汇总 Spring Loaded spring-boot-devtools IDE的JRebel插件 1、Spring Loaded 实现热部署Spring Loaded是一个用于在JVM运行时重新加载类文件更改的JVM代理,Spring Loaded允许你动态的新增/修改/删除某个方法/字段/构造方法,同样可以修改作用在类/方法/字段/构造方法上的注解.也可以新增/删除/改变枚举中的值。 spring-loaded是一个开源项目,项目地址:https://github.com/spring-projects/spring-loaded Spring Loaded有两种方式实现，分别是Maven引入依赖方式或者添加启动参数方式1234567891011&lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;springloaded&lt;/artifactId&gt; &lt;version&gt;1.2.6.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/plugin&gt; 启动：mvn spring-boot:run注意：maven依赖的方式只适合spring-boot:run的启动方式，右键那种方式不行。 出现如下配置表实配置成功：[INFO] Attaching agents: [C:\Users\tengj.m2\repository\org\springframework\springloaded\1.2.6.R 添加启动参数方式这种方式是右键运行启动类.首先先下载对应的springloaded-xxx.RELEASE.jar，可以去上面提到的官网获取,在VM options中输入-javaagent:&lt; pathTo &gt;/springloaded-{VERSION}.jar 上面2种方式随便选择一种即可,当系统通过 mvn spring-boot:run启动或者 右键application debug启动Java文件时，系统会监视classes文件，当有classes文件被改动时，系统会重新加载类文件，不用重启启动服务。注：IDEA下需要重新编译文件 Ctrl+Shift+F9或者编译项目 Ctrl+F9 在 Spring Boot，模板引擎的页面默认是开启缓存，如果修改页面内容，刷新页面是无法获取修改后的页面内容，所以，如果我们不需要模板引擎的缓存，可以进行关闭。1234spring.freemarker.cache=falsespring.thymeleaf.cache=falsespring.velocity.cache=falsespring.mustache.cache=false 不过还是有一些情况下需要重新启动，不可用的情况如下： 1：对于一些第三方框架的注解的修改，不能自动加载，比如：spring mvc的@RequestMapping 2：application.properties的修改也不行 3：log4j的配置文件的修改不能即时生效 2、spring-boot-devtools 实现热部署spring-boot-devtools为应用提供一些开发时特性，包括默认值设置，自动重启，livereload等。1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 将依赖关系标记为可选&lt; optional &gt;true&lt; /optional&gt;是一种最佳做法，可以防止使用项目将devtools传递性地应用于其他模块。在Spring Boot集成Thymeleaf时，spring.thymeleaf.cache属性设置为false可以禁用模板引擎编译的缓存结果。现在，devtools会自动帮你做到这些，禁用所有模板的缓存，包括Thymeleaf, Freemarker, Groovy Templates, Velocity, Mustache等。 自动重启的原理在于spring boot使用两个classloader：不改变的类（如第三方jar）由base类加载器加载，正在开发的类由restart类加载器加载。应用重启时，restart类加载器被扔掉重建，而base类加载器不变，这种方法意味着应用程序重新启动通常比“冷启动”快得多，因为base类加载器已经可用并已填充。 所以，当我们开启devtools后，classpath中的文件变化会导致应用自动重启。当然不同的IDE效果不一样，Eclipse中保存文件即可引起classpath更新(注：需要打开自动编译)，从而触发重启。而IDEA则需要自己手动CTRL+F9重新编译一下（感觉IDEA这种更好，不然每修改一个地方就重启，好蛋疼） 排除静态资源文件静态资源文件在改变之后有时候没必要触发应用程序重启，例如thymeleaf模板文件就可以实时编辑，默认情况下，更改/META-INF/maven, /META-INF/resources ,/resources ,/static ,/public 或/templates下的资源不会触发重启，而是触发live reload（devtools内嵌了一个LiveReload server，当资源发生改变时，浏览器刷新,需要浏览器插件支持）。 可以使用spring.devtools.restart.exclude属性配置，例如1spring.devtools.restart.exclude=static/**,public/** 如果想保留默认配置，同时增加新的配置，则可使用spring.devtools.restart.additional-exclude属性 观察额外的路径如果你想观察不在classpath中的路径的文件变化并触发重启，则可以配置 spring.devtools.restart.additional-paths 属性。 不在classpath内的path可以配置spring.devtools.restart.additionalpaths属性来增加到监视中，同时配置spring.devtools.restart.exclude可以选择这些path的变化是导致restart还是live reload。 关闭自动重启设置 spring.devtools.restart.enabled 属性为false，可以关闭该特性。可以在application.properties中设置，也可以通过设置环境变量的方式。1234public static void main(String[] args) &#123; System.setProperty(&quot;spring.devtools.restart.enabled&quot;, &quot;false&quot;); SpringApplication.run(MyApp.class, args);&#125; 使用一个触发文件若不想每次修改都触发自动重启，可以设置spring.devtools.restart.trigger-file指向某个文件，只有更改这个文件时才触发自动重启。 自定义自动重启类加载器默认时，IDE中打开的项目都会由restart加载器加载，jar文件由Base加载器加载，但是若你使用multi-module的项目，并且不是所有模块都被导入到IDE中，此时会导致加载器不一致。这时你可以创建META-INF/spring-devtools.properties文件，并增加restart.exclude.XXX，restart.include.XXX来配置哪些jar被restart加载，哪些被base加载。如：12restart.include.companycommonlibs=/mycorp-common-[\\w-]+\.jarrestart.include.projectcommon=/mycorp-myproj-[\\w-]+\.jar 如果您不想在应用程序运行时启动LiveReload服务器，则可以将spring.devtools.livereload.enabled属性设置为false。一次只能运行一个LiveReload服务器。开始应用程序之前，请确保没有其他LiveReload服务器正在运行。如果你的IDE启动多个应用程序，则只有第一个应用程序将支持LiveReload。 3、JRebel插件方式 ：略 数据库迁移库支持Spring Boot为两款流行的数据库迁移库提供了自动配置支持。 Flyway（http://flywaydb.org） Liquibase（http://www.liquibase.org） 用Flyway定义数据库迁移过程 1234&lt;dependency&gt;&lt;groupId&gt;org.flywayfb&lt;/groupId&gt;&lt;artifactId&gt;flyway-core&lt;/artifactId&gt;&lt;/dependency&gt; Flyway是一个非常简单的开源数据库迁移库，使用SQL来定义迁移脚本。它的理念是，每个脚本都有一个版本号，Flyway会顺序执行这些脚本，让数据库达到期望的状态。它也会记录已执行的脚本状态，不会重复执行。，Flyway脚本就是SQL。让其发挥作用的是其在Classpath里的位置和文件名。Flyway脚本都遵循一个命名规范，V版本号描述.sql,如`V1initdb.sql`Flyway脚本需要放在src/main/resources/db/migration里。你还需要将spring.jpa.hibernate.ddl-auto设置为none，由此告知Hibernate不要创建数据表。 原理：在应用程序部署并运行起来后，Spring Boot会检测到Classpath里的Flyway，自动配置所需的Bean。Flyway会依次查看/db/migration里的脚本，如果没有执行过就运行这些脚本。每个脚本都执行过后，向schema_version表里写一条记录。应用程序下次启动时，Flyway会先看schema_version里的记录，跳过那些脚本。 用Liquibase定义数据库迁移过程相比Flyway的优点，数据库无关性，脚本不是用sql写，而是支持yaml,json,xml等格式。1234&lt;dependency&gt;&lt;groupId&gt;org.liquibase&lt;/groupId&gt;&lt;artifactId&gt;liquibase-core&lt;/artifactId&gt;&lt;/dependency&gt; 具体介绍：略。 Actuator监控应用程序状态运行中的应用程序就像礼物盒。你可以刺探它，作出合理的推测，猜测它的运行情况。但如何了解真实的情况呢？有没有一种办法能让你深入应用程序内部一窥究竟，了解它的行为，检查它的健康状况，甚至触发一些操作来影响应用程序呢？Spring Boot的Actuator。它提供了很多生产级的特性，比如监控和度量Spring Boot应用程序。Actuator的这些特性可以通过众多REST端点、远程shell和JMX获得。 1234&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 1、Actuator的Web端点端点可以分为三大类：配置端点、度量端点和其他端点 GET /autoconfig 提供了一份自动配置报告，记录哪些自动配置条件通过了，哪些没通过 GET /configprops 描述配置属性（包含默认值）如何注入Bean GET /beans 描述应用程序上下文里全部的Bean，以及它们的关系 GET /dump 获取线程活动的快照 GET /env 获取全部环境属性 GET /env/{name} 根据名称获取特定的环境属性值 GET /health 报告应用程序的健康指标，这些值由HealthIndicator的实现类提供 GET /info 获取应用程序的定制信息，这些信息由info打头的属性提供 GET /mappings 描述全部的URI路径，以及它们和控制器（包含Actuator端点）的映射关系 GET /metrics 报告各种应用程序度量信息，比如内存用量和HTTP请求计数 GET /metrics/{name} 报告指定名称的应用程序度量值 POST /shutdown 关闭应用程序，要求endpoints.shutdown.enabled设置为true GET /trace 提供基本的HTTP请求跟踪信息（时间戳、HTTP头等） /beans端点产生的报告能告诉你Spring应用程序上下文里都有哪些Bean。/autoconfig端点能告诉你为什么会有这个Bean，或者为什么没有这个Bean。Spring Boot自动配置构建于Spring的条件化配置之上。它提供了众多带有@Conditional注解的配置类，根据条件决定是否要自动配置这些Bean。/autoconfig端点提供了一个报告，列出了计算过的所有条件，根据条件是否通过进行分组。 /env端点会生成应用程序可用的所有环境属性的列表，无论这些属性是否用到。这其中包括环境变量、JVM属性、命令行参数，以及applicaition.properties或application.yml文件提供的属性。 /metrics端点提供了一些针对Web请求的基本计数器和计时器，但那些度量值缺少详细信息。知道所处理请求的更多信息是很有帮助的，尤其是在调试时，所以就有了/trace这个端点。/trace端点能报告所有Web请求的详细信息，包括请求方法、路径、时间戳以及请求和响应的头信息 2、使用:CRaSH shell 3、通过JMX 监控应用程序 参考：Book: Spring-Boot In Action嘟嘟独立博客https://github.com/tengj/SpringBootDemo]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringBoot</category>
      </categories>
      <tags>
        <tag>SpringBoot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[editor]vscode快捷键]]></title>
    <url>%2F2017%2F12%2F19%2Feditor-vscode%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[最近从Sublime3切换到VScode，总结下快捷键。 官方地址：https://code.visualstudio.com/shortcuts/keyboard-shortcuts-windows.pdf 简单的配置 12345678&#123; &quot;files.autoSave&quot;: &quot;off&quot;, //禁用自动保存 &quot;workbench.iconTheme&quot;: &quot;vs-minimal&quot;, &quot;explorer.autoReveal&quot;: false, //禁止资源管理器在打开文件时自动显示并选择它们,类似于禁用Eclipse的link editor &quot;workbench.editor.enablePreviewFromQuickOpen&quot;: false, //使Ctrl+P打开的文件使用新的tab页，而不是替换已有的 &quot;workbench.editor.enablePreview&quot;: false //使得鼠标左键打开的文件使用新的tab页，而不是替换已有的&#125; 1、通用Ctrl+Shif+P , F1 打开命令面板 Ctrl+P 快速打开Ctrl+Shift+N 打开新实例窗口 Ctrl+Shift+W 关闭窗口实例 2、基础编辑Ctrl+X 剪切 Ctrl+C 复制Alt+下/上 移动行 shift+Alt+上/下 复制行Ctrl+Shift+K 删除行 ,改成Ctrl+DCtrl+(Shift)+Enter 插入行Ctrl+Shift+\ 跳转到匹配的括号Ctrl+[/] 行缩进Ctrl+Shift+[/] 代码折叠Ctrl+/ 行注释，Shift+Alt+A 块注释 ,改成Ctrl+shif+/Alt+Z 是否换行(word wrap) Ctrl+空格键 智能提示 ,改成Alt+/Ctrl+Shift+Space 参数提示，Tab 自动补全Ctrl+K Ctrl+I 显示悬停(类似于鼠标hover悬停，一般用于触发提示)Shift+Alt+F 格式化文档(改成Ctrl+Shift+F)，Ctrl+K Ctrl+F 格式化选中代码F12 跳转定义，Alt+F12 查看定义 分别改成F3，Alt+F3Ctrl+K F12 在侧边打开定义Ctrl+. 快速修复Shift+F12 显示引用F2 重命名变量Ctrl+K M 更改文件语言类型 3、导航Ctrl+T 显示所有变量、函数名等 #Ctrl+G 跳转行Ctrl+P 打开文件Ctrl+Shift+O 跳转到变量、函数等@Ctrl+Shift+M 显示终端、错误等程序面板F8 跳转到下一个错误或警告,改成Ctrl+,Shift+F8 跳转到上一个错误或警告, 改成ctrl+shift+,Ctrl+Shift+Tab 切换编辑器,我改成了Ctrl+EAlt+左/右 向前/后Ctrl+M 切换tab焦点 4、搜索和替换Ctrl+F , Ctrl+H , F3/SHift+F3Alt+Enter 选中所有匹配搜索的 5、多光标，选择，多行编辑Ctrl+I 选中当前行Alt+Click 插入多个光标Ctrl+Alt+上/下 插入多个光标 ，改成Ctrl++Shift+Alt+上/下Ctrl+U 撤销上一次光标操作Shift+Alt+I 在选中的所有行末尾插入光标Ctrl+Shift+L , Ctrl+F2 都可以选中文中所有和当前的选择或单词同名的，重构重命名时很方便Shift+Alt+左/右 缩小、扩大选择区块Shift+Alt+鼠标拖拽 ， Ctrl+Shift+Alt+方向键 列选择Ctrl+Shift+Alt+PgUp/PgDown 列页选择 6、编辑器管理Ctrl+W, Ctrl+F4 关闭当前编辑器 , Ctrl+K Ctrl+W关闭所有Ctrl+Shift+T 重新打开上一次关闭的编辑器Ctrl+K F 关闭目录Ctrl+\ 分割编辑器Ctrl+1/2/3 转移编辑器焦点到不同编辑组Ctrl+K (Ctrl+)左/右 转移编辑器焦点到左右组Shift+F10显示上下文菜单 7、文件管理Ctrl+N 新建文件，Ctrl+O 打开文件Ctrl+S , Ctrl+Shift+S , Ctrl+K S 保存，另存为，保存所有Ctrl+K P 复制文件路径Ctrl+K R 在资源管理器中打开文件Ctrl+K O 在新窗口打开文件 8、显示F11 全屏Shift+Alt+1 改变编辑器布局Ctrl+ =/- 放大或缩小Ctrl+B 开关侧边栏 Ctrl+Shift+E 焦点放到ExplorerCtrl+Shift+F 焦点放到搜索,改成ctrl+alt+fCtrl+Shift+G 焦点放GitCtrl+Shift+D 焦点放到DebugCtrl+Shift+X 焦点放到扩展Ctrl+Shift+H replace in files 9、调试F9 设置断点F5 开始/继续Shift+F5 停止F11/Shift+F11 step into/outF10 step overCtrl+K Ctrl+I show hover 10、终端集成Ctrl+显示集成的终端 Ctrl+Shift+ 创建新的终端Ctrl+Shift+C 复制选中Ctrl+Shift+V 粘贴到终端Ctrl+↑ / ↓ Scroll up/downShift+PgUp / PgDown Scroll page up/downCtrl+Home / End Scroll to top/bottom 针对Window快捷键冲突和Eclipse习惯改造自定义的部分12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758// 将键绑定放入此文件中以覆盖默认值[&#123; &quot;key&quot;: &quot;alt+/&quot;, &quot;command&quot;: &quot;editor.action.triggerSuggest&quot;, &quot;when&quot;: &quot;editorHasCompletionItemProvider &amp;&amp; editorTextFocus &amp;&amp; !editorReadonly&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+d&quot;, &quot;command&quot;: &quot;editor.action.deleteLines&quot;, &quot;when&quot;: &quot;editorTextFocus &amp;&amp; !editorReadonly&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+shift+/&quot;, &quot;command&quot;: &quot;editor.action.blockComment&quot;, &quot;when&quot;: &quot;editorTextFocus &amp;&amp; !editorReadonly&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+shift+f&quot;, &quot;command&quot;: &quot;editor.action.formatDocument&quot;, &quot;when&quot;: &quot;editorHasDocumentFormattingProvider &amp;&amp; editorTextFocus &amp;&amp; !editorReadonly&quot; &#125;, &#123; &quot;key&quot;: &quot;f3&quot;, &quot;command&quot;: &quot;editor.action.goToDeclaration&quot;, &quot;when&quot;: &quot;editorHasDefinitionProvider &amp;&amp; editorTextFocus &amp;&amp; !isInEmbeddedEditor&quot; &#125;, &#123; &quot;key&quot;: &quot;alt+f3&quot;, &quot;command&quot;: &quot;editor.action.goToImplementation&quot;, &quot;when&quot;: &quot;editorHasImplementationProvider &amp;&amp; editorTextFocus &amp;&amp; !isInEmbeddedEditor&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+e&quot;, &quot;command&quot;: &quot;workbench.action.openPreviousRecentlyUsedEditorInGroup&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+,&quot;, &quot;command&quot;: &quot;editor.action.marker.next&quot;, &quot;when&quot;: &quot;editorFocus &amp;&amp; !editorReadonly&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+shift+,&quot;, &quot;command&quot;: &quot;editor.action.marker.prev&quot;, &quot;when&quot;: &quot;editorFocus &amp;&amp; !editorReadonly&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+shift+alt+up&quot;, &quot;command&quot;: &quot;editor.action.insertCursorAbove&quot;, &quot;when&quot;: &quot;editorTextFocus&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+shift+alt+down&quot;, &quot;command&quot;: &quot;editor.action.insertCursorBelow&quot;, &quot;when&quot;: &quot;editorTextFocus&quot; &#125;, &#123; &quot;key&quot;: &quot;ctrl+alt+f&quot;, &quot;command&quot;: &quot;search.action.focusActiveEditor&quot;, &quot;when&quot;: &quot;searchInputBoxFocus &amp;&amp; searchViewletVisible&quot; &#125;] 推荐插件：Auto Close TagBeautifyDebugger for chromeMarkdown All in OnenpmPaste Image - 用hexo写博客，自动粘贴写md文档特别爽vscode-hexo]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>vscode</tag>
        <tag>editor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SpringCloud官文笔记]SpringCloud-Contract]]></title>
    <url>%2F2017%2F12%2F19%2FSpringCloud%E5%AE%98%E6%96%87%E7%AC%94%E8%AE%B0-SpringCloud-Contract%2F</url>
    <content type="text"><![CDATA[You need confidence when pushing new features to a new application or service in a distributed system. This project provides support for Consumer Driven Contracts and service schemas in Spring applications (for both HTTP and message-based interactions), covering a range of options for writing tests, publishing them as assets, and asserting that a contract is kept by producers and consumers. Spring Cloud Contract Verifier IntroductionSpring Cloud Contract Verifier enables Consumer Driven Contract (CDC) development of JVM-based applications. It moves TDD to the level of software architecture. Spring Cloud Contract Verifier ships with Contract Definition Language (CDL). Contract definitions are used to produce the following resources: JSON stub definitions to be used by WireMock when doing integration testing on the client code (client tests). Test code must still be written by hand, and test data is produced by Spring Cloud Contract Verifier. Messaging routes, if you’re using a messaging service. We integrate with Spring Integration, Spring Cloud Stream, Spring AMQP, and Apache Camel. You can also set your own integrations. Acceptance tests (in JUnit or Spock) are used to verify if server-side implementation of the API is compliant with the contract (server tests). A full test is generated by Spring Cloud Contract Verifier. Why a Contract Verifier?让我们设想如下场景，一个系统由很多微服务组成：那么如何测试其中的某个微服务呢。一般而言，有两种方式：1、部署所有的微服务，然后进行end-to-end测试优点：1、模拟生产环境；2、测试服务之间的真实交互缺点：必须全部部署，会化很长时间，反馈比较慢，难以debug。2、Mock其他的微服务进行单元/集成测试。优点：快速反馈，不需要基础设施。缺点：需要服务创建stubs，而一般地这是没有实际意义的；You can go to production with passing tests and failing production. Spring Cloud Contract Verifier with Stub Runner就是为了解决上述问题而创建的。 官方例子:https://github.com/spring-cloud-samples/spring-cloud-contract-samples 通常我们开发中主要由服务提供方约定接口，虽然提供方架构调整或改变接口之前通常会通知消费者，但可能还存在上述风险，如果上线出现问题就GG了，而CDC则是以消费者提出接口契约，交由服务提供方实现，并以测试用例对契约进行产生约束，所以服务提供方在满足测试用例的情况下可以自行更改接口或架构实现而不影响消费者。 消费者驱动的契约测试（Consumer-Driven Contracts，简称CDC），是指从消费者业务实现的角度出发，驱动出契约，再基于契约，对提供者验证的一种测试方式。 服务端是基于消费端的契约来开发接口，而测试用例由ContractVerifier依据锲约生成，因此就形成了对契约的约束，也就是消费端对服务提供方的约束，如果服务端不能满足测试用例则就不能通过测试。在消费端，开发者也是基于功能而产生的符合自己需求的契约，并编写了单元测试，因此就形成了完整的开发测试流程，并且能更早的发现服务端接口变动，确保了服务的可用性。使用SpringCloudContracts可以满足CDC测试. Spring Cloud Contract Verifier with Stub Runner的目标: To ensure that WireMock/Messaging stubs (used when developing the client) do exactly what the actual server-side implementation does. To promote ATDD method and Microservices architectural style. To provide a way to publish changes in contracts that are immediately visible on both sides. To generate boilerplate test code to be used on the server side. How It Works1、Defining the contract：定义约束规则和我们的期望值Assume that you want to send a request containing the ID of a client company and the amount it wants to borrow from us. You also want to send it to the /fraudcheck url via the PUT method.12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455org.springframework.cloud.contract.spec.Contract.make &#123; request &#123; // (1) method 'PUT' // (2) url '/fraudcheck' // (3) body([ // (4) "client.id": $(regex('[0-9]&#123;10&#125;')), loanAmount: 99999 ]) headers &#123; // (5) contentType('application/json') &#125; &#125; response &#123; // (6) status 200 // (7) body([ // (8) fraudCheckStatus: "FRAUD", "rejection.reason": "Amount too high" ]) headers &#123; // (9) contentType('application/json') &#125; &#125;&#125;/*From the Consumer perspective, when shooting a request in the integration test:(1) - If the consumer sends a request(2) - With the "PUT" method(3) - to the URL "/fraudcheck"(4) - with the JSON body that * has a field `clientId` that matches a regular expression `[0-9]&#123;10&#125;` * has a field `loanAmount` that is equal to `99999`(5) - with header `Content-Type` equal to `application/json`(6) - then the response will be sent with(7) - status equal `200`(8) - and JSON body equal to &#123; "fraudCheckStatus": "FRAUD", "rejectionReason": "Amount too high" &#125;(9) - with header `Content-Type` equal to `application/json`From the Producer perspective, in the autogenerated producer-side test:(1) - A request will be sent to the producer(2) - With the "PUT" method(3) - to the URL "/fraudcheck"(4) - with the JSON body that * has a field `clientId` that will have a generated value that matches a regular expression `[0-9]&#123;10&#125;` * has a field `loanAmount` that is equal to `99999`(5) - with header `Content-Type` equal to `application/json`(6) - then the test will assert if the response has been sent with(7) - status equal `200`(8) - and JSON body equal to &#123; "fraudCheckStatus": "FRAUD", "rejectionReason": "Amount too high" &#125;(9) - with header `Content-Type` matching `application/json.*` */ 2、Client Side Spring Cloud Contract generates stubs, which you can use during client-side testing. You get a running WireMock instance/Messaging route that simulates the service. You would like to feed that instance with a proper stub definition.At some point in time, you need to send a request to the Fraud Detection service.1234ResponseEntity&lt;FraudServiceResponse&gt; response = restTemplate.exchange("http://localhost:" + port + "/fraudcheck", HttpMethod.PUT, new HttpEntity&lt;&gt;(request, httpHeaders), FraudServiceResponse.class); Annotate your test class with @AutoConfigureStubRunner. In the annotation provide the group id and artifact id for the Stub Runner to download stubs of your collaborators.12345@RunWith(SpringRunner.class)@SpringBootTest(webEnvironment=WebEnvironment.NONE)@AutoConfigureStubRunner(ids = &#123;"com.example:http-server-dsl:+:stubs:6565"&#125;, workOffline = true)@DirtiesContextpublic class LoanApplicationServiceTests &#123; After that, during the tests, Spring Cloud Contract automatically finds the stubs (simulating the real service) in the Maven repository and exposes them on a configured (or random) port. 3、Server SideSince you are developing your stub, you need to be sure that it actually resembles your concrete implementation.To ensure that your application behaves the way you define in your stub, tests are generated from the stub you provide.The autogenerated test looks like this:12345678910111213141516171819@Testpublic void validate_shouldMarkClientAsFraud() throws Exception &#123; // given: MockMvcRequestSpecification request = given() .header("Content-Type", "application/vnd.fraud.v1+json") .body("&#123;\"client.id\":\"1234567890\",\"loanAmount\":99999&#125;"); // when: ResponseOptions response = given().spec(request) .put("/fraudcheck"); // then: assertThat(response.statusCode()).isEqualTo(200); assertThat(response.header("Content-Type")).matches("application/vnd.fraud.v1.json.*"); // and: DocumentContext parsedJson = JsonPath.parse(response.getBody().asString()); assertThatJson(parsedJson).field("['fraudCheckStatus']").matches("[A-Z]&#123;5&#125;"); assertThatJson(parsedJson).field("['rejection.reason']").isEqualTo("Amount too high");&#125; 太过复杂，此处不作翻译，入门建议参考：http://blog.csdn.net/j3t9z7h/article/details/78590351http://www.infoq.com/cn/news/2017/04/spring-cloud-contracthttp://www.sohu.com/a/200331844_617676]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>springcloud</tag>
        <tag>msa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SpringCloud官文笔记]SpringCloud-Security]]></title>
    <url>%2F2017%2F12%2F19%2FSpringCloud%E5%AE%98%E6%96%87%E7%AC%94%E8%AE%B0-SpringCloud-Security%2F</url>
    <content type="text"><![CDATA[Building on Spring Boot and Spring Security OAuth2 we can quickly create systems that implement common patterns like single sign on, token relay and token exchange. Quickstart实例地址：https://github.com/spring-cloud-samples/sso/blob/master/src/main/java/demo/SsoApplication.java More Detail略。待细看]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>springcloud</tag>
        <tag>msa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SpringCloud官文笔记]SpringCloud-Sleuth]]></title>
    <url>%2F2017%2F12%2F19%2FSpringCloud%E5%AE%98%E6%96%87%E7%AC%94%E8%AE%B0-SpringCloud-Sleuth%2F</url>
    <content type="text"><![CDATA[Spring Cloud Sleuth implements a distributed tracing solution for Spring Cloud.它是基于google的Dapper实现的。Span：基本工作单元，span通过一个64位ID唯一标识，trace以另一个64位ID表示，span还有其他数据信息，比如摘要、时间戳事件、键值对注释、span的ID、以及span父ID等span在不断的启动和停止，同时记录了时间信息，当你创建了一个span，你必须在未来的某个时刻停止它。初始化span称为”root span”,该span的id和trace的id相等。Trace：一系列spans组成的一个树状结构(root span为根共享),trace也用一个64位的id唯一标识，trace中的所有span都共享该trace id。Annotation：用来及时记录一个事件的存在，一些核心annotations用来定义一个请求的开始和结束 cs - Client Sent -客户端发起一个请求，这个annotion描述了这个span的开始 sr - Server Received -服务端获得请求并准备开始处理它，如果将其sr减去cs时间戳，便可得到网络延迟 ss - Server Sent -注解表明请求处理的完成(当请求返回客户端)，如果ss减去sr时间戳，便可得到服务端需要的处理请求所需的时间 cr - Client Received -表明span的结束，客户端成功接收到服务端的回复，如果cr减去cs时间戳，便可得到客户端从服务端获取回复的所有所需时间 Each color of a note signifies a span (7 spans - from A to G). If you have such information in the note:123Trace Id = XSpan Id = DClient Sent That means that the current span has Trace-Id set to X, Span-Id set to D. It also has emitted Client Sent event.span的父子关系如下： Distributed tracing with ZipkinAltogether there are 7 spans . If you go to traces in Zipkin you will see this number in the second trace:However if you pick a particular trace then you will see 4 spans:注意：When picking a particular trace you will see merged spans. That means that if there were 2 spans sent to Zipkin with Server Received and Server Sent / Client Received and Client Sent annotations then they will presented as a single span. Why is there a difference between the 7 and 4 spans in this case? 2 spans come from http:/start span. It has the Server Received (SR) and Server Sent (SS) annotations. 2 spans come from the RPC call from service1 to service2 to the http:/foo endpoint. It has the Client Sent (CS) and Client Received (CR) annotations on service1 side. It also has Server Received (SR) and Server Sent (SS) annotations on the service2 side. Physically there are 2 spans but they form 1 logical span related to an RPC call. 2 spans come from the RPC call from service2 to service3 to the http:/bar endpoint. It has the Client Sent (CS) and Client Received (CR) annotations on service2 side. It also has Server Received (SR) and Server Sent (SS) annotations on the service3 side. Physically there are 2 spans but they form 1 logical span related to an RPC call. 2 spans come from the RPC call from service2 to service4 to the http:/baz endpoint. It has the Client Sent (CS) and Client Received (CR) annotations on service2 side. It also has Server Received (SR) and Server Sent (SS) annotations on the service4 side. Physically there are 2 spans but they form 1 logical span related to an RPC call.So if we count the physical spans we have 1 from http:/start, 2 from service1 calling service2, 2 form service2 calling service3 and 2 from service2 calling service4. Altogether 7 spans. Logically we see the information of Total Spans: 4 because we have 1 span related to the incoming request to service1 and 3 spans related to RPC calls. Visualizing errorsThen if you click on one of the spans you’ll see the following Live examplesThe dependency graph in Zipkin would look like this: Log correlation：1234567service1.log:2016-02-26 11:15:47.561 INFO [service1,2485ec27856c56f4,2485ec27856c56f4,true] 68058 --- [nio-8081-exec-1] i.s.c.sleuth.docs.service1.Application : Hello from service1. Calling service2service2.log:2016-02-26 11:15:47.710 INFO [service2,2485ec27856c56f4,9aa10ee6fbde75fa,true] 68059 --- [nio-8082-exec-1] i.s.c.sleuth.docs.service2.Application : Hello from service2. Calling service3 and then service4service3.log:2016-02-26 11:15:47.895 INFO [service3,2485ec27856c56f4,1210be13194bfe5,true] 68060 --- [nio-8083-exec-1] i.s.c.sleuth.docs.service3.Application : Hello from service3service2.log:2016-02-26 11:15:47.924 INFO [service2,2485ec27856c56f4,9aa10ee6fbde75fa,true] 68059 --- [nio-8082-exec-1] i.s.c.sleuth.docs.service2.Application : Got response from service3 [Hello from service3]service4.log:2016-02-26 11:15:48.134 INFO [service4,2485ec27856c56f4,1b1845262ffba49d,true] 68061 --- [nio-8084-exec-1] i.s.c.sleuth.docs.service4.Application : Hello from service4service2.log:2016-02-26 11:15:48.156 INFO [service2,2485ec27856c56f4,9aa10ee6fbde75fa,true] 68059 --- [nio-8082-exec-1] i.s.c.sleuth.docs.service2.Application : Got response from service4 [Hello from service4]service1.log:2016-02-26 11:15:48.182 INFO [service1,2485ec27856c56f4,2485ec27856c56f4,true] 68058 --- [nio-8081-exec-1] i.s.c.sleuth.docs.service1.Application : Got response from service2 [Hello from service2, response from service3 [Hello from service3] and from service4 [Hello from service4]] If you’re using a log aggregating tool like Kibana, Splunk etc. you can order the events that took place. An example of Kibana would look like this: Logstash配置123456filter &#123; # pattern matching logback pattern grok &#123; match =&gt; &#123; &quot;message&quot; =&gt; &quot;%&#123;TIMESTAMP_ISO8601:timestamp&#125;\s+%&#123;LOGLEVEL:severity&#125;\s+\[%&#123;DATA:service&#125;,%&#123;DATA:trace&#125;,%&#123;DATA:span&#125;,%&#123;DATA:exportable&#125;\]\s+%&#123;DATA:pid&#125;\s+---\s+\[%&#123;DATA:thread&#125;\]\s+%&#123;DATA:class&#125;\s+:\s+%&#123;GREEDYDATA:rest&#125;&quot; &#125; &#125;&#125; JSON Logback with Logstash:添加依赖：net.logstash.logback:logstash-logback-encoder:4.6，12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;configuration&gt; &lt;include resource="org/springframework/boot/logging/logback/defaults.xml"/&gt; ​ &lt;springProperty scope="context" name="springAppName" source="spring.application.name"/&gt; &lt;!-- Example for logging into the build folder of your project --&gt; &lt;property name="LOG_FILE" value="$&#123;BUILD_FOLDER:-build&#125;/$&#123;springAppName&#125;"/&gt;​ &lt;!-- You can override this to have a custom pattern --&gt; &lt;property name="CONSOLE_LOG_PATTERN" value="%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;)&#123;faint&#125; %clr($&#123;LOG_LEVEL_PATTERN:-%5p&#125;) %clr($&#123;PID:- &#125;)&#123;magenta&#125; %clr(---)&#123;faint&#125; %clr([%15.15t])&#123;faint&#125; %clr(%-40.40logger&#123;39&#125;)&#123;cyan&#125; %clr(:)&#123;faint&#125; %m%n$&#123;LOG_EXCEPTION_CONVERSION_WORD:-%wEx&#125;"/&gt; &lt;!-- Appender to log to console --&gt; &lt;appender name="console" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;!-- Minimum logging level to be presented in the console logs--&gt; &lt;level&gt;DEBUG&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- Appender to log to file --&gt;​ &lt;appender name="flatfile" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;$&#123;LOG_FILE&#125;&lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;LOG_FILE&#125;.%d&#123;yyyy-MM-dd&#125;.gz&lt;/fileNamePattern&gt; &lt;maxHistory&gt;7&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; ​ &lt;!-- Appender to log to file in a JSON format --&gt; &lt;appender name="logstash" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;$&#123;LOG_FILE&#125;.json&lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;LOG_FILE&#125;.json.%d&#123;yyyy-MM-dd&#125;.gz&lt;/fileNamePattern&gt; &lt;maxHistory&gt;7&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder"&gt; &lt;providers&gt; &lt;timestamp&gt; &lt;timeZone&gt;UTC&lt;/timeZone&gt; &lt;/timestamp&gt; &lt;pattern&gt; &lt;pattern&gt; &#123; "severity": "%level", "service": "$&#123;springAppName:-&#125;", "trace": "%X&#123;X-B3-TraceId:-&#125;", "span": "%X&#123;X-B3-SpanId:-&#125;", "parent": "%X&#123;X-B3-ParentSpanId:-&#125;", "exportable": "%X&#123;X-Span-Export:-&#125;", "pid": "$&#123;PID:-&#125;", "thread": "%thread", "class": "%logger&#123;40&#125;", "rest": "%message" &#125; &lt;/pattern&gt; &lt;/pattern&gt; &lt;/providers&gt; &lt;/encoder&gt; &lt;/appender&gt; ​ &lt;root level="INFO"&gt; &lt;appender-ref ref="console"/&gt; &lt;!-- uncomment this to have also JSON logs --&gt; &lt;!--&lt;appender-ref ref="logstash"/&gt;--&gt; &lt;!--&lt;appender-ref ref="flatfile"/&gt;--&gt; &lt;/root&gt;&lt;/configuration&gt; Adding to the project注意：你需要在 bootstrap.yml中配置spring.application.name，这样才能在Zipkin中正确显示出来。几种配置方式：1、Sleuth with Zipkin via HTTP1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; 它默认包含了spring-cloud-starter-sleuth依赖。 2、Sleuth with Zipkin via RabbitMQ or KafkaIf you want to use RabbitMQ or Kafka instead of http, add the spring-rabbit or spring-kafka dependencies. The default destination name is zipkin. 注意: spring-cloud-sleuth-stream已经被废弃 and incompatible with these destinations12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.amqp&lt;/groupId&gt; &lt;artifactId&gt;spring-rabbit&lt;/artifactId&gt; &lt;/dependency&gt; 注意：通过spring.sleuth.sampler.percentage (default 0.1, i.e. 10%).配置采样比率，如果你的访问量不大，采样比率太低，可能会导致你在Zipkin中看不到结果。 注意：the SLF4J MDC is always set and logback users will immediately see the trace and span ids in logs per the example above. Other logging systems have to configure their own formatter to get the same result. The default is logging.pattern.level set to1%5p [$&#123;spring.zipkin.service.name:$&#123;spring.application.name:-&#125;&#125;,%X&#123;X-B3-TraceId:-&#125;,%X&#123;X-B3-SpanId:-&#125;,%X&#123;X-Span-Export:-&#125;] (this is a Spring Boot feature for logback users). This means that if you’re not using SLF4J this pattern WILL NOT be automatically applied. Example logs:1232016-02-02 15:30:57.902 INFO [bar,6bfd228dc00d216b,6bfd228dc00d216b,false] 23030 --- [nio-8081-exec-3] ...2016-02-02 15:30:58.372 ERROR [bar,6bfd228dc00d216b,6bfd228dc00d216b,false] 23030 --- [nio-8081-exec-3] ...2016-02-02 15:31:01.936 INFO [bar,46ab0d418373cbc9,46ab0d418373cbc9,false] 23030 --- [nio-8081-exec-4] ... notice the [appname,traceId,spanId,exportable] entries from the MDC: spanId - the id of a specific operation that took place appname - the name of the application that logged the span traceId - the id of the latency graph that contains the span exportable - whether the log should be exported to Zipkin or not. When would you like the span not to be exportable? In the case in which you want to wrap some operation in a Span and have it written to the logs only. Sampling自定义采样器：1234@Beanpublic Sampler defaultSampler() &#123; return new AlwaysSampler();&#125; You can set the HTTP header X-B3-Flags to 1 or when doing messaging you can set spanFlags header to 1. Then the current span will be forced to be exportable regardless of the sampling decision. InstrumentationSpring Cloud Sleuth instruments all your Spring application automatically, so you shouldn’t have to do anything to activate it. The instrumentation is added using a variety of technologies according to the stack that is available, e.g. for a servlet web application we use a Filter, and for Spring Integration we use ChannelInterceptors. Span lifecycle略 Customizations略 Sending spans to ZipkinBy default if you add spring-cloud-starter-zipkin as a dependency to your project, when the span is closed, it will be sent to Zipkin over HTTP. The communication is asynchronous. You can configure the URL by setting the spring.zipkin.baseUrl property as follows:1spring.zipkin.baseUrl: http://192.168.99.100:9411/ If you want to find Zipkin via service discovery it’s enough to pass the Zipkin’s service id inside the URL (example for zipkinserver service id)1spring.zipkin.baseUrl: http://zipkinserver/ Span Data as Messages略 Metrics略。 Integrations略]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>springcloud</tag>
        <tag>msa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SpringCloud官文笔记]SpringCloud-Bus]]></title>
    <url>%2F2017%2F12%2F19%2FSpringCloud%E5%AE%98%E6%96%87%E7%AC%94%E8%AE%B0-SpringCloud-Bus%2F</url>
    <content type="text"><![CDATA[This can then be used to broadcast state changes (e.g. configuration changes) or other management instructions. A key idea is that the Bus is like a distributed Actuator for a Spring Boot application that is scaled out, but it can also be used as a communication channel between apps. 依赖：spring-cloud-starter-bus-amqp or spring-cloud-starter-bus-kafka application.yml.123456spring: rabbitmq: host: mybroker.com port: 5672 username: user password: secret The bus currently supports sending messages to all nodes listening or all nodes for a particular service (as defined by Eureka).There are also some http endpoints under the /bus/ actuator namespace.:/bus/env, sends key/value pairs to update each node’s Spring Environment./bus/refresh, will reload each application’s configuration, just as if they had all been pinged on their /refresh endpoint. 它还支持指定destination参数./bus/refresh?destination=customers:9000,不过这里的destination指的是ApplcationConext Id，这个id是由SpingBoot在ContextIdApplicationContextInitializer中配置的，由spring.application.name, active profiles and server.port组成destination参数还支持PathMatcher的写法，如 “`/bus/refresh?destination=customers:*`” will target all instances of the “customers” service regardless of the profiles and ports set as the ApplicationContext ID. Tracing Bus Events通过spring.cloud.bus.trace.enabled=true来启用/trace端点。 Broadcasting Your Own Events事件类型必须为RemoteApplicationEvent的实现。注册自定义事件类型，两种方式：1、将事件类型放在org.springframework.cloud.bus.event包下；2、@RemoteApplicationEventScan扫描。]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>springcloud</tag>
        <tag>msa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SpringCloud官文笔记]SpringCloud-Stream]]></title>
    <url>%2F2017%2F12%2F19%2FSpringCloud%E5%AE%98%E6%96%87%E7%AC%94%E8%AE%B0-SpringCloud-Stream%2F</url>
    <content type="text"><![CDATA[Spring Cloud Stream is a framework for building message-driven microservice applications.使用SpringBoot作为独立应用，使用Spring Integration 来提供对message brokers的连接。支持几种代理中间件。核心观念:发布-订阅机制，消费组，分区通过@EnableBinding来使应用程序立马连接到message broker,通过@StreamListener来定义消费者。下面的例子定义了一个用来处理接收外部消息的应用。 注意：@EnableBinding只能注解the application’s configuration classes. 不要将其注解在普通的bean上面。 官方例子见：https://github.com/spring-cloud/spring-cloud-stream-samples 12345678910111213@SpringBootApplication@EnableBinding(Sink.class) //Source, Sink, and Processor,An interface declares input and/or output channels.public class VoteRecordingSinkApplication &#123; public static void main(String[] args) &#123; SpringApplication.run(VoteRecordingSinkApplication.class, args); &#125; @StreamListener(Sink.INPUT) public void processVote(Vote vote) &#123; votingService.recordVote(vote); &#125;&#125; Sink接口如下：123456public interface Sink &#123; String INPUT = "input"; @Input(Sink.INPUT)//An interface declares input and/or output channels.@Input,@Output SubscribableChannel input();&#125; SpringCloud Stream会为你创建接口实现，在使用时，你只需要简单地通过bean注入即可1234567891011121314@RunWith(SpringJUnit4ClassRunner.class)@SpringApplicationConfiguration(classes = VoteRecordingSinkApplication.class)@WebAppConfiguration@DirtiesContextpublic class StreamApplicationTests &#123; @Autowired private Sink sink; @Test public void contextLoads() &#123; assertNotNull(this.sink.input()); &#125;&#125; Main Concepts Spring Cloud Stream’s application model The Binder abstraction 实现支持Kafka and Rabbit MQ以及测试用的TestSupportBinder Persistent publish-subscribe support Consumer group support Partitioning support A pluggable Binder API The application communicates with the outside world through input and output channels injected into it by Spring Cloud Stream. Channels are connected to external brokers through middleware-specific Binder implementations. Stream的broker默认提供了两个实现以支持Kafka and Rabbit MQ，具体用哪个，是通过检测classpath。采用 publish-subscribe model定义消息来源：spring.cloud.stream.bindings.input.destination 消费者分组：(该分组下仅有一个消费者能消费到消息)spring.cloud.stream.bindings.&lt; channelName&gt;.group=组名All groups which subscribe to a given destination receive a copy of published data, but only one member of each group receives a given message from that destination. 消息持久性： That is, a binder implementation ensures that group subscriptions are persistent, and once at least one subscription for a group has been created, the group will receive messages, even if they are sent while all applications in the group are stopped. Anonymous subscriptions are non-durable by nature.所以，建议指定消息目的地的时候指定好分组。 主题分区：Stream提供对一个应用多个实例情况下的数据分区支持，在这种场景下，the broker topic会被视为拥有很多分区的结构，这样确保含有特定标识的数据总是会被同一个消费者实例消费。不管中间件是否原生支持，由于Stream的通用抽象，都可以使用这一特性。 To set up a partitioned processing scenario, you must configure both the data-producing and the data-consuming ends. Programming Model注解:@EnableBinding(仅可用于配置类中)支持多个channels的绑定声明，@Input,@Output定义不同通道.123456789101112131415161718public interface Barista &#123; @Input SubscribableChannel orders(); @Output MessageChannel hotDrinks(); @Input("inboundOrders") SubscribableChannel myorders();&#125;@EnableBinding(Barista.class)public class CafeConfiguration &#123; ...&#125; Source, Sink, and Processor:For easy addressing of the most common use cases, which involve either an input channel, an output channel, or both, Spring Cloud Stream provides three predefined interfaces out of the box. 访问绑定的channelsInjecting the Bound Interfaces1234567891011121314@Componentpublic class SendingBean &#123; private Source source; @Autowired public SendingBean(Source source) &#123; this.source = source; &#125; public void sayHello(String name) &#123; source.output().send(MessageBuilder.withPayload(name).build()); &#125;&#125; Injecting Channels Directly1234567891011121314@Componentpublic class SendingBean &#123; private MessageChannel output; @Autowired public SendingBean(MessageChannel output) &#123; this.output = output; &#125; public void sayHello(String name) &#123; output.send(MessageBuilder.withPayload(name).build()); &#125;&#125; 如果你自定义了channelname,那么需要修改为：1234567891011121314151617181920public interface CustomSource &#123; ... @Output("customOutput") MessageChannel output();&#125;@Componentpublic class SendingBean &#123; private MessageChannel output; @Autowired public SendingBean(@Qualifier("customOutput") MessageChannel output) &#123; this.output = output; &#125; public void sayHello(String name) &#123; this.output.send(MessageBuilder.withPayload(name).build()); &#125;&#125; Producing and Consuming Messages @StreamListener 可以自动进行类型转换1234567891011@EnableBinding(Sink.class)public class VoteHandler &#123; @Autowired VotingService votingService; @StreamListener(Sink.INPUT) public void handle(Vote vote) &#123; votingService.record(vote); &#125;&#125; 同时参数可以用@Payload, @Headers and @Header标注如果方法同时需要接收和发送消息：还需要加入@SendTo标注123456789101112@EnableBinding(Processor.class)public class TransformProcessor &#123; @Autowired VotingService votingService; @StreamListener(Processor.INPUT) @SendTo(Processor.OUTPUT) public VoteResult handle(Vote vote) &#123; return votingService.record(vote); &#125;&#125; 利用@StreamListener分发消息到多个满足不同condition的方法，要满足支持条件派发，使之生效，那么被派发的方法必须满足：1、返回值为void，2、必须是一个独立的消息处理方法(reactive API方式不被支持)同时，condition的写法是满足SpEL表达式的。如下：1234567891011121314@EnableBinding(Sink.class)@EnableAutoConfigurationpublic static class TestPojoWithAnnotatedArguments &#123; @StreamListener(target = Sink.INPUT, condition = "headers['type']=='foo'") public void receiveFoo(@Payload FooPojo fooPojo) &#123; // handle the message &#125; @StreamListener(target = Sink.INPUT, condition = "headers['type']=='bar'") public void receiveBar(@Payload BarPojo barPojo) &#123; // handle the message &#125;&#125; AggregationStream支持聚合多个application，并直接连接它们的input和output而不需要通过message broker。注意aggregation仅支持以下类型的applications: sources - applications with a single output channel named output, typically having a single binding of the type org.springframework.cloud.stream.messaging.Source sinks - applications with a single input channel named input, typically having a single binding of the type org.springframework.cloud.stream.messaging.Sink processors - applications with a single input channel named input and a single output channel named output, typically having a single binding of the type org.springframework.cloud.stream.messaging.Processor. 例如：12345678910@SpringBootApplicationpublic class SampleAggregateApplication &#123; public static void main(String[] args) &#123; new AggregateApplicationBuilder() .from(SourceApplication.class).args("--fixedDelay=5000") .via(ProcessorApplication.class) .to(SinkApplication.class).args("--debug=true").run(args); &#125;&#125; BindersbindProducer() 方法的第三个参数 contains properties (such as a partition key expression) to be used within the adapter that is created for that channel.bindConsumer()方法的第一个参数为 destination name,第二个参数为消费者组名. Binder SPI提供一些系列接口，优秀得到工具类，插件化连接中间件的自动发现策略。关键接口:Binder12345public interface Binder&lt;T, C extends ConsumerProperties, P extends ProducerProperties&gt; &#123; Binding&lt;T&gt; bindConsumer(String name, String group, T inboundBindTarget, C consumerProperties); Binding&lt;T&gt; bindProducer(String name, T outboundBindTarget, P producerProperties);&#125; A typical binder implementation consists of the following 实现Binder接口 a Spring @Configuration class that creates a bean of the type above along with the middleware connection infrastructure; a META-INF/spring.binders file found on the classpath containing one or more binder definitions, e.g.12kafka:\org.springframework.cloud.stream.binder.kafka.config.KafkaBinderConfiguration Binder DetectionClasspath Detection,比如1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 会导致自动绑定到rabbitmq Multiple Binders on the Classpath:在META-INF/spring.binders中指定, which is a simple properties file:12rabbit:\org.springframework.cloud.stream.binder.rabbit.config.RabbitServiceAutoConfiguration 除了上述的方式外也可以通过spring.cloud.stream.defaultBinder=rabbit来指定全局默认的实现，也可以指定到channel，如：12spring.cloud.stream.bindings.input.binder=kafkaspring.cloud.stream.bindings.output.binder=rabbit Connecting to Multiple Systems:1234567891011121314151617181920212223spring: cloud: stream: bindings: input: destination: foo binder: rabbit1 output: destination: bar binder: rabbit2 binders: rabbit1: type: rabbit environment: spring: rabbitmq: host: &lt;host1&gt; rabbit2: type: rabbit environment: spring: rabbitmq: host: &lt;host2&gt; Binder configuration properties前缀spring.cloud.stream.binders.&lt; configurationName&gt;.略。 Configuration Options1、Spring Cloud Stream Properties2、Binding Propertiesspring.cloud.stream.bindings.&lt; channelName&gt;.&lt; property&gt;=&lt; value&gt;spring.cloud.stream.default.&lt; property&gt;=&lt; value&gt;如:12spring.cloud.stream.bindings.input.destination=ticktockspring.cloud.stream.default.contentType=application/json 除了上面设置的destination,contentType外，还可以设置group，binder 还有一些关于消费者和生产者特定的配置，请参考官文。 Using dynamically bound destinations BinderAwareChannelResolver bean，当然也可以用spring.cloud.stream.dynamicDestinations来配destination的可选范围，否则所有的destination都是可选的。 123456789101112131415161718192021222324 @EnableBinding@Controllerpublic class SourceWithDynamicDestination &#123; @Autowired private BinderAwareChannelResolver resolver; @RequestMapping(path = "/&#123;target&#125;", method = POST, consumes = "*/*") @ResponseStatus(HttpStatus.ACCEPTED) public void handleRequest(@RequestBody String body, @PathVariable("target") target, @RequestHeader(HttpHeaders.CONTENT_TYPE) Object contentType) &#123; sendMessage(body, target, contentType); &#125; private void sendMessage(String body, String target, Object contentType) &#123; resolver.resolveDestination(target).send(MessageBuilder.createMessage(body, new MessageHeaders(Collections.singletonMap(MessageHeaders.CONTENT_TYPE, contentType)))); &#125;&#125;curl -H "Content-Type: application/json" -X POST -d "customer-1" http://localhost:8080/customerscurl -H "Content-Type: application/json" -X POST -d "order-1" http://localhost:8080/orders 类型转换支持 JSON to/from POJO JSON to/from org.springframework.tuple.Tuple Object to/from byte[] : Either the raw bytes serialized for remote transport, bytes emitted by an application, or converted to bytes using Java serialization(requires the object to be Serializable) String to/from byte[] Object to plain text (invokes the object’s toString() method) Spring Cloud Stream can handle messages based on this information in two ways: Through its contentType settings on inbound and outbound channels，通过spring.cloud.stream.bindings.&lt; channelName&gt;.content-type来配置 Through its argument mapping performed for methods annotated with @StreamListener 注意：If no content-type property is set on an outbound channel, Spring Cloud Stream will serialize the payload using a serializer based on the Kryo serialization framework.因此反序列化的时候需要保证消费端有payload class在classpath中。 MIME types：详细介绍见官文 Customizing message conversion1234567891011121314151617181920212223242526272829@EnableBinding(Sink.class)@SpringBootApplicationpublic static class SinkApplication &#123; ... @Bean public MessageConverter customMessageConverter() &#123; return new MyCustomMessageConverter(); &#125;&#125;public class MyCustomMessageConverter extends AbstractMessageConverter &#123; public MyCustomMessageConverter() &#123; super(new MimeType("application", "bar")); &#125; @Override protected boolean supports(Class&lt;?&gt; clazz) &#123; return (Bar.class == clazz); &#125; @Override protected Object convertFromInternal(Message&lt;?&gt; message, Class&lt;?&gt; targetClass, Object conversionHint) &#123; Object payload = message.getPayload(); return (payload instanceof Bar ? payload : new Bar((byte[]) payload)); &#125;&#125; Schema evolution support略 Inter-Application Communication略 Binder ImplementationsApache Kafka Binder1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-kafka&lt;/artifactId&gt;&lt;/dependency&gt; 或者1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-kafka&lt;/artifactId&gt;&lt;/dependency&gt; 具体见官文 RabbitMQ Binder1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-stream-binder-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 或1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-stream-rabbit&lt;/artifactId&gt;&lt;/dependency&gt; 具体见官文]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>springcloud</tag>
        <tag>msa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SpringCloud官文笔记]SpringCloud-Netflix]]></title>
    <url>%2F2017%2F12%2F18%2FSpringCloud%E5%AE%98%E6%96%87%E7%AC%94%E8%AE%B0-SpringCloud-Netflix%2F</url>
    <content type="text"><![CDATA[SpringCloud Netflix提供Netflix OSS的集成。如Service Discovery (Eureka), Circuit Breaker (Hystrix), Intelligent Routing (Zuul) and Client Side Load Balancing (Ribbon). Service Discovery: Eureka Clients首先添加spring-cloud-starter-netflix-eureka-client依赖。当一个client注册到Eureka server时，它提供关于自身的一些元数据如host,porthealth indicatorURL,home page etc.12345678910111213141516@Configuration@ComponentScan@EnableAutoConfiguration@RestControllerpublic class Application &#123; @RequestMapping("/") public String home() &#123; return "Hello world"; &#125; public static void main(String[] args) &#123; new SpringApplicationBuilder(Application.class).web(true).run(args); &#125;&#125; 配置application.yml.指定eureka地址。一旦检测到classpath中含有eureka client的依赖，那么就会自动把自己注册到eureka server中。1234eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 关于认证，略。 状态和健康检查：端点：”/info” and “/health”默认情况下，Eureka使用client heartbeat来决定客户端是否处于up状态。但是在默认情况中，Discovery Client不会发送目前的健康检查状态给Eureka，所以一旦client注册了，Eureka就会永远显示为up状态。所以我们需要启用 health checks功能：application.yml.1234eureka: client: healthcheck: enabled: true 需要注意的时，不要在bootstrap.yml中进行这样的配置,否则会导致UNKNOWN状态。 Eureka Metadata for Instances and Clients：metadata包含:hostname,ip,port,status page,health check. 额外的metadata可以配置eureka.instance.metadataMap Using the EurekaClient默认情况下EurekaClient使用Jersey作为Http交互工具，如果你不想引入它，可以排除掉，然后SpringCLoud会自动使用RestTemplate来代替。123456789101112131415161718&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-eureka&lt;/artifactId&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-client&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey&lt;/groupId&gt; &lt;artifactId&gt;jersey-core&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;exclusion&gt; &lt;groupId&gt;com.sun.jersey.contribs&lt;/groupId&gt; &lt;artifactId&gt;jersey-apache-client4&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt; Why is it so Slow to Register a Service?默认的心跳周期为30s，在经历3次心跳前，一个service对于discovery client是不可用的。原文如下：A service is not available for discovery by clients until the instance, the server and the client all have the same metadata in their local cache (so it could take 3 heartbeats).在开发、测试中，我们可以改变这一行为：eureka.instance.leaseRenewalIntervalInSeconds但在生产环境中最好不要这样做，因为内部有些计算规则，可以对租约续期作出假设。 Service Discovery: Eureka Server添加依赖：spring-cloud-starter-netflix-eureka-server123456789@SpringBootApplication@EnableEurekaServerpublic class Application &#123; public static void main(String[] args) &#123; new SpringApplicationBuilder(Application.class).web(true).run(args); &#125;&#125; server提供web ui监控界面，以及HTTP API endpoints per the normal Eureka functionality under /eureka/*. High Availability不管是Eureka Server还是client，都是将注册数据存储在内存中的。默认情况下Eureka Server也是一个Eureka client(通过互相注册来达到高可用)，需要配置至少一个别的EurekaServer的URL(作为peer，这是Eureka中的名词)。 Peer Awareness多个Eureka实例通过相互注册来达到高可用：123456789101112131415161718192021application.yml (Two Peer Aware Eureka Servers). ---spring: profiles: peer1eureka: instance: hostname: peer1 client: serviceUrl: defaultZone: http://peer2/eureka/---spring: profiles: peer2eureka: instance: hostname: peer2 client: serviceUrl: defaultZone: http://peer1/eureka/ Ip优先：默认情况下，注册到Eureka的时候使用hostname优先策略，如果想用ip优先，请配置eureka.instance.preferIpAddress=true Circuit Breaker: Hystrix ClientsHystrix是微服务中的circuit breaker pattern的实现.三个关键参数：circuitBreaker.requestVolumeThreshold (default: 20 requests) and failue percentage is greater than circuitBreaker.errorThresholdPercentage (default: &gt;50%) in a rolling window defined by metrics.rollingStats.timeInMilliseconds (default: 10 seconds), the circuit opens and the call is not made.In cases of error and an open circuit a fallback can be provided by the developer. 首先添加：spring-cloud-starter-netflix-hystrix依赖12345678910111213141516171819202122@SpringBootApplication@EnableCircuitBreakerpublic class Application &#123; public static void main(String[] args) &#123; new SpringApplicationBuilder(Application.class).web(true).run(args); &#125;&#125;@Componentpublic class StoreIntegration &#123; @HystrixCommand(fallbackMethod = "defaultStores") public Object getStores(Map&lt;String, Object&gt; parameters) &#123; //do stuff that might fail &#125; public Object defaultStores(Map&lt;String, Object&gt; parameters) &#123; return /* something useful */; &#125;&#125; 如果你需要对HystrixCommand作出一些配置，如超时，连接池大小等。可以参考这里和Hystrix Wiki Propagating the Security Context or using Spring ScopesHystrix默认对请求进行隔离，有两种隔离模式：thread pool or SEMAPHORE。前者在单独的线程池中执行(默认)，后者在当前线程中执行。如果你向使用相关上下文，那么可以采用后面的模式：12345@HystrixCommand(fallbackMethod = "stubMyService", commandProperties = &#123; @HystrixProperty(name="execution.isolation.strategy", value="SEMAPHORE") &#125;) 如果你只想使用SecurityContext，那么你可以配置hystrix.shareSecurityContext=true。当然你也可以自定义一个HystrixConcurrencyStrategy 策略(见文档)。 Health Indicator &amp;&amp; Hystrix Metrics StreamHealth Indicator:在/health端点可以看到Hystrix Metrics Stream:添加spring-boot-starter-actuator依赖，然后会启用端点/hystrix.stream作为一个manager端点 Hystrix Timeouts And Ribbon Clients当使用HystrixCommand，内嵌Ribbon时，我们需要确保Hystrix timemout比Ribbon timeout要长，同时还需要考虑到Ribbon的重试机制。比如： if your Ribbon connection timeout is one second and the Ribbon client might retry the request three times, than your Hystrix timeout should be slightly more than three seconds. Hystrix DashBoarddashboard展示了所有的断路器状态。首先添加spring-cloud-starter-hystrix-netflix-dashboard依赖，然后在SpringBoot main class中添加@EnableHystrixDashboard，之后便可以使用/Hystrix端点访问了。但此时只是展示了单个实例的Hystrix数据。我们需要展示所有系统的数据，此时我们需要用到Turbine.Turbine：用来为Hystrix Dashboard聚集所有的/hystrix.stream到/turbin.stream的一个工具.启用Turbine server with Stream需要添加spring-cloud-starter-netflix-turbine-stream 依赖，并配置@EnableTurbineStream.默认端口8989然后为所有需要采集的客户端，包括Turbine Server本身，添加spring-cloud-starter-stream-rabbit依赖(如果你使用rabbitmq的话)一般地，可以把Turbin Server与Hystrix-dashboard合并成一个服务。 Client Side Load Balancer: Ribbon注意：Feign已经使用了Ribbon，如果你使用@FeignClient，那么Ribbon会自动起作用。内部使用RibbonClientConfiguration来与Spring集成，同时包含ILoadBalancer,RestClient,ServerListFilter依赖：spring-cloud-starter-netflix-ribbon 定制Ribbon Client外部属性配置项&lt; client&gt;.ribbon.*所有配置项定义都在CommonClientConfigKey类中，作为static fieldSpringCloud提供注解式配置，如下：1234@Configuration@RibbonClient(name = "foo", configuration = FooConfiguration.class)public class TestConfiguration &#123;&#125; 注意FooConfiguration需要被@Comfiguration配置，但不要将它放在@ComponentScan扫描的范围内，否则会被所有的@RibbonClients共享。 SpringCloud还为Ribbon提供很多bean，均可自定义，具体见文档。 全局默认配置，适用于所有的RibbonClient，例如下：1234567891011121314151617181920212223242526272829303132333435@RibbonClients(defaultConfiguration = DefaultRibbonConfig.class)public class RibbonClientDefaultConfigurationTestsConfig &#123; public static class BazServiceList extends ConfigurationBasedServerList &#123; public BazServiceList(IClientConfig config) &#123; super.initWithNiwsConfig(config); &#125; &#125;&#125;@Configurationclass DefaultRibbonConfig &#123; @Bean public IRule ribbonRule() &#123; return new BestAvailableRule(); &#125; @Bean public IPing ribbonPing() &#123; return new PingUrl(); &#125; @Bean public ServerList&lt;Server&gt; ribbonServerList(IClientConfig config) &#123; return new RibbonClientDefaultConfigurationTestsConfig.BazServiceList(config); &#125; @Bean public ServerListSubsetFilter serverListFilter() &#123; ServerListSubsetFilter filter = new ServerListSubsetFilter(); return filter; &#125;&#125; 通过属性来自定义从1.2.0开始，提供属性配置：前缀为&lt; clientName&gt;.ribbon.: NFLoadBalancerClassName: should implement ILoadBalancer NFLoadBalancerRuleClassName: should implement IRule NFLoadBalancerPingClassName: should implement IPing NIWSServerListClassName: should implement ServerList NIWSServerListFilterClassName should implement ServerListFilter 属性配置的好处：运行你在不同环境下改变这些行为。 与Eureka结合When Eureka is used in conjunction with Ribbon (i.e., both are on the classpath) the ribbonServerList is overridden with an extension of DiscoveryEnabledNIWSServerList which populates the list of servers from Eureka. It also replaces the IPing interface with NIWSDiscoveryPing which delegates to Eureka to determine if a server is up. The ServerList that is installed by default is a DomainExtractingServerList and the purpose of this is to make physical metadata available to the load balancer without using AWS AMI metadata (which is what Netflix relies on). By default the server list will be constructed with “zone” information as provided in the instance metadata (so on the remote clients set eureka.instance.metadataMap.zone), and if that is missing it can use the domain name from the server hostname as a proxy for zone (if the flag approximateZoneFromHostname is set). Once the zone information is available it can be used in a ServerListFilter. By default it will be used to locate a server in the same zone as the client because the default is a ZonePreferenceServerListFilter. The zone of the client is determined the same way as the remote instances by default, i.e. via eureka.instance.metadataMap.zone. How to Use Ribbon Without Eureka? 略。Using the Ribbon API Directly：12345678910public class MyClass &#123; @Autowired private LoadBalancerClient loadBalancer; public void doStuff() &#123; ServiceInstance instance = loadBalancer.choose("stores"); URI storesUri = URI.create(String.format("http://%s:%s", instance.getHost(), instance.getPort())); // ... do something with the URI &#125;&#125; 一般通过Feign来使用，很少直接用的。 Caching of Ribbon Configuration每个Ribbon named client都有对应的child Application Context，它是懒加载的，直到第一个请求到来时才会加载至named client，这会导致第一次请求时间较长，我们可以通过配置来让它启动时加载，如下:application.yml.1234ribbon: eager-load: enabled: true clients: client1, client2, client3 How to Configure Hystrix thread poolsIf you change zuul.ribbonIsolationStrategy to THREAD, the thread isolation strategy for Hystrix will be used for all routes. In this case, the HystrixThreadPoolKey is set to “RibbonCommand” as default.这意味着HystrixCommands for all routes会在同一个Hystrix thread pool中执行. 我们可以通过以下配置来让 HystrixCommands being executed in the Hystrix thread pool for each route.application.yml.123zuul: threadPool: useSeparateThreadPools: true How to Provide a Key to Ribbon’s IRule你可以提供自定义的IRule实现，来处理特殊的routing，比如canary test中。12public interface IRule&#123; public Server choose(Object key); 通过如下方式设置的key，会被传送到IRule的choose方法中,12RequestContext.getCurrentContext() .set(FilterConstants.LOAD_BALANCER_KEY, "canary-test"); 那么这段代码方哪里呢，它必须在RibbonRoutingFilter之前执行，放在Zuul的pre filter之中是最佳位置。 Declarative REST Client: FeignFeign is a declarative web service client. It makes writing web service clients easier. To use Feign create an interface and annotate it.风格上类似于Retrofit.Feign also supports pluggable encoders and decoders.Spring Cloud adds support for Spring MVC annotations and for using the same HttpMessageConverters used by default in Spring Web. Spring Cloud integrates Ribbon and Eureka to provide a load balanced http client when using Feign. 首先添加依赖spring-cloud-starter-openfeign1234567891011121314151617181920@Configuration@ComponentScan@EnableAutoConfiguration@EnableFeignClientspublic class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125;@FeignClient("stores")public interface StoreClient &#123; @RequestMapping(method = RequestMethod.GET, value = "/stores") List&lt;Store&gt; getStores(); @RequestMapping(method = RequestMethod.POST, value = "/stores/&#123;storeId&#125;", consumes = "application/json") Store update(@PathVariable("storeId") Long storeId, Store store);&#125; 注意：@FeignClient(“stores”)中指定的名字stores是远程的serviceId. Overriding Feign Defaults基于FeignClientsConfiguration中的配置项来进行自定义配置1234@FeignClient(name = "stores", configuration = FooConfiguration.class)public interface StoreClient &#123; //..&#125; 注意FooConfiguration需要被@Comfiguration配置，但不要将它放在@ComponentScan扫描的范围内，否则会被所有的共享。 默认提供的bean有： Decoder feignDecoder: ResponseEntityDecoder (which wraps a SpringDecoder) Encoder feignEncoder: SpringEncoder Logger feignLogger: Slf4jLogger Contract feignContract: SpringMvcContract Feign.Builder feignBuilder: HystrixFeign.Builder Client feignClient: if Ribbon is enabled it is a LoadBalancerFeignClient, otherwise the default feign client is used.使用的Http请求工具配置：feign.okhttp.enabled or feign.httpclient.enabled to true 没有提供，但支持注入的bean: Logger.Level Retryer ErrorDecoder Request.Options Collection SetterFactory如：123456789101112@Configurationpublic class FooConfiguration &#123; @Bean public Contract feignContract() &#123; return new feign.Contract.Default(); &#125; @Bean public BasicAuthRequestInterceptor basicAuthRequestInterceptor() &#123; return new BasicAuthRequestInterceptor("user", "password"); &#125;&#125; This replaces the SpringMvcContract with feign.Contract.Default and adds a RequestInterceptor to the collection of RequestInterceptor. 现在还支持通过配置文件来配置@FeignClient：application.yml12345678910111213feign: client: config: feignName: connectTimeout: 5000 readTimeout: 5000 loggerLevel: full errorDecoder: com.example.SimpleErrorDecoder retryer: com.example.SimpleRetryer requestInterceptors: - com.example.FooRequestInterceptor - com.example.BarRequestInterceptor decode404: false 注意上述feignName需要你替换为自己的名字，如果需要全局默认，可以使用default作为名字。注意：相比于前面的@Configuration bean，属性文件拥有更高的优先级。注意：如果你需要在RequestInterceptor中使用ThreadLocal，你有两种选择：1、禁用Hystrix,这个比较极端，2、set the thread isolation strategy for Hystrix to `SEMAPHOREapplication.yml123456789101112# To disable Hystrix in Feignfeign: hystrix: enabled: false# To set thread isolation to SEMAPHOREhystrix: command: default: execution: isolation: strategy: SEMAPHORE Creating Feign Clients Manually12345678910111213141516171819202122@Import(FeignClientsConfiguration.class)class FooController &#123; private FooClient fooClient; private FooClient adminClient; @Autowired public FooController( Decoder decoder, Encoder encoder, Client client) &#123; this.fooClient = Feign.builder().client(client) .encoder(encoder) .decoder(decoder) .requestInterceptor(new BasicAuthRequestInterceptor("user", "user")) .target(FooClient.class, "http://PROD-SVC"); this.adminClient = Feign.builder().client(client) .encoder(encoder) .decoder(decoder) .requestInterceptor(new BasicAuthRequestInterceptor("admin", "admin")) .target(FooClient.class, "http://PROD-SVC"); &#125;&#125; Feign Hystrix Support1、确保Hystrix在classpath中，2、feign.hystrix.enabled=true这样Feign会将所有方法包含在断路器中，然后返回com.netflix.hystrix.HystrixCommand,它支持reactive pattern,(采用RxJava)。 Feign Hystrix Fallbacks12345678910111213@FeignClient(name = "hello", fallback = HystrixClientFallback.class)protected interface HystrixClient &#123; @RequestMapping(method = RequestMethod.GET, value = "/hello") Hello iFailSometimes();&#125;@Bean //官方文档没有加，但是有这句话:You also need to declare your implementation as a Spring bean.具体使用时，再测试下static class HystrixClientFallback implements HystrixClient &#123; @Override public Hello iFailSometimes() &#123; return new Hello("fallback"); &#125;&#125; fallbackFactory方式：123456789101112131415161718@FeignClient(name = "hello", fallbackFactory = HystrixClientFallbackFactory.class)protected interface HystrixClient &#123; @RequestMapping(method = RequestMethod.GET, value = "/hello") Hello iFailSometimes();&#125;@Componentstatic class HystrixClientFallbackFactory implements FallbackFactory&lt;HystrixClient&gt; &#123; @Override public HystrixClient create(Throwable cause) &#123; return new HystrixClient() &#123; @Override public Hello iFailSometimes() &#123; return new Hello("fallback; reason was: " + cause.getMessage()); &#125; &#125;; &#125;&#125; Feign and @Primary注意上述定义fallback时，导致了一个新问题，那就是很多同类型的beans，使得@Autowired无法工作，那么SpringCloudNetflix怎么解决的呢，那就是因为它默认给@FeignClient生成的bean加上了@Primary，如果不想使用这个特性，可以关闭，如下:1234@FeignClient(name = "hello", primary = false)public interface HelloClient &#123; // methods here&#125; Feign Inheritance Support支持继承来实现模版化通用UserService.java.12345public interface UserService &#123; @RequestMapping(method = RequestMethod.GET, value ="/users/&#123;id&#125;") User getUser(@PathVariable("id") long id);&#125; UserClient.java.123456package project.user;@FeignClient("users")public interface UserClient extends UserService &#123;&#125; Feign request/response compression启用压缩123456feign.compression.request.enabled=truefeign.compression.response.enabled=true#高级配法feign.compression.request.enabled=truefeign.compression.request.mime-types=text/xml,application/xml,application/jsonfeign.compression.request.min-request-size=2048 Feign loggingA logger is created for each Feign client created. By default the name of the logger is the full class name of the interface used to create the Feign client. Feign logging only responds to the DEBUG level. application.yml.1logging.level.project.user.UserClient: DEBUG The Logger.Level object that you may configure per client, tells Feign how much to log. Choices are: NONE, No logging (DEFAULT). BASIC, Log only the request method and URL and the response status code and execution time. HEADERS, Log the basic information along with request and response headers. FULL, Log the headers, body, and metadata for both requests and responses.1234567@Configurationpublic class FooConfiguration &#123; @Bean Logger.Level feignLoggerLevel() &#123; return Logger.Level.FULL; &#125;&#125; Router and Filter: Zuul Zuul is a JVM based router and server side load balancer by Netflix.sNetflix uses Zuul for the following: Authentication Insights Stress Testing Canary Testing Dynamic Routing Service Migration Load Shedding Security Static Response handling Active/Active traffic management 注意：zuul.max.host.connections属性已经被废弃，代之以两个新属性：zuul.host.maxTotalConnections and zuul.host.maxPerRouteConnections which default to 200 and 20 respectively. 注意：Default Hystrix isolation pattern (ExecutionIsolationStrategy) for all routes is SEMAPHORE. zuul.ribbonIsolationStrategy can be changed to THREAD if this isolation pattern is preferred. 首先添加依赖：spring-cloud-starter-netflix-zuul和spring-cloud-starter-netflix-eureka-client依赖 Embedded Zuul Reverse Proxy提供反向代理来解决CORS跨域问题及权限集中处理。通过@EnableZuulProxy来启用此处发现代理以serveiceId作为前缀标识，如/users，代理到serviceId为users的服务。同时proxy使用Ribbon来进行负载均衡，使用Hystrix来进行断路控制。 路由包含与排除：application.yml.1234zuul: ignoredServices: &apos;*&apos; routes: users: /myusers/** In this example, all services are ignored except “users”. 改变路由规则application.yml.123zuul: routes: users: /myusers/** This means that http calls to “/myusers” get forwarded to the “users” service 更为细粒度的控制：1234567application.yml. zuul: routes: users: path: /myusers/** serviceId: users_service 配置样例：1234567891011121314151617181920212223zuul: routes: echo: path: /myusers/** serviceId: myusers-service stripPrefix: truehystrix: command: myusers-service: execution: isolation: thread: timeoutInMilliseconds: ...myusers-service: ribbon: NIWSServerListClassName: com.netflix.loadbalancer.ConfigurationBasedServerList ListOfServers: http://example1.com,http://example2.com ConnectTimeout: 1000 ReadTimeout: 3000 MaxTotalHttpConnections: 500 MaxConnectionsPerHost: 100 你也可以通过正则表达式定义复杂的路由规则：123456@Beanpublic PatternServiceRouteMapper serviceRouteMapper() &#123; return new PatternServiceRouteMapper( "(?&lt;name&gt;^.+)-(?&lt;version&gt;v.+$)", "$&#123;version&#125;/$&#123;name&#125;");&#125; This means that a serviceId “myusers-v1” will be mapped to route “/v1/myusers/**”. Any regular expression is accepted but all named groups must be present in both servicePattern and routePattern. If servicePattern does not match a serviceId, the default behavior is used. In the example above, a serviceId “myusers” will be mapped to route “/myusers/**” (no version detected) To add a prefix to all mappings, set zuul.prefix to a value, such as /api. The proxy prefix is stripped from the request before the request is forwarded by default (switch this behaviour off with zuul.stripPrefix=false). You can also switch off the stripping of the service-specific prefix from individual routes, e.g.1234567application.yml. zuul: routes: users: path: /myusers/** stripPrefix: false In this example, requests to “/myusers/101” will be forwarded to “/myusers/101” on the “users” service.(默认行为是到”users” service的/101) The zuul.routes entries actually bind to an object of type ZuulProperties.所以你可以使用其中的属性来配置其他项。 The X-Forwarded-Host header is added to the forwarded requests by default. To turn it off set zuul.addProxyHeaders = false. The prefix path is stripped by default, and the request to the backend picks up a header &quot;X-Forwarded-Prefix&quot; (&quot;/myusers&quot; in the examples above). An application with @EnableZuulProxy could act as a standalone server if you set a default route (&quot;/&quot;), for example zuul.route.home: / would route all traffic (i.e. “/**”) to the “home” service. 指定需要忽略的路由：1234zuul: ignoredPatterns: /**/admin/** routes: users: /myusers/** This means that all calls such as “/myusers/101” will be forwarded to “/101” on the “users” service. But calls including “/admin/“ will not resolve. Zuul Http Client默认使用Apache HttpClient，如果你想使用OkHttp3,那么配置ribbon.okhttp.enabled=true Cookies and Sensitive Headers123456zuul: routes: users: path: /myusers/** sensitiveHeaders: Cookie,Set-Cookie,Authorization url: https://downstream this is the default value for sensitiveHeaders, so you don’t need to set it unless you want it to be different. 配置所有header都保留application.yml.123456zuul: routes: users: path: /myusers/** sensitiveHeaders: url: https://downstream 全局配置：zuul.sensitiveHeaders. Ignored Headerszuul.ignoredHeaders：来配置需要去除的头， (both request and response)默认情况下为空，但是如果Spring Security在classpath中，那么值为”security”,也就是说此时会忽略security头。如果你像禁用Spring Security的这一行为，请配置:zuul.ignoreSecurityHeaders=false Management Endpoints使用@EnableZuulProxy会启用两个endpoints:/routes/filters如下:1234567891011121314151617181920GET /routes. &#123; /stores/**: &quot;http://localhost:8081&quot;&#125;GET /routes?format=details. &#123; &quot;/stores/**&quot;: &#123; &quot;id&quot;: &quot;stores&quot;, &quot;fullPath&quot;: &quot;/stores/**&quot;, &quot;location&quot;: &quot;http://localhost:8081&quot;, &quot;path&quot;: &quot;/**&quot;, &quot;prefix&quot;: &quot;/stores&quot;, &quot;retryable&quot;: false, &quot;customSensitiveHeaders&quot;: false, &quot;prefixStripped&quot;: true &#125;&#125; A GET to the filters endpoint at /filters will return a map of Zuul filters by type. For each filter type in the map, you will find a list of all the filters of that type, along with their details. Strangulation Patterns and Local Forwards用于集成已有应用1234567891011121314zuul: routes: first: path: /first/** url: http://first.example.com second: path: /second/** url: forward:/second third: path: /third/** url: forward:/3rd legacy: path: /** url: http://legacy.example.com In this example we are strangling the “legacy” app which is mapped to all requests that do not match one of the other patterns. Paths in /first/ have been extracted into a new service with an external URL. And paths in /second/ are forwarded so they can be handled locally, e.g. with a normal Spring @RequestMapping. Paths in /third/** are also forwarded, but with a different prefix (i.e. /third/foo is forwarded to /3rd/foo). Uploading Files through Zuul默认情况下，小文件没有问题，如果是大文件呢？我们需要使用/zuul/前缀，例如：zuul.routes.customers=/customers/** then you can POST large files to “/zuul/customers/“.同时需要对大文件的超时配置，以免被Ribbon和hystrix超时。1234hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds: 60000ribbon: ConnectTimeout: 3000 ReadTimeout: 60000 Note that for streaming to work with large files, you need to use chunked encoding in the request12$ curl -v -H &quot;Transfer-Encoding: chunked&quot; \ -F &quot;file=@mylarge.iso&quot; localhost:9999/zuul/simple/file Query String Encoding略 Plain Embedded ZuulYou can also run a Zuul server without the proxying, or switch on parts of the proxying platform selectively, if you use @EnableZuulServer (instead of @EnableZuulProxy). Any beans that you add to the application of type ZuulFilter will be installed automatically, as they are with @EnableZuulProxy, but without any of the proxy filters being added automatically. Disable Zuul Filterszuul.&lt; SimpleClassName&gt;.&lt; filterType&gt;.disable=trueFor example to disable org.springframework.cloud.netflix.zuul.filters.post.SendResponseFilter set zuul.SendResponseFilter.post.disable=true Providing Hystrix Fallbacks For Routes123zuul: routes: customers: /customers/** 12345678910111213141516171819202122232425262728293031323334353637383940414243class MyFallbackProvider implements ZuulFallbackProvider &#123; @Override public String getRoute() &#123; return "customers"; &#125; @Override public ClientHttpResponse fallbackResponse() &#123; return new ClientHttpResponse() &#123; @Override public HttpStatus getStatusCode() throws IOException &#123; return HttpStatus.OK; &#125; @Override public int getRawStatusCode() throws IOException &#123; return 200; &#125; @Override public String getStatusText() throws IOException &#123; return "OK"; &#125; @Override public void close() &#123; &#125; @Override public InputStream getBody() throws IOException &#123; return new ByteArrayInputStream("fallback".getBytes()); &#125; @Override public HttpHeaders getHeaders() &#123; HttpHeaders headers = new HttpHeaders(); headers.setContentType(MediaType.APPLICATION_JSON); return headers; &#125; &#125;; &#125;&#125; 你也可以配置全局默认fallback:1234567891011class MyFallbackProvider implements ZuulFallbackProvider &#123; @Override public String getRoute() &#123; return "*"; &#125; @Override public ClientHttpResponse fallbackResponse() &#123; .... &#125;&#125; 如果你想知道异常信息，请使用FallbackProvider代替ZuulFallbackProvder .12345678910111213141516171819202122232425class MyFallbackProvider implements FallbackProvider &#123; @Override public String getRoute() &#123; return "*"; &#125; @Override public ClientHttpResponse fallbackResponse(final Throwable cause) &#123; if (cause instanceof HystrixTimeoutException) &#123; return response(HttpStatus.GATEWAY_TIMEOUT); &#125; else &#123; return fallbackResponse(); &#125; &#125; @Override public ClientHttpResponse fallbackResponse() &#123; return response(HttpStatus.INTERNAL_SERVER_ERROR); &#125; private ClientHttpResponse response(final HttpStatus status) &#123; ... &#125;&#125; Zuul TimeoutsIf you want to configure the socket timeouts and read timeouts for requests proxied through Zuul .If Zuul is using service discovery than you need to configure these timeouts via Ribbon properties, ribbon.ReadTimeout and ribbon.SocketTimeout. If you have configured Zuul routes by specifying URLs than you will need to use zuul.host.connect-timeout-millis and zuul.host.socket-timeout-millis. Zuul Developer GuideZuul is implemented as a Servlet. For the general cases, Zuul is embedded into the Spring Dispatch mechanism. This allows Spring MVC to be in control of the routing. In this case, Zuul is configured to buffer requests.If there is a need to go through Zuul without buffering requests (e.g. for large file uploads), the Servlet is also installed outside of the Spring Dispatcher. By default, this is located at /zuul. This path can be changed with the zuul.servlet-path property. RequestContext用于在filters中共享信息，采用ThreadLocal变量。同时包含HttpServletRequest and HttpServletResponse.而FilterConstants包含所有filters需要用到的常量。 @EnableZuulProxy vs. @EnableZuulServer:@EnableZuulProxy is a superset of @EnableZuulServer. In other words, @EnableZuulProxy contains all filters installed by @EnableZuulServer. The additional filters in the “proxy” enable routing functionality.各自的filters见官文 Custom Zuul Filter examples例如：write a pre filter:12345678910111213141516171819202122232425262728public class QueryParamPreFilter extends ZuulFilter &#123; @Override public int filterOrder() &#123; return PRE_DECORATION_FILTER_ORDER - 1; // run before PreDecoration &#125; @Override public String filterType() &#123; return PRE_TYPE; &#125; @Override public boolean shouldFilter() &#123; RequestContext ctx = RequestContext.getCurrentContext(); return !ctx.containsKey(FORWARD_TO_KEY) // a filter has already forwarded &amp;&amp; !ctx.containsKey(SERVICE_ID_KEY); // a filter has already determined serviceId &#125; @Override public Object run() &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); if (request.getParameter("foo") != null) &#123; // put the serviceId in `RequestContext` ctx.put(SERVICE_ID_KEY, request.getParameter("foo")); &#125; return null; &#125;&#125; 其他例子请参考官文。此处不作过多介绍。 How Zuul Errors WorkThe SendErrorFilter is only run if RequestContext.getThrowable() is not null. It then sets specific javax.servlet.error.* attributes in the request and forwards the request to the Spring Boot error page. Zuul Eager Application Context LoadingZuul internally uses Ribbon for calling the remote url’s and Ribbon clients are by default lazily loaded up by Spring Cloud on first call. This behavior can be changed for Zuul using the following configuration and will result in the child Ribbon related Application contexts being eagerly loaded up at application startup time. application.yml.1234zuul: ribbon: eager-load: enabled: true Polyglot support with Sidecar集成非jvm语言的系统。Do you have non-jvm languages you want to take advantage of Eureka, Ribbon and Config Server?首先添加依赖：spring-cloud-netflix-sidecar添加@EnableSidecar. This annotation includes @EnableCircuitBreaker, @EnableDiscoveryClient, and @EnableZuulProxy The sidecar.health-uri is a uri accessible on the non-jvm app that mimicks a Spring Boot health indicator. It should return a json document like the following: health-uri-document.123&#123; &quot;status&quot;:&quot;UP&quot;&#125; Here is an example application.yml for a Sidecar application: application.yml.123456789server: port: 5678spring: application: name: sidecarsidecar: port: 8000 health-uri: http://localhost:8000/health.json RxJava with Spring MVCRxJava is a Java VM implementation of Reactive Extensions: a library for composing asynchronous and event-based programs by using observable sequences. Spring Cloud Netflix provides support for returning rx.Single objects from Spring MVC Controllers. It also supports using rx.Observable objects for Server-sent events (SSE). This can be very convenient if your internal APIs are already built using RxJava (see Section 17.4, “Feign Hystrix Support” for examples). 具体见官文。 Metrics: Spectator, Servo, and AtlasServo已经废弃了，推荐使用Spectator.Spectator+Atlas提供近实时的系统监控功能。其中Spectator用于收集元数据metrics, atlas用于存储metrics，并管理多个维度的时序数据。除了官方文档外，建议参考：Atlas+Spectator+Grafana搭建实时监控平台和spring cloud atlas使用]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>springcloud</tag>
        <tag>msa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SpringCloud官文笔记]SpringCloudConfig]]></title>
    <url>%2F2017%2F12%2F18%2FSpringCloud%E5%AE%98%E6%96%87%E7%AC%94%E8%AE%B0-SpringCloudConfig%2F</url>
    <content type="text"><![CDATA[SpringCloud提供分布式系统中的外部化、多环境、版本化、跨语言的配置管理。概念上与Spring的Environment和PropertySource等同映射。它基于git作为配置存储实现。 QuickStart1234567$ cd spring-cloud-config-server$ ../mvnw spring-boot:run$ curl localhost:8888/foo/development&#123;&quot;name&quot;:&quot;foo&quot;,&quot;label&quot;:&quot;master&quot;,&quot;propertySources&quot;:[ &#123;&quot;name&quot;:&quot;https://github.com/scratches/config-repo/foo-development.properties&quot;,&quot;source&quot;:&#123;&quot;bar&quot;:&quot;spam&quot;&#125;&#125;, &#123;&quot;name&quot;:&quot;https://github.com/scratches/config-repo/foo.properties&quot;,&quot;source&quot;:&#123;&quot;foo&quot;:&quot;bar&quot;&#125;&#125;]&#125; 请求url的格式： /{application}/{profile}[/{label}] /{application}-{profile}.yml /{label}/{application}-{profile}.yml /{application}-{profile}.properties /{label}/{application}-{profile}.propertiesapplication代表你配置的spring.config.name,profile不解释，label是git的分支，默认是master. 配置git地址：123456spring: cloud: config: server: git: uri: https://github.com/spring-cloud-samples/config-repo 客户端使用需要添加spring-cloud-starter-config依赖：1234567891011121314151617181920212223242526272829303132333435363738394041 &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt; &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Brixton.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; &lt;!-- repositories also needed for snapshots and milestones --&gt; 然后在bootstrap.properties中配置config server的地址：1spring.cloud.config.uri: http://myconfigserver.com 然后通过/env端点检测：12345678$ curl localhost:8080/env&#123; &quot;profiles&quot;:[], &quot;configService:https://github.com/spring-cloud-samples/config-repo/bar.properties&quot;:&#123;&quot;foo&quot;:&quot;bar&quot;&#125;, &quot;servletContextInitParams&quot;:&#123;&#125;, &quot;systemProperties&quot;:&#123;...&#125;, ...&#125; Spring Cloud Config Server支持的backend：git,svn,file-base,Vault,jdbc。推荐git。@EnableConfigServer1234567@SpringBootApplication@EnableConfigServerpublic class ConfigServer &#123; public static void main(String[] args) &#123; SpringApplication.run(ConfigServer.class, args); &#125;&#125; application.properties.12server.port: 8888spring.cloud.config.server.git.uri: file://$&#123;user.home&#125;/config-repo #windows下:file:/// 本地测试：1234567$ cd $HOME$ mkdir config-repo$ cd config-repo$ git init .$ echo info.foo: bar &gt; application.properties$ git add -A .$ git commit -m &quot;Add application.properties&quot; Environment Repository不作过多介绍，默认采用git作为backend.提一下里面介绍的类：EnvironmentRepository Environment复杂配置支持：1234567891011121314spring: cloud: config: server: git: uri: https://github.com/spring-cloud-samples/config-repo repos: simple: https://github.com/simple/config-repo special: pattern: special*/dev*,*special*/dev* uri: https://github.com/special/config-repo local: pattern: local* uri: file:/home/configsvc/config-repo git认证方式及配置：1、如果git仓库需要用户名和密码认证：12345678spring: cloud: config: server: git: uri: https://github.com/spring-cloud-samples/config-repo username: trolley password: strongpassword 2、如果你向采用私钥公钥免密的方式：1、确保你的私钥在~/.ssh中，2、确保url使用git@开头，3、确保你git server的地址在~/.ssh/know_hosts中。3、通过配置文件直接配置ssh私钥：略 关于从git仓库pull：config server会在第一次被请求时从git仓库拉取配置文件，并缓存在本地。但是当git仓库配置文件发生改变后，config server无法知晓，所以此时数据为dirty。我们可以通过force-pull属性来开启改变后自动刷新本地缓存的功能：如果有多个，需要对每个都配置12345678910111213141516171819spring: cloud: config: server: git: uri: https://git/common/config-repo.git force-pull: true repos: team-a: pattern: team-a-* uri: http://git/team-a/config-repo.git force-pull: true team-b: pattern: team-b-* uri: http://git/team-b/config-repo.git force-pull: true team-c: pattern: team-c-* uri: http://git/team-a/config-repo.git Push Notifications and Spring Cloud Bus：利用git(github,gitlab等)的webhook功能。添加spring-cloud-config-monitor及SpringCloud Bus则会在config server中启用Sping Cloud Bus，此时会激活/monitor端点。当webhook配置为config server地址/monitor后，一旦发生改变，webhook通知config server，configserver之后会根据定义的PropertyPathNotificationExtractor通知策略发送RefreshRemoteApplicationEvent事件给相关客户端(客户端需要添加依赖SpringCloud Bus)。当然，你也可以手动POST /monitor端点，并附上form-encoded body parameters path={name}，那么也会触发更新。 需要注意的是：拉取git仓库配置后默认的缓存目录是放在操作系统的temp目录下，以config-repo-开头，比如linux下/tmp/config-repo-&lt; randomid&gt;，如果你需要改变这一行为，比如有些操作系统会不定时删除temp目录下的内容。那么可以通过配置：spring.cloud.config.server.git.basedir or spring.cloud.config.server.svn.basedir来改变. 健康检查：检查EnvironmentRepository是否处于工作状态，默认情况下只检查名为app，默认profile和默认label的，不过你可以配置如下：1234567891011spring: cloud: config: server: health: repositories: myservice: label: mylabel myservice-dev: name: myservice profiles: development Encryption and Decryption当属性名是:{cipher}开头时，如{cipher}*，就代表它是加密过的。config server在获取该属性发送到客户端之前是会进行解密，前提条件：1、添加Spring Security RSA依赖，2、确保JCE在JDK中；3、确保有效的公钥在应用中。application.yml.1234spring: datasource: username: dbuser password: '&#123;cipher&#125;FKSAJDFGYOS8F7GLHAKERGFHLSAJ' 同时config server提供方便的/encrypt and /decrypt端点，1234$ curl localhost:8888/encrypt -d mysecret682bc583f4641835fa2db009355293665d2647dade3375c0ee201de2a49f7bda$ curl localhost:8888/decrypt -d 682bc583f4641835fa2db009355293665d2647dade3375c0ee201de2a49f7bdamysecret 加密Key的管理config server可以使用对称symmetric ，非对称asymmetric (RSA)加密。如果仅使用对称加密,只需要把key配置在bootstrap.properties的encrypt.key中。如果使用非对称加密，有两种配置方式：1、直接用私钥内容配置在encrypt.key中，2、如果使用keystore，那么需要配置的项有:encrypt.keyStore.location/password/alias Spring Cloud Config Client1、Config Server的发现模式：”Config First” mode or Discovery First Bootstrap:在Config First mode中，你需要在bootstrap.yml配置死spring.cloud.config.uri (defaults to “http://localhost:8888&quot;).如果config server的ip发生改变，那就失去了灵活性。如果指派别人去一个一个改客户端配置，估计会fuck you。2、Discovery First Bootstrap:可以充分利用服务注册与发现的优势。而无需客户端绑死config server的ip or hostname。可以在bootstrap.yml配置spring.cloud.config.discovery.enabled=true (default “false”)启用。同时需要引入eureka client的客户端依赖，并配置eureka的地址：eureka.client.serviceUrl.defaultZone默认情况下configserverId为：configserver,如果你的configserver是你自己嵌入的微服务，那么该serviceId就是spring.application.name,在客户端你需要通过spring.cloud.config.discovery.serviceId来指定关于fastfail，及失败重试，密码认证、自定义请求RestTemplate请看文档，这里不作具体介绍。]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>springcloud</tag>
        <tag>msa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[SpringCloud官文笔记]SpringCloudContext与Commons]]></title>
    <url>%2F2017%2F12%2F18%2FSpringCloud%E5%AE%98%E6%96%87%E7%AC%94%E8%AE%B0-SpringCloudContext%E4%B8%8ECommons%2F</url>
    <content type="text"><![CDATA[基于Edgware版本，此篇为SpringCloud官方文档学习笔记的开篇。笔记就是散乱，还望各位看官见谅。http://cloud.spring.io/spring-cloud-static/Edgware.RELEASE/single/spring-cloud.html Spring Cloud Context为Spring Cloud应用程序（引导上下文，加密，刷新范围和环境端点）的ApplicationContext提供实用程序和特殊服务。Spring Cloud Commons是一组在不同的Spring Cloud实现中使用的抽象和常用类 SpringCloud ContextBootstrap ApplicationContext：这个上下文是主应用程序的父上下文。Bootstrap属性的优先级高，因此默认情况下不能被本地配置覆盖。这两个上下文共享一个Environment。引导上下文使用与主应用程序上下文不同的外部配置约定，因此使用bootstrap.yml application.yml（或.properties）代替引导和主上下文的外部配置。SpringApplicationBuilder，SpringApplication bootstrap.yml.123456spring: application: name: foo cloud: config: uri: $&#123;SPRING_CONFIG_URI:http://localhost:8888&#125; boostrap.yml支持profiles:通过指定spring.profiles.active的值来指定不同环境下的配置文件:bootstrap-development.properties一般实际场景中boostrap context中的property source都来自于远程，如Config Server，默认情况下是无法被本地环境覆盖的(除非通过命令行)。不过你可以通过在远程的属性源中配置:spring.cloud.config.allowOverride=true来授权(在本地配置无效)，然后在远程的配置中可配置spring.cloud.config.overrideNone=true(可覆盖所有)或者spring.cloud.config.overrideSystemProperties=false(仅可覆盖环境变量) bootstrap configuration配置方式是通过在/META-INF/spring.factories下的org.springframework.cloud.bootstrap.BootstrapConfiguration添加条目。bootstrap context首先会被来自spring.factories的类创建 ，然后所有的 @Beans of type ApplicationContextInitializer are added to the main SpringApplication before it is started.默认的来自外部配置的property source是Config Server,如果你向添加其他外部源如database，可以通过添加一个继承PropertySourceLocator的bean，然后配置到/META-INF/spring.factories来实现.如下：12345678910@Configurationpublic class CustomPropertySourceLocator implements PropertySourceLocator &#123; @Override public PropertySource&lt;?&gt; locate(Environment environment) &#123; return new MapPropertySource("customProperty", Collections.&lt;String, Object&gt;singletonMap("property.from.sample.custom.source", "worked as intended")); &#125;&#125; Environment Changes：应用会通过监听EnvironmentChangeEvent来响应环境变量的改变，用户也可以通过添加ApplicationListerner的bean来实现。一旦环境变量改变，应用会 Re-bind any @ConfigurationProperties beans in the context Set the logger levels for any properties in logging.level.* Refresh Scope：@Bean被标记为@RefreshScope时，当属性改变时，可以进行改变响应。注意：不要将@RefreshScope放到@Configuration上 属性加解密：当属性名是:{cipher}开头时，如{cipher}*，就代表它是加密过的。config server在获取该属性发送到客户端之前是会进行解密，但有几个前提条件：1、添加Spring Security RSA依赖，2、确保JCE在JDK中；3、确保有效的公钥在应用中。 Spring Boot Actuator Endpoints： POST to /env to update the Environment and rebind @ConfigurationProperties and log levels /refresh for re-loading the boot strap context and refreshing the @RefreshScope beans /restart for closing the ApplicationContext and restarting it (disabled by default) /pause and /resume for calling the Lifecycle methods (stop() and start() on the ApplicationContext) Spring Cloud Commons@EnableDiscoveryClient:它会在/META-INF/spring.factories的org.springframework.cloud.client.discovery.EnableDiscoveryClient键中查找DiscoveryClient 的实现，如Spring Cloud Netflix Eureka, Spring Cloud Consul Discovery and Spring Cloud Zookeeper Discovery.但该注解不是必须的，因为如果在classpath中存在DiscoveryClient 的实现，会自动启用。 HealthIndicator：一般被DiscoveryClient实现DiscoveryHealthIndicator,用来支持健康检查。 ServiceRegistry:默认会自动注册到注册中心。 A /service-registry actuator endpoint is provided by Commons. This endpoint relys on a Registration bean in the Spring Application Context. Calling /service-registry/instance-status via a GET will return the status of the Registration. A POST to the same endpoint with a String body will change the status of the current Registration to the new value. Spring RestTemplate as a Load Balancer Client：RestTemplate can be automatically configured to use ribbon. To create a load balanced RestTemplate create a RestTemplate @Bean and use the @LoadBalanced qualifier.如下：123456789101112131415161718@Configurationpublic class MyConfiguration &#123; @LoadBalanced @Bean RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125;public class MyClass &#123; @Autowired private RestTemplate restTemplate; public String doOtherStuff() &#123; String results = restTemplate.getForObject("http://stores/stores", String.class); return results; &#125;&#125; 注意，此处url需要使用server name,而不是hostname或者ip，否则就无法实现负载均衡了。具体实现见：https://github.com/spring-cloud/spring-cloud-netflix/blob/master/spring-cloud-netflix-core/src/main/java/org/springframework/cloud/netflix/ribbon/RibbonAutoConfiguration.java Retrying Failed Requests：负载均衡RestTemplate可以配置请求失败重试，默认是禁用的，如果在classpath中发现Spring Retry依赖，它会自动启用，如果你想禁用该行为：spring.cloud.loadbalancer.retry.enabled=false。 或者，你只是想自定义重试次数：client.ribbon.MaxAutoRetries, client.ribbon.MaxAutoRetriesNextServer, and client.ribbon.OkToRetryOnAllOperations。注意：这里的client需要替换为你的 Ribbon client’s name.详情见：https://github.com/Netflix/ribbon/wiki/Getting-Started#the-properties-file-sample-clientproperties%E3%80%82 如果你想自定义降级策略,可以实现BackOffPolicy接口，然后创建LoadBalancedBackOffPolicyFactory来返回它，如下:123456789101112@Configurationpublic class MyConfiguration &#123; @Bean LoadBalancedBackOffPolicyFactory backOffPolciyFactory() &#123; return new LoadBalancedBackOffPolicyFactory() &#123; @Override public BackOffPolicy createBackOffPolicy(String service) &#123; return new ExponentialBackOffPolicy(); &#125; &#125;; &#125;&#125; 多个RestTemplate Objects:123456789101112131415@Configurationpublic class MyConfiguration &#123; @LoadBalanced @Bean RestTemplate loadBalanced() &#123; return new RestTemplate(); &#125; @Primary //为了消除歧义，此bean为非负载均衡的普通的restTemplate bean @Bean RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; Ignore Network Interfaces:有个时候，在Service Discovery registration时，你需要忽略特定的网卡application.yml.123456spring: cloud: inetutils: ignoredInterfaces: - docker0 - veth.* 或者直接指定123456spring: cloud: inetutils: preferredNetworks: - 192.168 - 10.0 或者直接这样1234spring: cloud: inetutils: useOnlySiteLocalInterfaces: true HTTP Client Factories：支持ApachenHttpClientFactory/ApacheHttpClientConnectionManagerFactory和OkHttpClientFactory/OkHttpClientConnectionPoolFactory.启用哪个，取决于jar是否在classpath中，当然也可以通过配置来禁用。]]></content>
      <categories>
        <category>读书笔记</category>
        <category>SpringCloud</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>springcloud</tag>
        <tag>msa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[杂记]有趣的句子]]></title>
    <url>%2F2017%2F12%2F18%2F%E6%9D%82%E8%AE%B0-%E6%9C%89%E8%B6%A3%E7%9A%84%E5%8F%A5%E5%AD%90%2F</url>
    <content type="text"><![CDATA[微服务的定义：In short, the microservice architectural style is an approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API. These services are built around business capabilities and independently deployable by fully automated deployment machinery. There is a bare minimum of centralized management of these services, which may be written in different programming languages and use different data storage technologies. 康威定律：Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure. Of course, just because you can do something, doesn’t mean you should - but partitioning your system in this way means you have the option.你可以做什么事，并不意味着你必须做这些事，但如果将系统通过微服务的方式拆分，意味着你有这个选择。-灵活性。 Being woken up at 3am every night by your pager is certainly a powerful incentive to focus on quality when writing your code.被凌晨3点的传输机吵醒无疑对你写代码时关注质量是一个很大的激励(毕竟谁都不想在凌晨3点被吵醒) Change control doesn’t necessarily mean change reduction - with the right attitudes and tools you can make frequent, fast, and well-controlled changes to software.关于控制改变的 We can’t say for sure where we’ll end up, but one of the challenges of software development is that you can only make decisions based on the imperfect information that you currently have to hand.关于软件设计的 You can’t control what you can’t measure Should you measure something be sure what you really measure otherwise the results can keep you far from reality. Always validate your assumptions and RTFM! 一大波有趣的短语：RTFM/RTM：RTFM是一组缩写，Unix程序员的一种习惯，意思是：去读他妈的手册，Read the fucking manual！这句话通常用在回复那些只要查阅文件就可以解决，拿出来提问只是浪费别人时间的问题。延伸：RTFSC/RTFS(Reading The Fucking Source Code)UTFH (“Use The Fucking Help”)STFW (“Search The Fucking Web”)STFG (“Search The Fucking Google” or “Search The Fantastic Google”)GIYF (“Google Is Your Friend”)JFGI (“Just Fucking Google It”)JGIYN (“Just Google It You Noob”)UTSL (“Use The Source Luke”—alternately, RTFS)RTFA (“Read The Fucking Article”—common on news forums such as Fark.com[3] and Slashdot)RTFE (“Read The Fucking Email”)RTFC (“Read The Fucking Code,” or “Reboot The Fucking Computer”)RTFSC (“Read The Fucking Source Code”)RTFQ (“Read The Fucking Question”)RTFFAQ (“Read The Fucking Frequently Asked Questions”)LMGTFY (“Let Me Google That For You”)WIDGI (“When In Doubt Google It”—also occasionally “WIDGIT”)FIOTI (“Find It On The Internet”)]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>杂记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[限流令牌桶算法]]></title>
    <url>%2F2017%2F12%2F17%2F%E9%99%90%E6%B5%81%E4%BB%A4%E7%89%8C%E6%A1%B6%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[缓存(Caching)，限流(Throttling)和降级(BackOff)是系统的三把利剑。 原理做限流(Rate Limiting/Throttling)的时候，除了简单的控制并发，如果要准确的控制TPS，简单的做法是维护一个单位时间内的Counter，如判断单位时间已经过去，则将Counter重置零。此做法被认为没有很好的处理单位时间的边界，比如在前一秒的最后一毫秒里和下一秒的第一毫秒都触发了最大的请求数，将目光移动一下，就看到在两毫秒内发生了两倍的TPS。因此更平滑的算法如Leaky Bucket–漏桶算法，又或者将原来单位时间内单一的Counter拆分为单位时间内的多个Buckets并滑动计算。 Leaky Bucket 与 Token Bucket 算法 漏桶算法简单的想象有一个木桶，有新请求就是不断的倒水进来，然后桶底下有个洞，按照固定的速率把水漏走，如果水进来的速度比漏走的快，桶可能就会满了，然后就拒绝请求。可见这里有两个变量，一个是桶的大小，支持流量突发增多时可以存多少的水(burst)，另一个是水桶漏洞的大小(rate)，可以简单的让burst等于rate，也可以让burst更大接收更多突发请求，伪代码如下：123456789101112131415161718192021double rate; // leak rate in calls/sdouble burst; // bucket size in callslong refreshTime; // time for last water refreshdouble water; // water count at refreshTimerefreshWater() &#123; long now = getTimeOfDay(); water = max(0, water- (now - refreshTime)*rate); // 水随着时间流逝，不断流走，最多就流干到0. refreshTime = now;&#125;bool permissionGranted() &#123; refreshWater(); if (water &lt; burst) &#123; // 水桶还没满，继续加1 water ++; return true; &#125; else &#123; return false; &#125;&#125; 但是对于很多情况下，除了要求能够限制平均处理速度外，还要求能允许一定程度的的突发情况。这样的话，漏桶算法就不合适了，用令牌桶算法更合适。Token Bucket 是与 Leaky Bucket 效果一样但方向相反的算法，更加容易理解。随着时间流逝，系统会按速率 1/rate 的时间间隔(如果rate=100，则间隔是10ms)往桶里加入Token(想象和漏洞漏水相反，有个水龙头在不断的加水)，如果桶已经满了就不再加了。新请求来临时，会各自拿走一个Token，如果没有Token可拿了就拒绝服务。 Google Guava中的RateLimiter，实际上就实现了Token Bucket的算法。它支持两种获取permits接口，一种是如果拿不到立刻返回false，一种会阻塞等待一段时间看能不能拿到。Leacky Bucket算法默认一开始水桶是空的，可以立即就接收最多burst的请求，而Token Bucket就要设置初始Token的数量。RateLimiter有两个子类，一个是WarmingUp，一个是Bursty。 WarmingUp，burst = warmUp时间/固定token添加间隔(即上例那个10ms)，初始token数量 = burst，有算法保证系统总是相对平滑。 Bursty， burst = rate或另外的参数设置，初始token数量 = 0 ，当系统冷了一段时间，支持突发到burst。Guava以micros为时间单位，计算token的变化。 guava RateLimiter123456789101112131415161718192021222324252627private static RateLimiter one=RateLimiter.create(2);//每秒2个 private static RateLimiter two=RateLimiter.create(2);//每秒2个 private RateLimitUtil()&#123;&#125;; public static void acquire(RateLimiter r,int num)&#123; double time =r.acquire(num); System.out.println("wait time="+time); &#125; public static void main(String[] args) throws InterruptedException &#123; acquire(one,1); acquire(one,1); acquire(one,1); System.out.println("-----"); acquire(two,10); acquire(two,1); &#125; 输出:wait time=0.0wait time=0.499163wait time=0.489308-----wait time=0.0wait time=4.497819 Guava RateLimiter在Web应用中的使用一般Web系统的访问限制都可以用容器本身来实现，比如tomcat就可以在connector上面配置connection数目的限制，servlet thread限制。有时候系统复杂后希望对不同服务提供不同的RateLimiter，例如对数据库操作要求比较大的速率小些，在内存可以处理的速率大写，还有可能对集群提供rate limiter服务。 这里记录下实践过程中系统如何使用RateLimiter来限制所有spring访问的访问速率。1234567891011121314151617181920212223242526272829public class RateLimiterFilter implements Filter &#123; private static Logger logger = Logger.getLogger(RateLimiterFilter.class); private RateLimiter limiter = null; public void init(FilterConfig config) throws ServletException &#123; limiter = RateLimiter.create(100); //100 request per second &#125; public void destroy() &#123; &#125; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException &#123; HttpServletRequest req = (HttpServletRequest) request; HttpServletResponse res = (HttpServletResponse) response; if(limiter.tryAcquire()) &#123; if(logger.isTraceEnabled())&#123; logger.trace("get access: "); &#125; chain.doFilter(request, response) &#125; else &#123; logger.info("system limitation reached!"); req.getRequestDispatcher("/WEB-INF/jsp/error/429.jsp").forward(req,res); &#125; &#125;&#125; 源码解析guava的限流算法有2种模式，一种是稳定速度，还有一种是生成令牌的速度慢慢提升直到维持在一个稳定的速度。2种模式原理类似，只是在具体等待多久的时间计算上有区别。以下就专门指稳定速度的模式。 先来看看它的acquire()方法：12345public double acquire(int permits) &#123; long microsToWait = reserve(permits);//先计算获取这些请求需要让线程等待多长时间 stopwatch.sleepMicrosUninterruptibly(microsToWait);//让线程阻塞microTowait微秒长的时间 return 1.0 * microsToWait / SECONDS.toMicros(1L);//返回阻塞的时间 &#125; 主要分3步： 1. 根据limiter创建时传入的参数，计算出生成这些数量的令牌需要多长的时间。 2. 让线程阻塞microTowait这么长的时间（单位：微秒） 3. 再返回阻塞了多久，单位：秒 具体它是怎么计算需要多长时间的呢？让我们来看看reserve(permits)方法。 12345678910111213141516171819202122232425262728final long reserve(int permits) &#123; checkPermits(permits);//检查参数是否合法 synchronized (mutex()) &#123; return reserveAndGetWaitLength(permits, stopwatch.readMicros()); &#125; &#125; ↓ ↓ ↓final long reserveAndGetWaitLength(int permits, long nowMicros) &#123; long momentAvailable = reserveEarliestAvailable(permits, nowMicros); return max(momentAvailable - nowMicros, 0); &#125; ↓ ↓ ↓final long reserveEarliestAvailable(int requiredPermits, long nowMicros) &#123; resync(nowMicros);//here long returnValue = nextFreeTicketMicros; double storedPermitsToSpend = min(requiredPermits, this.storedPermits); double freshPermits = requiredPermits - storedPermitsToSpend; long waitMicros = storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long) (freshPermits * stableIntervalMicros); this.nextFreeTicketMicros = nextFreeTicketMicros + waitMicros; this.storedPermits -= storedPermitsToSpend; return returnValue; 最终调用的是reserveEarliestAvailable方法。先看看resync(nowMicros)方法。 12345678private void resync(long nowMicros) &#123; // if nextFreeTicket is in the past, resync to now if (nowMicros &gt; nextFreeTicketMicros) &#123; storedPermits = min(maxPermits, storedPermits + (nowMicros - nextFreeTicketMicros) / stableIntervalMicros); nextFreeTicketMicros = nowMicros; &#125; &#125; nextFreeTicketMicros的意思是：下次获取的时候需要减去的时间。如果是第一次调用accquire()方法，那nowMicros - nextFreeTicketMicros 就是从初始化（初始化的时候会给nextFreeTicketMicros 赋值一次,具体可以看RateLimiter的构造器）到第一次请求，这中间发生的时间。 这个方法的意思，如果当前时间比上一轮设置的下次获取的时间大（因为存在提前获取的情况，比如上次直接获取了10个，那上轮设置的nextFreeTicketMicros就是上一轮的时间+5s。后面会提到），那就计算这个中间理论上能生成多少的令牌。比如这中间隔了1秒钟，然后stableIntervalMicros=5000（稳定生成速度的情况下）,那么，就这中间就可以生成2个令牌。再加上它原先存储的storedPermits个，如果比maxPermits大，那最大也只能存maxPermits这么多。如果比maxPermits小，那就是storedPermits=原先存的+这中间生成的数量。同时记录下下次获取的时候需要减去的时间，也就是当前时间 （nextFreeTicketMicros ）。接下来继续看reserveEarliestAvailable方法： 12345678910111213final long reserveEarliestAvailable(int requiredPermits, long nowMicros) &#123; //1 resync(nowMicros); //2 long returnValue = nextFreeTicketMicros;//3 double storedPermitsToSpend = min(requiredPermits, this.storedPermits);//4 double freshPermits = requiredPermits - storedPermitsToSpend;//5 long waitMicros = storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long) (freshPermits * stableIntervalMicros);//6 this.nextFreeTicketMicros = nextFreeTicketMicros + waitMicros;//7 this.storedPermits -= storedPermitsToSpend;//8 return returnValue;//9 &#125; 我们一行一行来看： 第二行设置好之后。第3行中将下次获取的时候需要减去的时间作为返回值（这点很重要）。 第4行是从存储的许可数量和请求的数量中获取小的那个值，第5行是获取这个值和请求的值的差。 这2句是什么意思呢？ 其实这2句就是使得RateLimiter能一定程度的突发请求的原因。假设requiredPermits=10，而我们能存的storedPermits=2，那么freshPermits=8，也就是多取了8个。而第6行就是计算这多取的8个需要多长时间才能生成？需要3秒。那么，就将这3秒钟加到我们前面赋值的“下次获取的时候需要减去的时间 ”。 比如在05秒的时候一次性获取了10个，那么，第7行的意思就是nextFreeTicketMicros=13S对应的系统的毫秒数。然后storedPermits就是-8。当过了1秒钟，下一次请求来调用acquire(1)的时候，resync方法中由于nowMicros1234final long reserveAndGetWaitLength(int permits, long nowMicros) &#123; long momentAvailable = reserveEarliestAvailable(permits, nowMicros); return max(momentAvailable - nowMicros, 0);//取较大的值 &#125; 也就是说，reserveAndGetWaitLength会返回max(13-6,0)，也就是7。而该方法的返回值又是用于sleep线程的，也就是我们在一开始看到的：12345public double acquire(int permits) &#123; long microsToWait = reserve(permits); stopwatch.sleepMicrosUninterruptibly(microsToWait); return 1.0 * microsToWait / SECONDS.toMicros(1L); &#125; 总结起来，最主要的是nowMicros,nextFreeTicketMicros这2个值。nextFreeTicketMicros在一开始构造器执行的时候会赋值一次为构造器执行的时间。当第一次调用accquire()的时候，resync会被执行，然后在accquire()中将nextFreeTicketMicros设置为当前时间。但是，需要注意的是，在reserveEarliestAvailable中会根据请求的令牌数和当前存储的令牌数进行比较。如果请求的令牌数很大，则会计算出生成这些多余的令牌需要的时间，并加在nextFreeTicketMicros上，从而保证下次调用accquire()的时候，根据nextFreeTicketMicros和当时的nowMicros相减，若&gt;0，则需要等到对应的时间。也就能应对流量的突增情况了。 所以最重要的是nextFreeTicketMicros，它记录了你这次获取的时候，能够开始生成令牌的时间。比如当前是05S，那若nextFreeTicketMicros=10，表示它要到10S才能开始生成令牌，谁叫前面的多拿了这么多呢。至于它这次是多拿了还是只是拿一个令牌，等待时间都是这么多。如果这次又多拿了，那下次就等待更久！ 123456789101112131415private static RateLimiter too=RateLimiter.create(2);//每秒2个 private RateLimitUtil()&#123;&#125;; public static void acquire(RateLimiter r,int num)&#123; double time =r.acquire(num); System.out.println("wait time="+time); &#125; public static void main(String[] args) throws InterruptedException &#123; acquire(too,1); acquire(too,10);//只等待了0.5秒就获取了10个 acquire(too,10);//等待了5秒就获取了10个 acquire(too,1);//虽然只获取1个，也是等待5秒 &#125; 参考:https://github.com/springside/springside4/wiki/Rate-Limiterhttp://blog.csdn.net/FoolishAndStupid/article/details/76285690http://blog.csdn.net/jiesa/article/details/50412027http://ifeve.com/guava-ratelimiter/https://github.com/google/guava/blob/fd919e54a55ba169dc7d9f54b7b3485aa7fa0970/guava/src/com/google/common/util/concurrent/RateLimiter.javahttps://github.com/google/guava/blob/fd919e54a55ba169dc7d9f54b7b3485aa7fa0970/guava-tests/test/com/google/common/util/concurrent/RateLimiterTest.java]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MicroService</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Consistent-Hash一致性哈希算法]]></title>
    <url>%2F2017%2F12%2F17%2FConsistent-Hash%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[理论部分一致性哈希算法在1997年由麻省理工学院提出的一种分布式哈希（DHT）实现算法，设计目标是为了解决因特网中的热点(Hot spot)问题。一致性哈希修正了简单哈希算法带来的问题，使得分布式哈希（DHT）可以在P2P环境中真正得到应用。 一致性hash算法提出了在动态变化的Cache环境中，判定哈希算法好坏的四个定义：1、平衡性(Balance)：平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。2、单调性(Monotonicity)：单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。3、分散性(Spread)：在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。4、负载(Load)：负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同 的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 场景描述：在了解一致性哈希算法之前，最好先了解一下缓存中的一个应用场景。假设，我们有三台缓存服务器，用于缓存图片，我们为这三台缓存服务器编号为0号、1号、2号，现在，有3万张图片需要缓存，我们希望这些图片被均匀的缓存到这3台服务上，以便它们能够分摊缓存的压力。原始的做法是对缓存项的键进行哈希，将hash后的结果对缓存器的数量进行取模操作，通过取模后的结果，决定缓存项将会缓存在哪一台服务器上，我们仍然以刚才描述的场景为例，假设我们使用名称作为访问图片的key，假设图片名称是不重复的，那么，我们可以使用如下公式，计算出图片应该存放在哪台服务器上。hash（图片名称）% N,我们暂时称上述算ASH算法或者取模算法。 但是，使用上述HASH算法进行缓存时，会出现一些缺陷，试想一下，如果3台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？没错，很简单，多增加两存服务器不就行了，假设，我们增加了一台缓存服务器，那么缓存服务器的数量就由3台变成了4台，此时，如果仍然使用上述方法对同一张图片进行缓存，那么这张图片所在务器编号必定与原来3台服务器时所在的服务器编号不同，因为除数由3变为了4，被除数不变的情况下，余数肯定不同，这种情况带来的结果就是当服务器数量变动时，所有的位置都要发生改变，换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端服务器请求数据，同理，假设3存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从3台变为2台，如果想要访张图片，这张图片的缓存位置必定会发生改变，以前缓存的图片也会失去缓存的作用与意义， 由于大量缓存在同一时间失效，造成了缓存的雪崩，此时前端缓存已经无法起到部分压力的作用，后端服务器将会承受巨大的压力，整个系统很有可能被压垮，所以，我们应该想办法不让这种情况发生，但是由于上述HASH算法本身的缘故，使用取模法缓存时，这种情况是无法避免的，为了解决这些问题，一致性哈希算法诞生了。 我们来回顾一下使用上述算法会出现的问题。问题1：当缓存服务器数量发生变化时，会引起缓存的雪崩，可能会引起整体系统压力过大而崩溃（大量缓存同一时间失效）。问题2：当缓存服务器数量发生变化时，几乎所有缓存的位置都会发生改变，怎样才能尽量减少受影响的缓存呢？ 其实，一致性哈希算法也是使用取模的方法，只是，刚才描述的取模法是对服务器的数量进行取模，而一致性哈希算法是对2^32取模.首先，我们把二的三十二次方想象成一个圆，就像钟表一样，钟表的圆可以理解成由60个点组成的圆，而此处我们把这个圆想象成由2^32个点组成的圆,称为Hash环.仍然以之前描述的场景为例，假设我们有3台缓存服务器，服务器A、服务器B、服务器C，那么，在生产中，这三台服务器肯定有自己的IP地址，我们使用它们各自的IP地址进行哈希计算，使用哈希后的结果对2^32取模，可以使用如下公式示意。hash（服务器A的IP地址） % 2^32通过上述公式算出的结果一定是一个0到2^32-1之间的一个整数，我们就用算出的这个整数，代表服务器A，既然这个整数肯定处于0到2^32-1之间，那么，上图中的hash环定有一个点与这个整数对应，而我们刚才已经说明，使用这个整数代表服务器A，那么，服务器A就可以映射到这个环上,以此类推。 好了，到目前为止，我们已经把缓存服务器与hash环联系在了一起，我们通过上述方法，把缓存服务器映射到了hash环上，那么使用同样的方法，我们也可以将需要缓存的映射到hash环上。假设，我们需要使用缓存服务器缓存图片，而且我们仍然使用图片的名称作为找到图片的key，那么我们使用如下公式可以将图片映射到上图中的hash环上。hash（图片名称） % 2^32 现在服务器与图片都被映射到了hash环上，那么这个图片到底应该被缓存到哪一台服务器上呢？从图位置开始，沿顺时针方向遇到的第一个服务器就是需要缓存该图片的服务器。节点机器的失效删除：节点机器的添加：总结一下：1、与简单哈希不同，一致性hash有个hash环，同时会对服务器和需要换乘的对象进行hash，最终所有结果都会落到这个hash环上。然后根据顺时针规则找到最近的服务器进行缓存的存取。2、基于第1点,一致性hash无论是新增主机还是删除主机,需要改变位置的都是离那台主机最近的那些缓存,其他换成不需要改变位置。从而避免当服务器数量发生变化时，会产生缓存的雪崩。(如果使用之前的算法，服务器数量发生改变时，所有服务器的所有缓存在同一时间失效了，而使用一致性哈希算法时，服务器的数量如果发生改变，并不是所有缓存都会失效，而是只有部分会失效，而不至于所有压力都在同一时间集中到后端服务器上。) hash环的偏斜在介绍一致性哈希的概念时，我们理想化的将3台服务器均匀的映射到了hash环上但是，理想很丰满，现实很骨感，我们想象的与实际情况往往不一样。在实际的映射中，服务器可能会被映射成如下模样。如果服务器被映射成上图中的模样，那么被缓存的对象很有可能大部分集中缓存在某一台服务器上。如果出现上图中的情况，A、B、台服务器并没有被合理的平均的充分利用，缓存分布的极度不均匀，而且，如果此时服务器A出现故障，那么失效缓存的数量也将达到最大值，在极端情况下，仍然有可能引统的崩溃，上图中的情况则被称之为hash环的偏斜，那么，我们应该怎样防止hash环的偏斜呢？一致性hash算法中使用”虚拟节点“解决了这个问题 虚拟节点如果想要均衡的将缓存分布到3台服务器上，最好能让这3台服务器尽量多的、均匀的出现在hash环上，但是，真实的服务器资源只有3台，我样凭空的让它们多起来呢，没错，就是凭空的让服务器节点多起来，既然没有多余的真正的物理服务器节点，我们就只能将现有的物理节点通过虚拟的方法复制出来，这些由节点虚拟复制而来的节点被称为”虚拟节点”。加入虚拟节点以后的hash环如下。在一致性哈希算法中，为了尽可能的满足平衡性，其引入了虚拟节点。“虚拟节点(virtual node )”是实际节点（实际的物理服务器）在hash环上的复制品,一个实际节点可以对应多个虚拟节点。从上图可以看出，A、B、C三台服务器分别虚拟出了一个虚拟节点，当然，如果你需要，也可以虚拟出更多的虚拟节点。引入虚拟节点的概念后，缓存的分布就均衡多了，上中，1号、3号图片被缓存在服务器A中，5号、4号图片被缓存在服务器B中，6号、2号图片被缓存在服务器C中，如果你还不放心，可以虚拟出更多的虚拟节点，以便减小hash偏斜所带来的影响，虚拟节点越多，hash环上的节点就越多，缓存被均匀分布的概率就越大。 通过虚拟节点的引入，对象的分布就比较均衡了。那么在实际操作中，正真的对象查询是如何工作的呢？对象从hash到虚拟节点到实际节点的转换如下图：“虚拟节点”的hash计算可以采用对应节点的IP地址加数字后缀的方式。例如假设NODE1的IP地址为192.168.1.100。引入“虚拟节点”前，计算 cache A 的 hash 值：Hash(“192.168.1.100”);引入“虚拟节点”后，计算“虚拟节”点NODE1-1和NODE1-2的hash值：Hash(“192.168.1.100#1”); // NODE1-1Hash(“192.168.1.100#2”); // NODE1-2 Java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101import java.util.Collection;import java.util.HashSet;import java.util.Iterator;import java.util.Set;import java.util.SortedMap;import java.util.SortedSet;import java.util.TreeMap;import java.util.TreeSet;public class ConsistentHash&lt;T&gt; &#123; private final HashFunction hashFunction; private final int numberOfReplicas;// 节点的复制因子(100左右比较合理),实际节点个数 * numberOfReplicas =虚拟节点个数 private final SortedMap&lt;Long, T&gt; circle = new TreeMap&lt;Long, T&gt;();// 存储虚拟节点的hash值到真实节点的映射,server节点分布圆 public ConsistentHash(HashFunction hashFunction, int numberOfReplicas, Collection&lt;T&gt; nodes) &#123; this.hashFunction = hashFunction; this.numberOfReplicas = numberOfReplicas; for (T node : nodes) add(node); &#125; public void add(T node) &#123; for (int i = 0; i &lt; numberOfReplicas; i++) // 对于一个实际机器节点 node, 对应 numberOfReplicas 个虚拟节点 /* * 不同的虚拟节点(i不同)有不同的hash值,但都对应同一个实际机器node * 虚拟node一般是均衡分布在环上的,数据存储在顺时针方向的虚拟node上 */ circle.put(hashFunction.hash(node.toString() + i), node); &#125; public void remove(T node) &#123; for (int i = 0; i &lt; numberOfReplicas; i++) circle.remove(hashFunction.hash(node.toString() + i)); &#125; /* * 获得一个最近的顺时针节点,根据给定的key 取Hash * 然后再取得顺时针方向上最近的一个虚拟节点对应的实际节点 * 再从实际节点中取得 数据 */ public T get(Object key) &#123; if (circle.isEmpty()) return null; long hash = hashFunction.hash((String) key);// node 用String来表示,获得node在哈希环中的hashCode if (!circle.containsKey(hash)) &#123;//数据映射在两台虚拟机器所在环之间,就需要按顺时针方向寻找机器 SortedMap&lt;Long, T&gt; tailMap = circle.tailMap(hash); hash = tailMap.isEmpty() ? circle.firstKey() : tailMap.firstKey(); &#125; return circle.get(hash); &#125; public long getSize() &#123; return circle.size(); &#125; /* * 查看MD5算法生成的hashCode值---表示整个哈希环中各个虚拟节点位置 */ public void testBalance()&#123; Set&lt;Long&gt; sets = circle.keySet();//获得TreeMap中所有的Key SortedSet&lt;Long&gt; sortedSets= new TreeSet&lt;Long&gt;(sets);//将获得的Key集合排序 for(Long hashCode : sortedSets)&#123; System.out.println(hashCode); &#125; System.out.println("----each location 's distance are follows: ----"); /* * 查看用MD5算法生成的long hashCode 相邻两个hashCode的差值 */ Iterator&lt;Long&gt; it = sortedSets.iterator(); Iterator&lt;Long&gt; it2 = sortedSets.iterator(); if(it2.hasNext()) it2.next(); long keyPre, keyAfter; while(it.hasNext() &amp;&amp; it2.hasNext())&#123; keyPre = it.next(); keyAfter = it2.next(); System.out.println(keyAfter - keyPre); &#125; &#125; public static void main(String[] args) &#123; Set&lt;String&gt; nodes = new HashSet&lt;String&gt;(); nodes.add("A"); nodes.add("B"); nodes.add("C"); ConsistentHash&lt;String&gt; consistentHash = new ConsistentHash&lt;String&gt;(new HashFunction(), 100, nodes); consistentHash.add("D"); System.out.println("hash circle size: " + consistentHash.getSize()); System.out.println("location of each node are follows: "); consistentHash.testBalance(); //根据一致性hash算法获取客户端对应的服务器节点 System.out.println(consistentHash.get(RandomStringUtils.random(12))); &#125; &#125; 哈希函数如下：12345678910111213141516171819202122232425262728import java.security.MessageDigest;import java.security.NoSuchAlgorithmException;/* * 实现一致性哈希算法中使用的哈希函数,使用MD5算法来保证一致性哈希的平衡性 */public class HashFunction &#123; private MessageDigest md5 = null; public long hash(String key) &#123; if (md5 == null) &#123; try &#123; md5 = MessageDigest.getInstance("MD5"); &#125; catch (NoSuchAlgorithmException e) &#123; throw new IllegalStateException("no md5 algrithm found"); &#125; &#125; md5.reset(); md5.update(key.getBytes()); byte[] bKey = md5.digest(); //具体的哈希函数实现细节--每个字节 &amp; 0xFF 再移位 long result = ((long) (bKey[3] &amp; 0xFF) &lt;&lt; 24) | ((long) (bKey[2] &amp; 0xFF) &lt;&lt; 16 | ((long) (bKey[1] &amp; 0xFF) &lt;&lt; 8) | (long) (bKey[0] &amp; 0xFF)); return result &amp; 0xffffffffL; &#125;&#125; 代码片段解释:在具体JAVA实现代码中，定义了一个TreeMap&lt; k, V&gt;用来保存虚拟机器节点到实际的物理机器的映射。机器以字符串形式来标识，故hash函数的参数为String而对于 数据的存储而言，逻辑上是按顺时针方向存储在虚拟机器节点中，虚拟机器节点通过TreeMap知道它实际需要将数据存储在哪台物理机器上。此外，TreeMap中的Key是有序的，而环也是顺时针有序的，这样才能当数据被映射到两台虚拟机器之间的弧上时，通过TreeMap的 tailMap()来寻找顺时针方向上的下一台虚拟机。1234if (!circle.containsKey(hash)) &#123;//数据映射在两台虚拟机器所在环之间,就需要按顺时针方向寻找机器 SortedMap&lt;Long, T&gt; tailMap = circle.tailMap(hash); hash = tailMap.isEmpty() ? circle.firstKey() : tailMap.firstKey(); &#125; 参考:https://www.codeproject.com/Articles/56138/Consistent-hashinghttp://blog.csdn.net/cywosp/article/details/23397179http://www.zsythink.net/archives/1182http://blog.csdn.net/zhangskd/article/details/50256111https://www.cnblogs.com/hapjin/p/4737207.htmlhttp://langyu.iteye.com/blog/684087http://afghl.github.io/2016/11/19/implement-consistent-hashing.htmlhttps://gist.github.com/meigesir/1bf6338787946c18b47d]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>MicroService</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[读书笔记]chapter4-系统稳定性-大型网站分布式架构与设计实践]]></title>
    <url>%2F2017%2F12%2F17%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chapter4-%E7%B3%BB%E7%BB%9F%E7%A8%B3%E5%AE%9A%E6%80%A7-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%AE%BE%E8%AE%A1%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[主要内容:常用的日志分析命令，如cat,grep,wc,less,sed,awk等如何进行集群的监控，包括监控指标的定义、心跳检测、容量评估等如何保障高并发系统的稳定运行，如采用流量控制、依赖管理、服务分级、开关等策略。如何优化应用的性能，包括前端优化、Java程序优化、数据库查询优化等如何进行Java应用故障的在线排查，包括一系列排查工具的使用，及案例。 “You can’t control what you can’t measure”“Should you measure something be sure what you really measure otherwise the results can keep you far from reality. Always validate your assumptions and RTFM”“Don’t assumptions base on lack of understanding of used terminology or it’s ambiguity can be accounted for it” 在线日志分析日志包含信息：异常堆栈信息，访问ip，请求url，应用响应时间，内存垃圾回收信息，及程序日志信息。通过对异常堆栈信息分析定位程序bug；对访问ip及url，参数的分析排查是否遭到攻击，及攻击的形式；通过应用的响应时间、垃圾回收及系统load来判断系统负载；通过线程dump，判断是否死锁及现场阻塞的原因；通过应用的GC(Garbage Collection)日志，对系统代码和JVM内存参数今夕优化，减少GC次数与stop the world时间，优化响应时间。 1、日志分析常用命令1234567891011cat 查看文件内容， -n 显示行号；分页查看的有:more/less; head/tail 显示文件头/尾多少行， -n指定行数， 对于tail -f可以实时持续显示最新的行。 sort:对列进行排序，默认按字符排序，-n指定按数字，-r逆序，-k指定列(从1开始)，-t指定列间分隔符(默认空格) wc:字符统计，-l 行数，-c字节数，-L最长行的长度 ，-w单词数 uniq:连续行去重,通常与sort联合使用。-c 显示重复次数，-u只显示不重复的，-d只显示重复的行 grep:字符串查找，-c显示行数, grep支持正则表达式。 find:文件查找 find path -name [filename|print] whereis:定位可执行文件的位置 expr:表达式求值，*需要转义 tar:归档文件 curl:URL访问工具 cut:过滤指定列，-f指定列号，-d指定列分隔符 例子:nginx为例，查看请求的访问量，访问量排名前10的ip地址。这样可以定位是否存在HTTP flood攻击(也称CC攻击).1cat access.log | cut -f1 -d &quot; &quot; | sort | uniq -c | sort -k 1 -n -r |head -10 例子:页面访问量排名前10的url1cat access.log | cut -f4 -d &quot; &quot; | sort | uniq -c | sort -k 1 -n -r |head -10 例子:查看最耗时页面(响应时间最长的url)1cat access.log | sort -k 2 -n -r |head -10 例子:统计404请求的占比(404请求过多，要么就是有恶意攻击者在进行扫描，要么就是系统出问题了，500也是如此。)1export total_line=`wc -l access.log | cut -f1 -d &quot; &quot;` &amp;&amp; export not_found_line=`awk &apos;$6==&apos;404&apos;&#123;print $6&#125;&apos; access.log|wc -l` &amp;&amp; expr $not_found_line \* 100 / $total_line 2、日志分析脚本sed：流编辑器，一行一行读取，面向行，不会修改文件本身。 set [options] ‘command’ files 将日志中的xxx替换成yahoo输出: sed ‘s/xxx/yahoo/‘ access.log | head -10筛选日志中指定的行输出: sed -n ‘2,6p’ access.log根据正则表达式删除日志中指定的行 sed ‘/qq/d’ access.log支持将command写到文件里再加载执行: sed [options] -f scriptfile files awk:提供一个类似于编程的开放环境，可以自定义文本处理规则，修改和重新组织文件中的内容。awk [option] ‘pattern {action}’ fileawk ‘/google/{print $5,$6}’ access.log | head -10awk ‘length($0)&gt;40{print $3}’ access.log | head -10 $0表示当前行awk ‘{line = sprintf(“method:%s,response:%s”,$3,$7);print line}’ access.log | head -10支持将command写到文件里再加载执行: awk [options] -f scriptfile files 集群监控1、监控指标系统运行的繁忙程度、健康状态，反映在一系列的运行期指标上。理论基础：木桶原理。指标如CPU负载，磁盘IO，内存占用，FullGC，请求QPS过高，网络繁忙，丢包率等。 load: load即特定时间间隔内运行队列中的平均线程数。如果一个线程没有处于IO等待、等待wait、终止等状态时，那么该线程就会处于运行队列中。每个CPU的核都维护了一个运行队列，系统的load主要由运行队列来决定。load值越大说明系统CPU越繁忙。一般来说，只要每个CPU当前的活动现场数不大于3，其负载就可以认为是正常的，如果大于5，则表示负载挺高了，需要采取措施来降低系统负载。top和uptime可以查看系统的load值。执行uptime将会显示出系统的当前时间、上线时间、当前的用户数量以及过去1、5、15分钟内的系统负载。12$ uptime10:52 PM up 1337 days, 7:45, 3 users, load averages: 0.21, 0.24, 0.23 CPU利用率： CPU消耗主要在这几个方面:用户进程，内核进程，中断处理，I/O等待，Nice时间(优先级处理)，丢失时间，空闲等。CPU利用率就是这些时间所占总时间的百分比。top命令查看:123456789top | grep Cpu us — 用户空间占用CPU的百分比。sy — 内核空间占用CPU的百分比。ni — 改变过优先级的进程占用CPU的百分比id — 空闲CPU百分比wa — IO等待占用CPU的百分比hi — 硬中断（Hardware IRQ）占用CPU的百分比si — 软中断（Software Interrupts）占用CPU的百分比 st — 丢失时间(Steal Time),是在硬件虚拟化开始流行后新增的，表示被强制等待虚拟CPU的时间，此时hypervisor正在为另一个虚拟处理器服务。如果st占比较高，则表示当前虚拟机与宿主上的其他虚拟机间的CPU争用较为频繁。 若 %iowait 的值过高，表示硬盘存在I/O瓶颈 若 %idle 的值高但系统响应慢时，有可能是 CPU 等待分配内存，此时应加大内存容量 若 %idle 的值持续低于 10，则系统的 CPU 处理能力相对较低，表明系统中最需要解决的资源是 CPU。 对于多U多核CPU监控，在top基本视图中，按键盘数字“1”，可监控每个逻辑CPU的状况。如果按Shift+H键，则可以按线程来查看CPU消耗情况。这一点对java应用来说很有用。 Linux查看物理CPU个数、核数、逻辑CPU个数123456789# 总核数 = 物理CPU个数 X 每颗物理CPU的核数# 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数# 查看物理CPU个数cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l# 查看每个物理CPU中core的个数(即核数)cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq# 查看逻辑CPU的个数cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l 磁盘剩余空间: df -h 查看磁盘剩余空间du 查看目录和文件的大小，如du -d 1 -h /home/xby , -d指定递归深度 网络traffic: 对于进行负载均衡和反向代理的节点，或作为集群master的节点，对网卡和带宽的要求更高。在某些突发大流量情况下有可能会成为瓶颈。因此关注网络的流量，清楚各节点的阀值和水位也很重要。sar -n DEV 1 1 查看系统网络状态。-n 汇报网络状态，DEV表示查看各个网卡的流量，1表示抽样间隔为1秒1次，后面的1表示抽样总次数。 12345678IFACE：就是网络设备的名称；rxpck/s：每秒钟接收到的包数目txpck/s：每秒钟发送出去的包数目rxbyt/s：每秒钟接收到的字节数txbyt/s：每秒钟发送出去的字节数rxcmp/s：每秒钟接收到的压缩包数目txcmp/s：每秒钟发送出去的压缩包数目txmcst/s：每秒钟接收到的多播包的包数目 如果你使用SOCK关键字，则会针对socket连接进行汇报，例如：123456$ sar -n SOCK 1 3tcpsck：当前正在被使用于TCP的socket数目udpsck：当前正在被使用于UDP的socket数目rawsck：当前正在被使用于RAW的socket数目ip-frag：当前的IP分片的数目如果你使用FULL关键字，相当于DEV、EDEV和SOCK三者的综合。 让sar在某个特定时间结束12sar 1 0 -e 15:00:00 &gt; data.txt //每隔1秒记录CPU的使用情况，直到15点，数据将保存到data.txt文件中。(-e 参数表示结束时间，注意时间格式：必须为hh:mm:ss格式) 磁盘I/O iostat -d -k 查看系统的I/O状况，-d表示查看磁盘使用情况 -k表示kb单位。12345678tps：该设备每秒的传输次数（transfers per second）。kB_read/s：每秒从设备（drive expressed）读取的数据量；kB_wrtn/s：每秒向设备（drive expressed）写入的数据量；kB_read：读取的总数据量；kB_wrtn：写入的总数量数据量；这些单位都为Kilobytes。 await表示平均每次设备I/O操作的等待时间（以毫秒为单位）。 svctm表示平均每次设备I/O操作的服务时间（以毫秒为单位）。%util表示一秒中有百分之几的时间用于I/O操作。 对以磁盘IO性能，一般有如下评判标准：正常情况下svctm应该是小于await值的，而svctm的大小和磁盘性能有关，CPU、内存的负荷也会对svctm值造成影响，过多的请求也会间接的导致svctm值的增加。await值的大小一般取决与svctm的值和I/O队列长度以及I/O请求模式，如果svctm的值与await很接近，表示几乎没有I/O等待，磁盘性能很好，如果await的值远高于svctm的值，则表示I/O队列等待太长，系统上运行的应用程序将变慢，此时可以通过更换更快的硬盘来解决问题。%util项的值也是衡量磁盘I/O的一个重要指标，如果%util接近100%，表示磁盘产生的I/O请求太多，I/O系统已经满负荷的在工作，该磁盘可能存在瓶颈。长期下去，势必影响系统的性能，可以通过优化程序或者通过更换更高、更快的磁盘来解决此问题。 内存使用:free -m 可用内存=free+buffers+cached123456789101112131415161718[root@nonamelinux ~]# free total used free shared buffers cachedMem: 386024 377116 8908 0 21280 155468-/+ buffers/cache: 200368 185656Swap: 393552 0 393552第二行(mem)：total:总计物理内存的大小。used:已使用多大。free:可用有多少。Shared:多个进程共享的内存总额。Buffers/cached:缓存的大小。第三行(-/+ buffers/cached):used:已使用多大。free:可用有多少。第二行(mem)的used/free与第三行(-/+ buffers/cache) used/free的区别。这两个的区别在于使用的角度来看，第一行是从OS的角度来看，因为对于OS，buffers/cached 都是属于被使用，所以他的可用内存是8908KB,已用内存是377116KB,其中包括，内核（OS）使用+Application(X,oracle,etc)使用的+buffers+cached.第三行所指的是从应用程序角度来看，对于应用程序来说，buffers/cached 是等于可用的，因为buffer/cached是为了提高文件读取的性能，当应用程序需在用到内存的时候，buffer/cached会很快地被回收。所以从应用程序的角度来说，可用内存=free+buffers+cached 使用vmstat查看swap I/O的情况,确保swap I/O较低。否则会严重影响系统性能。 QPS-query per second. 每秒的查询数： qps在很大程度上代表了系统在业务上的繁忙程度，而每次请求的背后，可能对应着多次磁盘io，多次网络请求，以及多个cpu时间片。通过关注qps是否超过阀值，来决定是否进行扩容，以避免压力过大而宕机。 RT-response time 响应时间: rt是一个非常关键的指标，直接关系到前端用户的体验。因此需要尽可能降低rt。例如，通过CDN缩短用户请求的物理路径，通过内容压缩来减少传输的字节数，使用缓存来减少磁盘io和网络请求等。而通过nginx的access log ，便可以得知每个请求的响应时间。不过需要在访问日志的输出格式中加上$request_time变量。 数据库相关指标select/ps,update/ps,delete/ps. 措施:增加读库，分库分表等。 GC:对于Java应用而言，不得不关注GC。GC又分为Minor GC与Full GC.在JVM内存分代回收的情况下，对象在JVM内存的新生代Eden区中分配。当Eden区没有足够的空间时虚拟机将发起一次MinorGC，该GC发生在新生代，而且也会比较频繁，回收速度也很快。而Major GC,也称Full GC,是发生在年老代的，速度比MinorGC慢得多，因此导致应用停顿时间(stop the world)也就更长。可以对JVM的一些内存参数进行调整和优化，以降低GC时应用停止响应的时间。如果一个应用频繁进行Full GC，那么它的性能肯定是有问题的。这时候就需要我们实时获取GC情况。 2、心跳检测对于集群服务器和部署于其上的应用的心跳检测是必不可少的，因为这可以帮助我们及时感知问题。对于自治的分布式系统而言，一般都有一整套的集群心跳检测机制，能够实时地移除掉宕机的Slave，避免路由规则再次分配到它。如果是Master宕机，集群也能够自动进行Master选举。如Zookeeper.也有一部分系统如MySQL,Nginx，可以通过外部干预，使备份机器stand by，或是双机互为备份，以实现故障切换，避免单点故障。ping 是最常用的心跳检测方法。 -c 指定执行次数应用层检测：虽然ping可以检测网络是否畅通，但是对于应用层而言，即使网络畅通，也有可能出现问题，如频繁FullGC导致应用不能响应等。应用可以开放一个自检接口，我们可以通过脚本curl去请求自检接口，来达到心跳检测的目的。 curl -s选项为静默模式。 3、容量评估和应用水位 新系统上线之前或者已上线运行的系统上需要做一些推广活动时，相关的业务方需要对系统的访问量进行评估。业务方给出PV,UV预估，然后我们再逐一细化，推导出落到每一个独立的系统，接口上的流量大概是多少，这样一来，每个子系统所承载的量也就清晰了。然后再评估机器的数量，网络的带宽和技术实现方式。压力测试最关系的是qps和rt两个指标在进行系统峰值评估时，一般会遵循80/20原则。我们也可以通过水位图来了解系统的压力。当前水位=当前总qps/(单台机器极限qpsx机器数)x100% 流量控制为了防止某些热点事件或者推广活动导致的访问量激增的情况，需要做好流量控制，在流量到来之前，指定相应的应急预案，以避免系统被激增的流量压垮。流量控制可以从多个维度来进行，如系统的总并发请求数限制，或者限制单位时间内的请求次数(如限制qps)，或者通过白名单机制来限制每一个接入系统调用的频率等。流控实现，最简单的是基于java的信号量Semaphore,更高级点的是采用漏桶或令牌桶算法，guava中有个RetaLimiter实现了令牌桶算法。还有种方式，就是将消息异步化，扔到消息队列后不管，直接返回响应给用户。如基于ActiveMQ,不过需要考虑消息积压，事务消息，重复投递去重等问题。 服务稳定性：分布式SOA环境下系统的依赖错综复杂。如何控制由于第三方服务不稳定而形成的多米诺骨牌效应。1、依赖管理：分布式系统由于高度解耦，最终形成一个网状的依赖关系。对于服务提供者而言，它必须轻蹙，谁调用了自己，调用的频次怎样，这样才能知道当前系统的压力和水位在一个怎样的层次上，是否需要进行扩容。同时，服务提供者也需要对自己依赖的服务了然于胸，哪些是核心链路所依赖的服务，哪些是非核心链路的依赖，以便依赖的系统出现问题时，及时进行服务降级,避免因非核心依赖导致的故障传导，影响当前系统的稳定性。分布式依赖管理的依据：通过调用日志的收集和整理，将其中的调用关系，频次统计分析出来. 2、优雅降级通过依赖管理，我们知道了服务间的调用关系。接下来，我们便可以根据当前系统所依赖的服务及系统流程，来判断依赖的服务是否会影响应用的主流程，以此来决定当前应用依赖的优先级。当依赖的服务出现不稳定，响应缓慢或者掉调用超时，宕机等时，当前系统需要能够及时感知并进行相应处理，否则大量超时的调用，有可能将当前系统的线程和可用连接数用完，导致新的请求进不来，服务僵死，这便是故障传递。最终形成多米诺骨牌效应。使得整个集群都不能对外提供服务。这时服务调用优雅降级的重要性便体现出来了。对于调用超时的非核心服务，可以设定一个阀值，如果调用超时的次数超过这个阀值，便自动将该服务降级。此时跳过对该服务的调用，并指定一个休眠的时间点进行重试。 3、服务分级服务提供者需要对服务消费者的优先级进行区分，哪些调用将影响核心链路，哪些调用是非核心链路，当系统压力过大时，必须确保等级高的应用、核心的调用链路优先畅通，而其他的可以暂时”丢车保帅”。 4、开关系统需要预先定义一些开关来控制程序的服务提供策略。 5、应急方案应急方案需要明确地规定服务的级别，梳理清楚核心应用的调用链路，对于每一种故障，都做出合理的假设，并且要有针对性的处理方法。对于级别低的调用和功能，事先应准备好屏蔽的开关和接口。服务的级别决定哪些调用者是”车”，哪些调用者是”帅”，必要时候要丢车保帅。备用扩容，开关，验证码，流控，负载均衡策略动态修改，多机房部署，异地容灾等 高并发系统设计高并发系统设计与普通系统设计的区别在于，既要保障系统的可用性和可扩展性，又要兼顾数据的一致性。还要处理多线程同步的问题。任何细微问题，都有可能在高并发环境下被无限放大，直至系统宕机。 1、操作原子性：线程锁，CAS(CompareAndSet) Atomic类,数据库事务操作(ACID,Atomic,Consistency,Isolation,Durability)2、数据一致性:分布式系统常常通过复制数据来提高系统的可靠性和容错性，并且将数据的副本放到不同的机器上。由于多个副本的存在，使得维护副本一致性的代价很高。因此许多分布式系统都采用弱一致性或者是最终一致性，来提高系统的性能和吞吐能力，所以出现了不同的一致性模型和算法。 强一致性要求无论数据的更新操作是在哪个副本上执行，之后所有的读操作都要能够获取到更新的最新数据。这种情况下需要通过分布式事务来保证操作的原子性，并且外界无法读到系统的中间状态。 弱一致性指的是系统的某个数据被更新后，后续对该数据的读取操作取到的可能是更新前的值，也可能是更新后的值。全部用户完全读取到更新后的数据需要经过一段时间，这段时间称为”不一致性窗口”。 最终一致性是弱一致性的特殊形式，与弱一致性的区别就是”不一致性窗口”的时间依赖于网络的延迟、系统的负载、副本的个数。 分布式系统采用最终一致性的例子很多，如MySQL的主从数据同步，ZooKeeper的Leader election和Atomic broadcast等。 3、系统可扩展性/可伸缩性 是一种对软件系统计算处理能力的评价指标。高可扩展性意味着系统只要经过很少的改动，甚至只需要添加硬件设备，便能够实现整个系统处理能力的线性增长。由于单台机器硬件受制于科技发展水平和成本，因此，可扩展性更加侧重于系统的水平扩展。设计好的系统可以限制扩展。系统的可扩展性也会受到一些因素的制约，CAP理论(Consistency,Availability,Tolerance of network Partition)指出，系统的一致性、可用性和可扩展性这三个要素对于分布式系统来说，很难同时满足。因此在设计过程中需要进行一些取舍。某些情况下可以放宽对于一致性的严格要求，以使得系统更易于扩展，可靠性更高。 4、例子：并发减库存采用图像验证码防止机器请求(现在图像识别，打码平台的出现，对验证码的要求更高了，不然很容易被绕过)对于高并发访问的浏览型系统来说，单机数据库如不进行扩展，往往很难支撑。因此常常会采用分库技术来提高数据库的并发能力，并通过分布式缓存技术，降低磁盘io及数据库压力，加快后端的响应速度，qps也就越高。使用分库和缓存技术，吞吐量的确是上去了，但是却带来了跨数据库或者是分布式缓存与数据库之间难以进行事务操作。(分布式事务实现所需付出的性能代价太高)为了避免数据不一致的情况发生，可采用实际库存和浏览库存分离的方式。浏览库存取缓存数据。MySQL的myisam对写操作采用表锁，innodb则是行锁。但即便是行锁缩小了粒度，仍然在高并发修改某一行的情况下可能会出现性能瓶颈，此时我们需要拆行，将原本一行的存储，放在多行上，路由策略可以采用用户id取模等方式。 性能优化如何找到性能瓶颈Web的性能优化涉及前端优化、服务端优化、操作系统优化、数据库查询优化、JVM调优等。对于性能优化来说，第一步也是最重要的一步，便是寻找可以优化的点，即性能瓶颈。根据木桶原理，性能瓶颈就是那块最短的木板。 1、前端优化工具-YSlow：网页性能分析2、页面响应时间：这个受影响因素会很多，不能作为最终的依据。我们更关注服务端的RT(response time)时间。3、方法响应时间：定位到响应慢的请求以后，接下来需要深入发掘导致请求响应慢的原因，并且定位到具体的代码。通过对代码的检查分析，能够定位到具体的方法和代码行。不过我们一般借助于btrace——java动态跟踪工具来快速定位和发现耗时的方法。首先，编写一段测试代码：12345678910@Overrideprotected void doPost(HttpServletRequest req,HttpServletResponse resp) throws ServletException,IOException&#123; PrintWriter out=resp.getWriter(); try&#123; Thread.sleep(500L);//通过休眠模拟执行时间较长的方法 &#125;catch(InterruptException e)&#123; &#125; out.write("success");&#125; 然后，编写计算方法响应的btrace脚本123456789101112131415161718192021import com.sun.btrace.annotations.*;import static com.sun.btrace.BTraceUtils.*;@BTracepublic class MethodTimeCost&#123; @TLS private static long starttime; @OnMethod(clazz="/com\\.http\\.testbtrace\\..*/",method="/.+/",location=@Location(Kind.ENTRY)) public static void startExecute()&#123; starttime=timeMillis(); &#125; @OnMethod(clazz="/com\\.http\\.testbtrace\\..*/",method="/.+/",location=@Location(Kind.RETURN)) public static void endExecute()&#123; long timecost=timeMillis()-starttime; if(timecost&gt;50)&#123; print(strcat(strcat(name(probeClass()),"."),probeMethod())); print(" ["); print(strcat("Time taken: ",str(timecost))); println("] "); &#125; &#125;&#125; 启动需要跟踪的java程序，然后执行jps获取该进程的id，最后执行这段btrace脚本。如id为3683。1btrace -cp build 3683 MethodTimeCost.java 当然btrace的使用并不局限于此，它的功能十分强大，特别是在Java应用在线故障排查方面，是不可或缺的利器。 4、GC日志分析GC日志能够反映出Java应用执行内存回收详细情况，如Minor GC,Full GC的频繁程度、GC所导致应用停止响应的时间，引起GC的原因等。根据程序吞吐量优先还是响应时间优先的不同，sun HotSpot虚拟机1.6版在服务器端提供Parallel Scavenge/Parallel Old与ParNew/CMS两种垃圾收集器组合，其中Parallel Scavenge和ParNew为新生代的垃圾收集器，而Parallel Old和CMS为老年带的垃圾收集器。在JVM启动时加上下面几个参数：-verbose:gc -Xloggc:/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps其他GC日志相关参数有-XX:+PrintGC 输出GC日志 -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息日志示例：1234560.756: [Full GC (System) 0.756: [CMS: 0K-&gt;1696K(204800K), 0.0347096 secs] 11488K-&gt;1696K(252608K), [CMS Perm : 10328K-&gt;10320K(131072K)], 0.0347949 secs] [Times: user=0.06 sys=0.00, real=0.05 secs] 1.728: [GC 1.728: [ParNew: 38272K-&gt;2323K(47808K), 0.0092276 secs] 39968K-&gt;4019K(252608K), 0.0093169 secs] [Times: user=0.01 sys=0.00, real=0.00 secs] 2.642: [GC 2.643: [ParNew: 40595K-&gt;3685K(47808K), 0.0075343 secs] 42291K-&gt;5381K(252608K), 0.0075972 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 4.349: [GC 4.349: [ParNew: 41957K-&gt;5024K(47808K), 0.0106558 secs] 43653K-&gt;6720K(252608K), 0.0107390 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 5.617: [GC 5.617: [ParNew: 43296K-&gt;7006K(47808K), 0.0136826 secs] 44992K-&gt;8702K(252608K), 0.0137904 secs] [Times: user=0.03 sys=0.00, real=0.02 secs] 7.429: [GC 7.429: [ParNew: 45278K-&gt;6723K(47808K), 0.0251993 secs] 46974K-&gt;10551K(252608K), 0.0252421 secs] 我们取倒数第二条记录分析一下各个字段都代表了什么含义15.617（时间戳）: [ GC（Young GC） 5.617（时间戳）: [ParNew（使用ParNew作为年轻代的垃圾回收期）: 43296K（年轻代垃圾回收前的大小）- &gt;7006K（年轻代垃圾回收以后的大小）(47808K)（年轻代的总大小）, 0.0136826 secs（回收时间）] 44992K（堆区垃圾回收前的大小）-&gt;8702K（堆区垃圾回收后的大小）(252608K)（堆区总大小）, 0.0137904 secs（回收时间）] [ Times: user=0.03（Young GC用户耗时） sys=0.00（Young GC系统耗时）, real=0.02 secs（Young GC实际耗时）] 我们再对数据做一个简单的分析从最后一条GC记录中我们可以看到 Young GC回收了 45278-6723=38555K的内存Heap区通过这次回收总共减少了 46974-10551=36423K的内存。38555-36423=2132K说明通过该次Young GC有2132K的内存被移动到了Old Gen， 我们来验证一下在最后一次Young GC的回收以前 Old Gen的大小为8702-7006=1696回收以后Old Gen的内存使用为10551-6723=3828Old Gen在该次Young GC以后内存增加了3828-1696=2132K 与预计的相符 CMS(Concurrent Mark-Sweep)是以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。在启动JVM参数加上-XX:+UseConcMarkSweepGC (默认HotSpot JVM使用的是并行收集器)，这个参数表示对于老年代的回收采用CMS。CMS采用的基础算法是：标记—清除。整个过程大致分为4步： 初始标记 (CMS initial mark):会STW(Stop The World),为了收集应用程序的对象引用需要暂停应用程序线程,该阶段完成后，应用程序线程再次启动 并发标记 (CMS concurrent mark):从第一阶段收集到的对象引用开始，遍历所有其他的对象引用 重新标记 (CMS remark) :会STW,由于对象引用可能会发生进一步改变，因此应用程序线程会再一次被暂停以更新这些变化,并且在进行实际的清理之前确保一个正确的对象引用视图 并发清理 (CMS concurrent sweep) :所有不再被引用的对象将从堆里清除掉整个过程中，1、3会stw，但是2、4是最耗时的。所以可以减少stw的时间。 注意：一次CMS至少会给Full GC的次数 + 2，因为Full GC的次数是按照老年代GC时stop the world的次数而定的。一般CMS引起的GC时间会很短如ms级，如果达到秒级，那么就需要注意了，很可能是CMS发生了concurrent mode fail之后会退化成Serial Old收集器，它是单线程的标记-压缩收集器，所以耗时会非常的长。查看日志时需要注意是否存在concurrent mode fail 123456789101112132014-12-08T17:24:18.514+0800: 77443.326: [GC [1 CMS-initial-mark: 1382782K(1843200K)] 1978934K(4710400K), 0.0702700 secs] [Times: user=0.07 sys=0.00, real=0.07 secs]2014-12-08T17:24:18.586+0800: 77443.398: [CMS-concurrent-mark-start] 2014-12-08T17:24:19.890+0800: 77444.702: [CMS-concurrent-mark: 1.206/1.303 secs] [Times: user=2.80 sys=0.07, real=1.30 secs] 2014-12-08T17:24:19.890+0800: 77444.702: [CMS-concurrent-preclean-start] 2014-12-08T17:24:19.906+0800: 77444.718: [CMS-concurrent-preclean: 0.015/0.015 secs] [Times: user=0.02 sys=0.00, real=0.02 secs] 2014-12-08T17:24:19.906+0800: 77444.718: [CMS-concurrent-abortable-preclean-start] CMS: abort preclean due to time 2014-12-08T17:24:25.181+0800: 77449.993: [CMS-concurrent-abortable-preclean: 5.241/5.275 secs] [Times: user=6.03 sys=0.09, real=5.27 secs] 2014-12-08T17:24:25.187+0800: 77449.999: [GC[YG occupancy: 749244 K (2867200 K)]77450.000: [Rescan (parallel) , 0.0276780 secs]77450.028: [weak refs processing, 0.2029030 secs] [1 CMS-remark: 1382782K(1843200K)] 2132027K(4710400K), 0.2340660 secs] [Times: user=0.43 sys=0.00, real=0.23 secs2014-12-08T17:24:25.424+0800: 77450.236: [CMS-concurrent-sweep-start] 2014-12-08T17:24:27.420+0800: 77452.232: [CMS-concurrent-sweep: 1.918/1.996 secs] [Times: user=2.61 sys=0.05, real=2.00 secs] 2014-12-08T17:24:27.421+0800: 77452.233: [CMS-concurrent-reset-start] 2014-12-08T17:24:27.430+0800: 77452.242: [CMS-concurrent-reset: 0.010/0.010 secs] [Times: user=0.01 sys=0.00, real=0.01 secs] 我们可以看到Full GC的次数应该是2,(因为Full GC的次数是按照老年代GC时stop the world的次数而定的):0.07 secs(initial mark),0.23 secs(remark).可以用jstat -gc得到的时间Full GC的次数和时间。 最后再次强调一下: Full GC == Major GC指的是对老年代/永久代的stop the world的GC Full GC的次数 = 老年代GC时 stop the world的次数 Full GC的时间 = 老年代GC时 stop the world的总时间 CMS 不等于Full GC，我们可以看到CMS分为多个阶段，只有stop the world的阶段被计算到了Full GC的次数和时间，而和业务线程并发的GC的次数和时间则不被认为是Full GC Full GC本身不会先进行Minor GC，我们可以配置，让Full GC之前先进行一次Minor GC，因为老年代很多对象都会引用到新生代的对象，先进行一次Minor GC可以提高老年代GC的速度。比如老年代使用CMS时，设置CMSScavengeBeforeRemark优化，让CMS remark之前先进行一次Minor GC。 最后要重点注意的是：jstat命令的man-pages说FGC代表的是Full GC事件，而我们通常认为那就是是Full GC的次数。CMS GC参考:https://kamilszymanski.github.io/interpreting-jstats-number-of-full-gc-events/http://blog.csdn.net/iter_zc/article/details/41825395http://www.iteye.com/topic/1119491 CMS缺点:CMS回收器采用的基础算法是Mark-Sweep。所有CMS不会整理、压缩堆空间。这样就会有一个问题：经过CMS收集的堆会产生空间碎片。CMS不对堆空间整理压缩节约了垃圾回收的停顿时间，但也带来的堆空间的浪费。为了解决堆空间浪费问题，CMS回收器不再采用简单的指针指向一块可用堆空间来为下次对象分配使用。而是把一些未分配的空间汇总成一个列表，当JVM分配对象空间的时候，会搜索这个列表找到足够大的空间来hold住这个对象。 需要更多的CPU资源。从上面的图可以看到，为了让应用程序不停顿，CMS线程和应用程序线程并发执行，这样就需要有更多的CPU，单纯靠线程切 换是不靠谱的。并且，重新标记阶段，为空保证STW快速完成，也要用到更多的甚至所有的CPU资源。当然，多核多CPU也是未来的趋势！ CMS的另一个缺点是它需要更大的堆空间。因为CMS标记阶段应用程序的线程还是在执行的，那么就会有堆空间继续分配的情况，为了保证在CMS回 收完堆之前还有空间分配给正在运行的应用程序，必须预留一部分空间。也就是说，CMS不会在老年代满的时候才开始收集。相反，它会尝试更早的开始收集，已 避免上面提到的情况：在回收完成之前，堆没有足够空间分配！默认当老年代使用68%的时候，CMS就开始行动了。 – XX:CMSInitiatingOccupancyFraction =n 来设置这个阀值。但是如果在CMS运行期间，预留的内存无法满足程序需要时，则会出现concurrent mode fail之后会退化成Serial Old收集器，它是单线程的标记-压缩收集器，所以耗时会非常的长.总得来说，CMS回收器减少了回收的停顿时间，但是降低了堆空间的利用率。 啥时候用CMS:如果你的应用程序对停顿比较敏感，并且在应用程序运行的时候可以提供更大的内存和更多的CPU(也就是硬件牛逼)，那么使用CMS来收集会给你带来好处。还有，如果在JVM中，有相对较多存活时间较长的对象(老年代比较大)会更适合使用CMS。 问题：minor GC是否也会导致STW呢? https://www.zhihu.com/question/29114369?sort=createdhttps://blogs.oracle.com/jonthecollector/our-collectorshttps://www.zhihu.com/question/21535747/answer/144884632目前所有的新生代gc都是需要STW的，STW总会发生 不管是新生代还是老年代 就算是CMS也有STW的时候。重点是 时间长短Serial：单线程STW，复制算法ParNew：多线程并行STW，复制算法Parallel Scavange：多线程并行STW，吞吐量优先，复制算法G1：多线程并发，可以精确控制STW时间，整理算法因为full gc耗时远高于minor gc，所以通常忽略minor gc几十毫秒的停顿。 GC收集器分类与常见组合：按线程：单线程：Serial、SerialOld多线程：ParNew、Parallel Scavenge、Parallel Old、CMS、G1按适用代：新生代: Serial、ParNew、Parallel Scavenge老年代: SerialOld、CMS 、Parallel OldG1可以在新生代和老年代使用常见的组合ParNew+CMS ； Parallel Scavenge+Parallel Old HotSpot JVM支持哪些垃圾收集器？Hotspot JVM实现包括了Serial GC, Parallel GC, CMS, G1 GC 4套算法组合。下面来讲一讲这些算法组合分别包括了哪些算法。 Serial GC算法：Serial Young GC ＋ Serial Old GC （实际上它是全局范围的Full GC），适用于小程序或低配置计算机系统； Parallel GC算法：（并行的）Parallel Young GC ＋ PS MarkSweep GC / （并行的）Parallel Old GC（全局范围的Full GC），选PS MarkSweep GC 还是 Parallel Old GC 由参数UseParallelOldGC来控制，适用于对吞吐量敏感的应用； CMS算法：（并行的）ParNew（Young）GC + （并发的）CMS（Old）GC （piggyback on ParNew的结果／老生代存活下来的object只做记录，不做compaction）＋ Full GC for CMS算法（应对核心的CMS GC某些时候的不赶趟，开销很大），适用于对延时敏感的应用； G1 GC：（并行的）Young GC ＋（并行的）mixed GC（新生代，再加上部分老生代）＋ Full GC for G1 GC算法（应对G1 GC算法某些时候的不赶趟，开销很大）。G1 GC中开销较大的object marking算法部分是跟applicaiton一起并发的，其开始到结束时间上甚至可以跨越好几次Young GC。适用于延时和吞吐量都有要求的应用，调教相对前述3中GC算法组合为烦。 上述组合描述已特别指出并行（parallel）还是并发（concurrent）。Hotspot JVM语境下，这两个概念是严格区分的。并行是指STW（stop-the-world）状态下的GC算法或部分算法的多线程运行；并发是指非STW状态下GC算法或部分算法跟applicaiton一起分享多个线程来运行。关于G1，可参考:http://www.importnew.com/15311.html http://blog.csdn.net/qq_34280276/article/details/52863551 https://blogs.oracle.com/jonthecollector/our-collectors http://ivywang.iteye.com/blog/2146645 5、数据库查询许多请求响应速度慢的原因，最终都是由于糟糕的数据库查询语句所导致的。如何定位到这些糟糕的查询语句呢？MySQL提供慢查询日志的功能。能够记录下响应时间超过一定阀值(默认10秒)的SQL查询.查看是否启用慢日志: show variables like ‘log_slow_queries’; 查看慢于多少秒的SQL会记录到慢日志中:show variables like ‘long_query_time’;通过配置my.cnf，可以修改满日志的相关配置: 123456[mysqld]port= 3306slow-query-log=ON # 慢查询：确认开启slow-query-log-file=&quot;/var/log/mysql/mysql-slow.log&quot; # 慢查询：日志文件及路径long_query_time = 1 # 慢查询：指定超过1s仍未完成的语句，为执行过慢的语句.默认是10s 测试： 123451.执行一条慢查询SQL语句mysql&gt; select sleep(2);2.查看是否生成慢查询日志ls /usr/local/mysql/data/slow.log如果日志存在，MySQL开启慢查询设置成功！ 6、系统资源的使用：查看CPU当前的利用率和系统的load，查看网卡的流量，查看磁盘IO的密集程度，查看内存的使用等。通过硬件指标来判断资源是否已经达到瓶颈。通过这些指标可以将应用分为CPU密集型、网络密集型、磁盘IO密集型、内存使用密集型等。根据应用的特征来进行机器配置的选型，以便使资源的利用达到最大化。 性能测试工具性能测试是指通过一些自动化的测试工具模拟多种正常、峰值，以及异常负载的条件来对系统的各项性能指标进行测试。在系统上线之前，需要经过一系列的性能测试，以确定系统在各种负载下的性能指标变化，发现系统潜在的一些瓶颈和问题。 1、ab-apache bench，内置于apache http server中。是一款专门用来对HTTP服务器进行性能测试的工具，可以模拟多个并发请求来对服务器进行压力测试，得出服务器在高负载下能够支持的qps及应用的响应时间。 2、Apache JMeter ：功能比ab更强大。在执行性能测试的同时，可以通过一些工具，如jsconsole,VisualVM,来远程实时查看测试机的负载，内存使用，GC等情况。以Tomcat为例，配置JAVA_OPTS： 12CATALINA_OPTS=&quot;$CATALINA_OPTS -Djava.rmi.server.hostname=172.16.18.155 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=18081 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false&quot; 3、HP LoadRunner -商业性能测试工具 4、反向代理引流在分布式环境下，流量真正到达服务器之前，一般会经过负载均衡设备进行转发，通过改变负载均衡的策略，可以改变后端服务器所承受的压力。在新版本发布之前，可以先对少部分机器进行灰度发布，以验证程序的正确性和稳定性，并且通过修改负载均衡策略，可以改变机器所承受的负载，达到对在线机器进行性能基准测试的目的。 5、TCPCopyTCPCopy是网易开源的，它是一款请求复制工具，能够将在线请求复制到测试机器，模拟真实环境，达到程序在不上线的情况下承担线上真是流量的效果。TCPCopy分为client与server，client运行在真实环境的线上服务器之上，用来捕获在线请求数据包，而server运行在测试机上，用来截获响应包，并将响应包的头部信息传递给client，已完成TCP交互。 性能优化措施通过上述方法找到性能瓶颈点之后，就需要对找到的性能瓶颈点进行优化。可以从多个方面入手，如:前端的资源文件，后端的Java程序，数据传输，结果缓存，数据库，JVM的GC，服务器硬件等。1、前端性能优化指标：页面造成的HTTP请求数量，是否使用CDN网络，是否使用压缩。2、Java程序优化单例模式，Future模式，线程池，服务端网络IO模型，减少线程上下文切换，降低锁竞争(使用原子变量，减小锁范围，将独占锁改为读写锁等)，压缩，结果缓存，数据库查询性能优化(合理使用索引，explain分析SQL，反范式设计如冗余存储以减少表关联带来的随机io和全表扫描，MySQL使用查询缓存：查看是否开启缓存:select @@query_cache_type;查看缓存总大小：select @@query_cache_size;查看记录集缓存限制:select @@query_cache_limit; 使用搜索引擎：在分库分表后，无法进行复杂的条件查询，这个时候就需要搜索引擎了。 使用key-value数据库:对于保有海量数据的互联网企业来说，多表的关联查询是非常忌讳的。SQL的功能被很大程度地弱化了。为了达到更大的并发，可以采用NoSQL数据库。 GC优化: JVM在进行垃圾回收时，会导致所有的工作线程暂停(stop the world),GC已成为影响Java应用性能的一个重要因素。查看GC日志中的MinorGC、Full GC的频率，GC导致的停顿时间及GC发生的原因等。需要注意一点的是:如果GC在PermGen上操作，而通常永久代存放的是已被虚拟机加载的类信息，及常量、静态变量、即使编辑器编译后的代码等数据，启动后一般非常稳定，GC回收的内存也十分有限。如果是因为PermGen空间不够而频繁发生FullGC，可能的情况是1，PermGen确实设置得过小，调整-XX:PermSize和-XX:MaxPermSize，2:可能是由于错误的代码导致频繁类加载，需要使用jmap将堆dump下来进行分析。 硬件提升性能:缓存服务器加大内存，磁盘IO密集的选用SSD，CPU密集的增加CPU核数，负载均衡节点注意网卡和带宽。 Java应用故障排查常用工具在进行故障定位时，知识和经验是发现问题的基础，数据是依据，而工具则是运用知识的手段。知识和经验告诉我们怎么去做，而运用工具能帮助我们更加快速地发现和定位问题。1、jps 输出java进程id，并显示其主类。选项： -q 只输出进程id -m 输出传递给main函数的参数 -l 输出主类全名，如果是jar，则输出jar文件路径 -v 输出虚拟机进程启动时所带的JVM参数 2、jstat可参考http://blog.csdn.net/zhaozheng7758/article/details/8623549用来对虚拟机各种运行状态进行监控，如查看类加载与卸载情况，管理内存使用和垃圾收集等信息，监视JIT的运行情况等。几乎包括了JVM运行的方方面面。在无法使用图形化工具如jconsole,VisualVM时，jstat成为了运行期定位问题的首选。jstat -options 可列出所有选项常见的有 -class (查看类加载器的统计信息) -compiler (JIT信息) -gc (查看JVM中垃圾收集情况的统计信息，包括Eden区，2个survivor区，老年代，永久代的容量和已用空间，GC时间等) -gccapacity (各区大小) -gccause (最近一次GC统计和原因) -gcnew (新区统计) -gcnewcapacity (新区大小) -gcold (老区统计) -gcoldcapacity (老区大小) -gcpermcapacity (永久区大小) -gcutil (GC统计汇总) -printcompilation (HotSpot编译统计) 1234567891011jstat -gcutil &lt;pid&gt;:统计gc信息显示列名具体描述S0 年轻代中第一个survivor（幸存区）已使用的占当前容量百分比S1 年轻代中第二个survivor（幸存区）已使用的占当前容量百分比E 年轻代中Eden（伊甸园）已使用的占当前容量百分比O old代已使用的占当前容量百分比P perm代已使用的占当前容量百分比YGC 从应用程序启动到采样时年轻代中gc次数YGCT 从应用程序启动到采样时年轻代中gc所用时间(s)FGC 从应用程序启动到采样时old代(全gc)gc次数FGCT 从应用程序启动到采样时old代(全gc)gc所用时间(s)GCT 从应用程序启动到采样时gc用的总时间(s) 3、jinfo 查看应用程度的配置参数，及打印运行JVM时所制定的JVM参数。比jsp -v能查看未被显式指定的JVM参数的系统默认值。-sysprops选项将虚拟机进程中所指定的System.getProperties()的内容打印出来-flags：查看vm参数 ,如果不指定，则同时包含-sysprops和flags的输出。jinfo还能够在运行期间修改JVM参数，通过使用-flag name=value或者-flag [+|-]name来修改。 4、jstack用来生成虚拟机当前的线程快照信息，线程快照就是当前虚拟机每一个线程正在执行的方法堆栈的集合。主要是为了定位线程长时间没有响应的原因，如线程死锁、网络请求没有设置超时时间而长时间没有返回、死循环、信号量没有释放等，都有可能导致线程长时间停顿。 -F当jstack [-l] pid’没有相应的时候强制打印栈信息 -l长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表. -m打印java和native c/c++框架的所有栈信息. pid 需要被打印配置信息的java进程id,可以用jps查询. 5、jmap可以输出所有内存中对象的工具。可以用来查看等待回收对象的队列，查看堆的概要信息(包括采用的是哪种GC收集器，堆使用情况，及通过JVM参数指定的各个内存空间的大小等)，甚至可以将VM 中的heap以二进制dump输出成文本，之后便可以通过图形化工具如MAT进行堆分析，内存中有哪些对象，分别占用的空间，以便找到诸如内存泄漏等问题的祸根。需要注意的是，jmap执行堆dump操作时，由于生成的转储文件较大，将耗费大量的系统资源。因此，应避免在系统高位运行时执行该指令，否则有可能造成短时间内系统无法响应的情况。 -heap 打印heap的概要信息，包括使用的回收器类型、堆的配置信息、各内存分代的空间使用情况 -dump:[live,]format=b,file= 使用hprof二进制形式,输出jvm的heap内容到文件=. live子选项是可选的，假如指定live选项,那么只输出活的对象到文件. -finalizerinfo 打印正等候回收的对象的信息. -histo[:live] 打印每个class的实例数目,内存占用,类全名信息. VM的内部类名字开头会加上前缀”*”. 如果live子参数加上后,只统计活的对象数量. -permstat 打印classload和jvm heap永久层的信息. 包含每个classloader的名字,活泼性,地址,父classloader和加载的class数量. 另外,内部String的数量和占用内存数也会打印出来. -F 当JVM对-dump操作没有响应时，强制生成转储快照。 -J 传递参数给jmap启动的jvm. 6、BTrace可参考：http://mgoann.iteye.com/blog/1409667 http://www.jianshu.com/p/93e94b724476BTrace是一个开源的Java程序动态跟踪工具，前面已介绍过如何使用它来监控方法的执行时间。它的基本工作原理是通过Hotsopt虚拟机的HotSwap技术将跟踪的代码动态替换到被跟踪的Java程序内，以观察程序运行的细节(BTrace 主要使用了 Instrumentation + ASM技术来实现对正在运行进程的探测。)。如打印方法的参数，变量的值及返回值等。通过使用BTrace，可以在不修改代码、不重启应用的情况下，动态地查看程序运行的细节，方便地对程序进行调试。 123456789101112131415161718192021222324原方法：priavte int sub(int a,int b)&#123; return a+b;&#125;import com.sun.btrace.annotations.*;import static com.sun.btrace.BTraceUtils.*;btrace脚本：@BTracepublic class MethodTimeCost&#123; @TLS private static long starttime; @OnMethod(clazz="net.xby1993.test.Main", method="sub",location=@Location(Kind.ENTRY)) public static void startExecute()&#123; starttime=timeMillis(); &#125; @OnMethod(clazz="net.xby1993.test.Main", method="sub",location=@Location(Kind.RETURN)) public static void endExecute(@Return int rtn,int a,int b)&#123; long timecost=timeMillis()-starttime; println(strcat("a:",str(a))); println(strcat("b:",str(b))); println(strcat("return:",str(rtn))); println(strcat("costtime:",str(timecost))); &#125;&#125; 12btrace &lt; pid &gt; &lt; btrace-script &gt;btrace 3050 MethodTimeCost.java 值得注意的是，@TLS声明的变量是 ThreadLocal的， 每个线程都会有一份这个自己的startTime 变量。 btrace还提供了一个vaisualvm上的一个插件，可以执行btrace脚本。尝试了下，可以attach到本机的jvm进程上，但是远程主机的JVM进程不行。有的说通过端口转发绑定的方式可以，但是还是没有试出来。运行jvisualvm.exe, 选择工具-&gt;插件-&gt;可用插件 选择 BTrace Workbench进行在线安装。 选择需要监控的进程,右击 trace application,在btrace的工作台中直接编写脚本并执行,执行后，当被监控的程序运行了这些检查点的方法时，btrace会在控制台对执行时间进行输出。 注解说明： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061@BTrace声明这个类是个BTrace脚本.unsafe参数表示是否不安全的模式执行.方法annotation@OnMethod: 声明 探查点（probe point）clazz: 全路径类名，支持正则表达式，格式为/正则表达式/+类名 匹配子类@前缀 匹配anotation声明的类method: 方法名，支持正则表达式，格式为/正则表达式/， anotation使用@location: 用@Location来表明在什么时候时候去执行脚本Kind.ENTRY 进入方法时Kind.RETURN 方法返回时Kind.THROW 抛出异常时Kind.ARRAY_SET 设置数组元素时Kind.ARRAY_GET 获取数组元素时Kind.NEWARRAY 创建新数组时Kind.NEW 创建新对象时Kind.CALL 调用方法时Kind.CATCH 捕获异常时Kind.FIELD_SET 获取对象属性时Kind.FIELD_SET 设置对象属性时Kind.ERROR 方法由于发生未被捕获的异常结束时Kind.SYNC_ENTRY 进入同步块时Kind.SYNC_EXIT 离开同步块时type 方法类型， 不含方法名、参数民、异常声明。@OnTimer 定时器，间隔出发动作。参数： 间隔时间，单位毫秒@OnError BTrace代码发生异常时回调@OnExit BTrace代码调用exit来结束探测时回调该注释的方法@OnEvent 接受客户端事件时会回调。目前，当客户端命令上执行Ctrl-C (SIGINT)时会发送一个时间到服务器端，从而触发@OnEvent注释的方法。@OnLowMemory 内存低于设置的阈值时回调方法pool 内存池名称threshold 阈值大小@OnProbe 支持使用xml格式来声明探测点点和探测动作。未声明注解的方法参数未声明注解的方法参数的映射，根据探测点类型locaiton的不同而不同：Kind.ENTRY 方法参数Kind.RETURN 方法返回值Kind.THROW 被抛出的异常Kind.ARRAY_SET 数值下标Kind.ARRAY_GET 数组下标Kind.CATCH 被捕获的异常Kind.FIELD_SET 被设置属性的值Kind.NEW 创建的对象的类型Kind.ERROR 被抛出的异常字段注解Export 将字段保罗给jstat访问Property 将字段暴露注册为MBean 属性，可以通过JMX进行查看TLS 将字段声明为TheadLocal字段，每个线程拥有自己独立的字段参数annotation介绍@Self 声明探测的当前对象this@Return 方法返回对象@ProbeClassName 当前探测点所在的类名@ProbeMethodName 当前探测点所在的方法名fqn 是否获取全路径方法名称fully qualified name (FQN)@Duration 执行时间，单位纳秒，一般同 Kind.RETURN 和 Kind.ERROR 配合使用@TargetInstance 配合 Kind.CALL使用，声明了被调用方法所在的对象@TargetMethodOrField Kind.CALL使用，声明了被调用方法所在的对象的方法 7、JConsole可参考 http://jiajun.iteye.com/blog/810150不过目前推荐使用JVisualVM来替代JConsole 8、JVisualVM可参考：https://www.ibm.com/developerworks/cn/java/j-lo-visualvm/ http://blog.csdn.net/a19881029/article/details/8432368连接：1、本地机器的程序直接可以监听到2、远程机器的程序需要加上JVM参数1234-Dcom.sun.management.jmxremote= true-Dcom.sun.management.jmxremote.port= 9090-Dcom.sun.management.jmxremote.ssl= false-Dcom.sun.management.jmxremote.authenticate= false VisualVM是一款”All-in-One”工具，涵盖了JVM内存监视，性能分析，线程，及堆转储分析、垃圾回收监视等几乎所有功能。常用功能:内存监控，GC监控，应用程序分析，线程分析，堆dump分析，CPU及内存抽样、BTrace跟踪等。 Java VisualVM 插件地址：打开Java VisualVM检查更新插件时，默认的连接连不上，通过浏览器访问之后发现默认的服务器已经404，新地址已经迁移到github，下面这个地址里面有不同版本jdk对应的插件中心地址：https://visualvm.github.io/pluginscenters.html 9、MAT(Memory Analyzer Tool)可参考：http://flychao88.iteye.com/blog/2192266 https://www.ibm.com/developerworks/cn/opensource/os-cn-ecl-ma/一个基于Eclipse的内存分析工具，是一个快速、功能丰富的JAVA heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗。使用内存分析工具从众多的对象中进行分析，快速的计算出在内存中对象的占用大小，看看是谁阻止了垃圾收集器的回收工作，并可以通过报表直观的查看到可能造成这种结果的对象。巧妇难为无米之炊，我们首先需要获得一个堆转储文件。只要你设置了如下所示的 JVM 参数：-XX:+HeapDumpOnOutOfMemoryError JVM 就会在发生内存泄露时抓拍下当时的内存状态，也就是我们想要的堆转储文件。除此之外，还有很多的工具，例如 JMap，JConsole 都可以帮助我们得到一个堆转储文件。 典型案例分析1、内存溢出OutOfMemory测试:1234567891011public class TestOOM&#123; static class Obj&#123; public byte[] bytes="hello world".getBytes(); &#125; public static void main(String[] args)&#123; ArrayList&lt;Obj&gt; list=new ArrayList&lt;&gt;(); while(true)&#123; list.add(new Obj()); &#125; &#125;&#125; 为了尽快出现问题，这里限制堆内存的大小，并在发生OOM时，dump堆。使用的JVM参数如下:-Xms10m -Xmx10m -Xmn5m -XX:+HeapDumpOnOutOfMemoryError 2、线程死锁或信号量没有释放当线程因为资源争用而发生死锁，或者因为使用了信号量而没有及时释放，在测试环境下很难发现该问题，特别是没有进行压力测试就上线的情况下，即便是上线，应用访问量不高，短时间内可能故障也不会发作。这些会导致线程资源耗光(如果采用线程池，此时并不会出现OOM异常，而是表现为请求长时间没有响应，应用僵死)。但是对应的JVM进程却是活跃的，并且此时的系统资源消耗，如CPU的load往往非常低。 这时需要线程dump进行分析。 3、类加载冲突有时候，当使用相同代码的应用发布上线以后，在分布式环境下，会发现一部分机器运行正常，而另一部分机器则会抛出NoClassDefFoundError、NoSuchMethodError这样的异常，这是为何？在一个大型的企业级应用中，可能会依赖很多jar包，而这些jar可能又会依赖其他的jar，最终会导致依赖关系变得错综复杂。有时候，可能会出现依赖一个jar的多个版本，更有甚者，某些jar会直接将依赖的jar也打包进去，这样就使得很多class签名相同的类同时存在。在不同的机器上，对不同jar中同名类的加载有时候并不完全一致。所以才会导致这些问题。那为什么测试时没有问题，因为由于在相同机器上，无论启动多少次，类得加载顺序基本不变。通过在JVM启动时加上-verbose:class,可以查到具体的class究竟是从哪个jar文件中加载进来的。]]></content>
      <categories>
        <category>读书笔记</category>
        <category>大型网站分布式架构与设计实践</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>msa</tag>
        <tag>架构</tag>
        <tag>soa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[读书笔记]chapter3-互联网安全架构-大型网站分布式架构与设计实践]]></title>
    <url>%2F2017%2F12%2F17%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chapter3-%E4%BA%92%E8%81%94%E7%BD%91%E5%AE%89%E5%85%A8%E6%9E%B6%E6%9E%84-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%AE%BE%E8%AE%A1%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[内容：常见的Web攻击手段和防御方法:如XSS,CSRF,SQL注入等安全算法:如数字摘要，对称加密，非对称加密，数字签名，数字证书等如何采用摘要认证防止信息篡改，通过数字签名来验证通信双方的合法性，及通过Https保证通信过程中数据不被第三方监听和截获。OAuth协议介绍 XSS攻击XSS(Cross-Site Scripting),跨站脚本攻击。防范:Html实体字符转义escapeXml CSRF攻击CSRF(cross site request forgery):跨站请求伪造。举例：CSRF攻击的主要目的是让用户在不知情的情况下攻击自己已登录的一个系统，类似于钓鱼。如用户当前已经登录了邮箱，或bbs，同时用户又在使用另外一个，已经被你控制的站点，我们姑且叫它钓鱼网站。这个网站上面可能因为某个图片吸引你，你去点击一下，此时可能就会触发一个js的点击事件，构造一个bbs发帖的请求，去往你的bbs发帖，由于当前你的浏览器状态已经是登陆状态，所以session登陆cookie信息都会跟正常的请求一样，纯天然的利用当前的登陆状态，让用户在不知情的情况下，帮你发帖或干其他事情。 CSRF防御： 将cookie设置为HttpOnly,这样通过脚本就无法读取到cookie信息。 通过 referer、token 或者 验证码 来检测用户提交。而该token不存在于cookie，而是随着每次响应返回到页面中的，服务端token存在于session中。 尽量不要在页面的链接中暴露用户隐私信息。 对于用户修改删除等操作最好都使用post 操作 。 避免全站通用的cookie，严格设置cookie的域。 SQL注入攻击防范:使用预编译语句；避免密码明文存放；处理好相应异常。 文件上传漏洞防范：对上传的文件进行白名单校验，限制上传文件大小，上传后的文件进行重命名。使用文件的魔数(magic number)来判断文件类型，而不是通过后缀名；对图片文件，可考虑使用imagemagick先缩放再保存。 DDoS攻击DDoS(Distributed Denial of Service)即分布式拒绝服务攻击。http://netsecurity.51cto.com/art/200903/114969.htm先说Dos，DoS攻击是最早出现的,它的攻击方法说白了就是单挑,是比谁的机器性能好、速度快。但是现在的科技飞速发展,一般的网站主机都有十几台主机,而且各个主机的处理能力、内存大小和网络速度都有飞速的发展,有的网络带宽甚至超过了千兆级别。这样我们的一对一单挑式攻击就没有什么作用了,搞不好自己的机子就会死掉。不过,科技在发展,黑客的技术也在发展。DDoS攻击。它的原理说白了就是群殴,用好多的机器对目标机器一起发动DoS攻击,但这不是很多黑客一起参与的,这种攻击只是由一名黑客来操作的。这名黑客不是拥有很多机器,他是通过他的机器在网络上占领很多的“肉鸡”,并且控制这些“肉鸡”来发动DDoS攻击。 DDoS攻击方式： 1、SYN Flood攻击SYN-Flood攻击是当前网络上最为常见的DDoS攻击，也是最为经典的拒绝服务攻击，它利用了TCP协议实现上的一个缺陷，通过向网络服务所在端口发送大量的伪造源地址的攻击报文，就可能造成目标服务器中的半开连接队列被占满，从而阻止其他合法用户进行访问。这种攻击早在1996年就被发现，但至今仍然显示出强大的生命力。很多操作系统，甚至防火墙、路由器都无法有效地防御这种攻击，而且由于它可以方便地伪造源地址，追查起来非常困难。它的数据包特征通常是，源发送了大量的SYN包，并且缺少三次握手的最后一步握手ACK回复。原理例如，攻击者首先伪造地址对服务器发起SYN请求（我可以建立连接吗？），服务器就会回应一个ACK+SYN（可以+请确认）。而真实的IP会认为，我没有发送请求，不作回应。服务器没有收到回应，会重试3-5次并且等待一个SYN Time（一般30秒-2分钟）后，丢弃这个连接。如果攻击者大量发送这种伪造源地址的SYN请求，服务器端将会消耗非常多的资源来处理这种半连接，保存遍历会消耗非常多的CPU时间和内存，何况还要不断对这个列表中的IP进行SYN+ACK的重试。最后的结果是服务器无暇理睬正常的连接请求—拒绝服务。在服务器上用netstat –an命令查看SYN_RECV状态的话 2、ACK Flood攻击ACK Flood攻击是在TCP连接建立之后，所有的数据传输TCP报文都是带有ACK标志位的，主机在接收到一个带有ACK标志位的数据包的时候，需要检查该数据包所表示的连接四元组是否存在，如果存在则检查该数据包所表示的状态是否合法，然后再向应用层传递该数据包。如果在检查中发现该数据包不合法，例如该数据包所指向的目的端口在本机并未开放，则主机操作系统协议栈会回应RST包告诉对方此端口不存在。这里，服务器要做两个动作：查表、回应ACK/RST。这种攻击方式显然没有SYN Flood给服务器带来的冲击大，因此攻击者一定要用大流量ACK小包冲击才会对服务器造成影响。按照我们对TCP协议的理解，随机源IP的ACK小包应该会被Server很快丢弃，因为在服务器的TCP堆栈中没有这些ACK包的状态信息。但是实际上通过测试，发现有一些TCP服务会对ACK Flood比较敏感，比如说JSP Server，在数量并不多的ACK小包的打击下，JSP Server就很难处理正常的连接请求。对于Apache或者IIS来说，10kpps的ACK Flood不构成危胁，但是更高数量的ACK Flood会造成服务器网卡中断频率过高，负载过重而停止响应。可以肯定的是，ACK Flood不但可以危害路由器等网络设备，而且对服务器上的应用有不小的影响。 3、UDP DNS Query Flood攻击原理UDP DNS Query Flood攻击实质上是UDP Flood的一种，但是由于DNS服务器的不可替代的关键作用，一旦服务器瘫痪，影响一般都很大。UDP DNS Query Flood攻击采用的方法是向被攻击的服务器发送大量的域名解析请求，通常请求解析的域名是随机生成或者是网络世界上根本不存在的域名，被攻击的DNS 服务器在接收到域名解析请求的时候首先会在服务器上查找是否有对应的缓存，如果查找不到并且该域名无法直接由服务器解析的时候，DNS 服务器会向其上层DNS服务器递归查询域名信息。域名解析的过程给服务器带来了很大的负载，每秒钟域名解析请求超过一定的数量就会造成DNS服务器解析域名超时。根据微软的统计数据，一台DNS服务器所能承受的动态域名查询的上限是每秒钟9000个请求。而我们知道，在一台P3的PC机上可以轻易地构造出每秒钟几万个域名解析请求，足以使一台硬件配置极高的DNS服务器瘫痪，由此可见DNS 服务器的脆弱性。同时需要注意的是，蠕虫扩散也会带来大量的域名解析请求。 4、CC(Challenge Collapsar)攻击 安全算法12345678public static String toHexString(byte[] b) &#123; StringBuilder sb = new StringBuilder(b.length * 2); for (int i = 0; i &lt; b.length; i++) &#123; sb.append(hexChar[(b[i] &amp; 0xf0) &gt;&gt;&gt; 4]); sb.append(hexChar[b[i] &amp; 0x0f]); &#125; return sb.toString(); &#125; 数字摘要http://www.jb51.net/article/96121.htmhash碰撞:如果待摘要的关键字为k1,Hash函数为f(x),则关键字k1的摘要为f(k1),若关键字k2不等于k1, 而f(k1)=f(k2),这种现象称为hash碰撞。一个hash函数的好坏是由发生碰撞的几率决定的。MD5(Message Digest Algorithm 5)SHA-1(Secure Hash Algorithm) 12345678910111213141516171819202122232425262728293031323334353637383940414243import java.security.MessageDigest; /** * 采用MD5加密 */public class MD5Util &#123; /*** * MD5加密 生成32位md5码 * @param 待加密字符串 * @return 返回32位md5码 */ public static String md5Encode(String inStr) throws Exception &#123; MessageDigest md5 = null; try &#123; md5 = MessageDigest.getInstance("MD5"); &#125; catch (Exception e) &#123; System.out.println(e.toString()); e.printStackTrace(); return ""; &#125; byte[] byteArray = inStr.getBytes("UTF-8"); byte[] md5Bytes = md5.digest(byteArray); StringBuffer hexValue = new StringBuffer(); for (int i = 0; i &lt; md5Bytes.length; i++) &#123; int val = ((int) md5Bytes[i]) &amp; 0xff; if (val &lt; 16) &#123; hexValue.append("0"); &#125; hexValue.append(Integer.toHexString(val)); &#125; return hexValue.toString(); &#125; /** * 测试主函数 */ public static void main(String args[]) throws Exception &#123; String str = new String("amigoxiexiexingxing"); System.out.println("原始：" + str); System.out.println("MD5后：" + md5Encode(str)); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041/** * 采用SHAA加密 */public class SHAUtil &#123; /*** * SHA加密 生成40位SHA码 * @param 待加密字符串 * @return 返回40位SHA码 */ public static String shaEncode(String inStr) throws Exception &#123; MessageDigest sha = null; try &#123; sha = MessageDigest.getInstance("SHA"); &#125; catch (Exception e) &#123; System.out.println(e.toString()); e.printStackTrace(); return ""; &#125; byte[] byteArray = inStr.getBytes("UTF-8"); byte[] md5Bytes = sha.digest(byteArray); StringBuffer hexValue = new StringBuffer(); for (int i = 0; i &lt; md5Bytes.length; i++) &#123; int val = ((int) md5Bytes[i]) &amp; 0xff; if (val &lt; 16) &#123; hexValue.append("0"); &#125; hexValue.append(Integer.toHexString(val)); &#125; return hexValue.toString(); &#125; /** * 测试主函数 */ public static void main(String args[]) throws Exception &#123; String str = new String("amigoxiexiexingxing"); System.out.println("原始：" + str); System.out.println("SHA后：" + shaEncode(str)); &#125;&#125; SHA-1和MD5的比较因为二者均由MD4导出，SHA-1和MD5彼此很相似。相应的，他们的强度和其他特性也是相似，但还有以下几点不同：1）对强行攻击的安全性：最显著和最重要的区别是SHA-1摘要比MD5摘要长32 位。使用强行技术，产生任何一个报文使其摘要等于给定报摘要的难度对MD5是2^128数量级的操作，而对SHA-1则是2^160数量级的操作。这样，SHA-1对强行攻击有更大的强度。2）对密码分析的安全性：由于MD5的设计，易受密码分析的攻击，SHA-1显得不易受这样的攻击。3）速度：在相同的硬件上，SHA-1的运行速度比MD5慢。 彩虹表破解Hash算法：彩虹表预先建立一个可逆向的散列链并将其存储在表中，在破解时先查表得到可能包含结果的散列链，然后在内存中重新计算并得到最终结果。折中方式综合了计算暴力破解和查找表破解的优点，并将计算时间和存储空间降低到可以接受的范围。 对称加密算法http://www.iteye.com/topic/1122076/常用的有:DES,AES等。目前AES是主流。加密：大体上分为双向加密和单向加密，而双向加密又分为对称加密和非对称加密。对称加密 采用单钥密码系统的加密方法，同一个密钥可以同时用作信息的加密和解密，这种加密方法称为对称加密，也称为单密钥加密。需要对加密和解密使用相同密钥的加密算法。由于其速度，对称性加密通常在消息发送方需要加密大量数据时使用。对称性加密也称为密钥加密。所谓对称，就是采用这种加密方法的双方使用方式用同样的密钥进行加密和解密。密钥是控制加密及解密过程的指令。 对称加密一般java类中中定义成员 ：1234567891011121314151617//KeyGenerator 提供对称密钥生成器的功能，支持各种算法 private KeyGenerator keygen; //SecretKey 负责保存对称密钥 private SecretKey deskey; //Cipher负责完成加密或解密工作 private Cipher c; //该字节数组负责保存加密的结果 private byte[] cipherByte; //Security.addProvider(new com.sun.crypto.provider.SunJCE()); //实例化支持DES算法的密钥生成器(算法名称命名需按规定，否则抛出异常) keygen = KeyGenerator.getInstance("DES");// //生成密钥 deskey = keygen.generateKey(); //生成Cipher对象,指定其支持的DES算法 c = Cipher.getInstance("DES"); 1、DES算法为密码体制中的对称密码体制，又被成为美国数据加密标准，是1972年美国IBM公司研制的对称密码体制加密算法。 明文按64位进行分组, 密钥长64位，密钥事实上是56位参与DES运算（第8、16、24、32、40、48、56、64位是校验位， 使得每个密钥都有奇数个1）分组后的明文组和56位的密钥按位替代或交换的方法形成密文组的加密方法。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class EncrypDES &#123; //KeyGenerator 提供对称密钥生成器的功能，支持各种算法 private KeyGenerator keygen; //SecretKey 负责保存对称密钥 private SecretKey deskey; //Cipher负责完成加密或解密工作 private Cipher c; //该字节数组负责保存加密的结果 private byte[] cipherByte; public EncrypDES() throws NoSuchAlgorithmException, NoSuchPaddingException&#123; Security.addProvider(new com.sun.crypto.provider.SunJCE()); //实例化支持DES算法的密钥生成器(算法名称命名需按规定，否则抛出异常) keygen = KeyGenerator.getInstance("DES"); //生成密钥 deskey = keygen.generateKey(); //生成Cipher对象,指定其支持的DES算法 c = Cipher.getInstance("DES"); &#125; /** * 对字符串加密 */ public byte[] Encrytor(String str) throws InvalidKeyException, IllegalBlockSizeException, BadPaddingException &#123; // 根据密钥，对Cipher对象进行初始化，ENCRYPT_MODE表示加密模式 c.init(Cipher.ENCRYPT_MODE, deskey); byte[] src = str.getBytes(); // 加密，结果保存进cipherByte cipherByte = c.doFinal(src); return cipherByte; &#125; /** * 对字符串解密 */ public byte[] Decryptor(byte[] buff) throws InvalidKeyException, IllegalBlockSizeException, BadPaddingException &#123; // 根据密钥，对Cipher对象进行初始化，DECRYPT_MODE表示加密模式 c.init(Cipher.DECRYPT_MODE, deskey); cipherByte = c.doFinal(buff); return cipherByte; &#125; public static void main(String[] args) throws Exception &#123; EncrypDES de1 = new EncrypDES(); String msg ="郭XX-搞笑相声全集"; byte[] encontent = de1.Encrytor(msg); byte[] decontent = de1.Decryptor(encontent); System.out.println("明文是:" + msg); System.out.println("加密后:" + new String(encontent)); System.out.println("解密后:" + new String(decontent)); &#125; &#125; 2、AES密码学中的高级加密标准（Advanced Encryption Standard，AES）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 public class EncrypAES &#123; //KeyGenerator 提供对称密钥生成器的功能，支持各种算法 private KeyGenerator keygen; //SecretKey 负责保存对称密钥 private SecretKey deskey; //Cipher负责完成加密或解密工作 private Cipher c; //该字节数组负责保存加密的结果 private byte[] cipherByte; public EncrypAES() throws NoSuchAlgorithmException, NoSuchPaddingException&#123; Security.addProvider(new com.sun.crypto.provider.SunJCE()); //实例化支持DES算法的密钥生成器(算法名称命名需按规定，否则抛出异常) keygen = KeyGenerator.getInstance("AES"); //生成密钥 deskey = keygen.generateKey(); //生成Cipher对象,指定其支持的DES算法 c = Cipher.getInstance("AES"); &#125; /** * 对字符串加密 */ public byte[] Encrytor(String str) throws InvalidKeyException, IllegalBlockSizeException, BadPaddingException &#123; // 根据密钥，对Cipher对象进行初始化，ENCRYPT_MODE表示加密模式 c.init(Cipher.ENCRYPT_MODE, deskey); byte[] src = str.getBytes(); // 加密，结果保存进cipherByte cipherByte = c.doFinal(src); return cipherByte; &#125; /** * 对字符串解密 */ public byte[] Decryptor(byte[] buff) throws InvalidKeyException, IllegalBlockSizeException, BadPaddingException &#123; // 根据密钥，对Cipher对象进行初始化，DECRYPT_MODE表示加密模式 c.init(Cipher.DECRYPT_MODE, deskey); cipherByte = c.doFinal(buff); return cipherByte; &#125; public static void main(String[] args) throws Exception &#123; EncrypAES de1 = new EncrypAES(); String msg ="郭XX-搞笑相声全集"; byte[] encontent = de1.Encrytor(msg); byte[] decontent = de1.Decryptor(encontent); System.out.println("明文是:" + msg); System.out.println("加密后:" + new String(encontent)); System.out.println("解密后:" + new String(decontent)); &#125; &#125; 因为某些国家的进口管制限制，Java发布的运行环境包中的默认仅允许128位密钥的AES加解密 非对称加密与对称加密算法不同，非对称加密算法需要两个密钥：公开密钥（publick ey）和私有密钥 (private key)公开密钥与私有密钥是一对，如果用公开密钥对数据进行加密，只有用对应的私有密钥才能解密；如果用私有密钥对数据进行加密，那么只有用对应的公开密钥才能解密。因为加密和解密使用的是两个不同的密钥，所以这种算法叫作非对称加密算法。RSA算法基于一个十分简单的数论事实：将两个大素数相乘十分容易，但那时想要对其乘积进行因式分解却极其困难，因此可以将乘积公开作为加密密钥。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859public class EncrypRSA &#123; /** * 加密 */ protected byte[] encrypt(RSAPublicKey publicKey,byte[] srcBytes) throws NoSuchAlgorithmException, NoSuchPaddingException, InvalidKeyException, IllegalBlockSizeException, BadPaddingException&#123; if(publicKey!=null)&#123; //Cipher负责完成加密或解密工作，基于RSA Cipher cipher = Cipher.getInstance("RSA"); //根据公钥，对Cipher对象进行初始化 cipher.init(Cipher.ENCRYPT_MODE, publicKey); byte[] resultBytes = cipher.doFinal(srcBytes); return resultBytes; &#125; return null; &#125; /** * 解密 */ protected byte[] decrypt(RSAPrivateKey privateKey,byte[] srcBytes) throws NoSuchAlgorithmException, NoSuchPaddingException, InvalidKeyException, IllegalBlockSizeException, BadPaddingException&#123; if(privateKey!=null)&#123; //Cipher负责完成加密或解密工作，基于RSA Cipher cipher = Cipher.getInstance("RSA"); //根据公钥，对Cipher对象进行初始化 cipher.init(Cipher.DECRYPT_MODE, privateKey); byte[] resultBytes = cipher.doFinal(srcBytes); return resultBytes; &#125; return null; &#125; public static void main(String[] args) throws NoSuchAlgorithmException, InvalidKeyException, NoSuchPaddingException, IllegalBlockSizeException, BadPaddingException &#123; EncrypRSA rsa = new EncrypRSA(); String msg = "郭XX-精品相声"; //KeyPairGenerator类用于生成公钥和私钥对，基于RSA算法生成对象 KeyPairGenerator keyPairGen = KeyPairGenerator.getInstance("RSA"); //初始化密钥对生成器，密钥大小为1024位 keyPairGen.initialize(1024); //生成一个密钥对，保存在keyPair中 KeyPair keyPair = keyPairGen.generateKeyPair(); //得到私钥 RSAPrivateKey privateKey = (RSAPrivateKey)keyPair.getPrivate(); //得到公钥 RSAPublicKey publicKey = (RSAPublicKey)keyPair.getPublic(); //用公钥加密 byte[] srcBytes = msg.getBytes(); byte[] resultBytes = rsa.encrypt(publicKey, srcBytes); //用私钥解密 byte[] decBytes = rsa.decrypt(privateKey, resultBytes); System.out.println("明文是:" + msg); System.out.println("加密后是:" + new String(resultBytes)); System.out.println("解密后是:" + new String(decBytes)); &#125; &#125; 数字签名https://www.cnblogs.com/SirSmith/p/4985571.htmlhttp://840327220.iteye.com/blog/2225109数字签名是一种安全措施，分为：消息摘要和消息签名。1、消息摘要：是一种算法，分为MD5/SHA算法，主要作用用来防止消息在传递途中被“第三者”篡改了。2、消息签名：其基础是公钥和私钥的非对称加密。主要作用是验证发消息者的身份，确保消息来源的可靠性。 1234567891011121314151617181920212223242526String password="test"; // 1.初始化密钥 KeyPairGenerator keyPairGenerator = KeyPairGenerator.getInstance("RSA"); keyPairGenerator.initialize(512); KeyPair keyPair = keyPairGenerator.generateKeyPair(); RSAPublicKey rsaPublicKey = (RSAPublicKey)keyPair.getPublic(); RSAPrivateKey rsaPrivateKey = (RSAPrivateKey)keyPair.getPrivate(); // 2.进行签名 PKCS8EncodedKeySpec pkcs8EncodedKeySpec = new PKCS8EncodedKeySpec(rsaPrivateKey.getEncoded()); KeyFactory keyFactory = KeyFactory.getInstance("RSA"); PrivateKey privateKey = keyFactory.generatePrivate(pkcs8EncodedKeySpec); Signature signature = Signature.getInstance("MD5withRSA"); signature.initSign(privateKey); signature.update(password.getBytes()); byte[] result = signature.sign(); //System.out.println("jdk rsa sign:" + Hex.encodeHexString(result) ); // 3.验证签名 X509EncodedKeySpec x509EncodedKeySpec = new X509EncodedKeySpec(rsaPublicKey.getEncoded()); keyFactory = KeyFactory.getInstance("RSA"); PublicKey publicKey = keyFactory.generatePublic(x509EncodedKeySpec); signature = Signature.getInstance("MD5withRSA"); signature.initVerify(publicKey); signature.update(password.getBytes()); boolean bool = signature.verify(result); 数字证书https://www.cnblogs.com/JeffreySun/archive/2010/06/24/1627247.htmlhttps://www.2cto.com/article/201203/122095.htmlhttp://yale.iteye.com/blog/1675344http://blog.csdn.net/abinge317/article/details/51791856由于网络上通信的双方可能都不认识对方，那么就需要第三者来介绍，这就是数字证书。数字证书由Certificate Authority( CA 认证中心)颁发。 用户A把自己的证书发送给用户B。用户B使用CA的公钥对证书的签名进行验证，由于只有CA才能生成该证书，因此只要证书验证正确，即说明证书是由CA发布的，证书中用户A的公钥是值得信赖的。用户B以后就可以使用该公钥验证用户A的签名或者进行和A进行加密通信。 数字证书为发布公钥提供了一种简便的途径，其数字证书则成为加密算法以及公钥的载体，依靠数字证书，我们可以构建一个简单的加密网络应用平台，数字证书就好比我们生活中的身份证，现实中，身份证由公安机关签发，而网络用户的身份凭证由数字证书颁发认证机构—CA签发，只有经过CA签发的证书在网络中才具备可认证性，CA并不是一个单纯的防御手段，它集合了多种密码学算法： 消息摘要算法：MD5、和SHA（对数字证书本省做摘要处理，用于验证数据完整性服务器） 对称加密算法：RC2、RC4、IDEA、DES、AES（对数据进行加密/解密操作，用于保证数据保密性服务） 非对称加密算法：RSA、DH（对数据进行加密/解密操作，用于保证数据保密性服务） 数字签名算法：RSA、DSA(对数据进行签名/验证操作，保证数据的完整性和抗否认性)。 证书的签发过程实际上是对申请数字证书的公钥做数字签名，证书的验证过程实际上是对数字证书的公钥做验证签名，其中还包含证书有效期验证，通过CA数字证书，我们对网络上传输的数据进行加密/解密和签名/验证操作，确保数据机密性、完整性、抗否认性、认证性，保证交易实体身份的真实性，保证网络安全性。 所有证书有多种文件编码格式，主要包括: CER编码(规范编码格式)：是数字证书的一种编码格式，它是BER(基本编码格式)的一个变种，比BER规定得更严格 DER(卓越编码格式)：同样是BER的一个变种，与CER的不同在于，DER使用定长模式，而CER使用变长模式。所有证书都符合公钥基础设施(PKI)制定的ITU-T X509国际标准，PKCS(公钥加密标准)由RSA实验室和其他安全系统开发商为促进公钥密码的发展而制定的一系列标准，比如:PKCS#7(密码消息语法标准—-文件后缀名:.p7b、.p7c、.spc)、PKCS#10(证书请求语法标准—-文件后缀名:.p10、.csr)、PKCS#12(个人信息交换语法标准—-文件后缀名:.p12、.pfx)等。在获得数字证书后，可以将其保存在电脑中，也可以保存在USB Key等相应的设备中。 一个数字证书的例子1234567891011121314151617181920212223242526272829303132333435363738394041424344Certificate: Data: &lt;span style=&quot;color:#FF0000;&quot;&gt;证书标准版本号&lt;/span&gt; Version: 1 (0x0) &lt;span style=&quot;color:#FF0000;&quot;&gt;该证书的唯一编号&lt;/span&gt; Serial Number: 7829 (0x1e95) &lt;span style=&quot;color:#FF0000;&quot;&gt;该证书的签名算法&lt;/span&gt; Signature Algorithm: md5WithRSAEncryption &lt;span style=&quot;color:#FF0000;&quot;&gt;颁布本证书的证书机构&lt;/span&gt; Issuer: C=ZA, ST=Western Cape, L=Cape Town, O=Thawte Consulting cc, OU=Certification Services Division, CN=Thawte Server CA/emailAddress=server-certs@thawte.com &lt;span style=&quot;color:#FF0000;&quot;&gt;证书有效期&lt;/span&gt; Validity Not Before: Jul 9 16:04:02 1998 GMT Not After : Jul 9 16:04:02 1999 GMT &lt;span style=&quot;color:#FF0000;&quot;&gt;证书持有人的姓名、地址等信息&lt;/span&gt; Subject: C=US, ST=Maryland, L=Pasadena, O=Brent Baccala, OU=FreeSoft, CN=www.freesoft.org/emailAddress=baccala@freesoft.org &lt;span style=&quot;color:#FF0000;&quot;&gt;证书持有人的公钥&lt;/span&gt; Subject Public Key Info: Public Key Algorithm: rsaEncryption RSA Public Key: (1024 bit) Modulus (1024 bit): 00:b4:31:98:0a:c4:bc:62:c1:88:aa:dc:b0:c8:bb: 33:35:19:d5:0c:64:b9:3d:41:b2:96:fc:f3:31:e1: 66:36:d0:8e:56:12:44:ba:75:eb:e8:1c:9c:5b:66: 70:33:52:14:c9:ec:4f:91:51:70:39:de:53:85:17: 16:94:6e:ee:f4:d5:6f:d5:ca:b3:47:5e:1b:0c:7b: c5:cc:2b:6b:c1:90:c3:16:31:0d:bf:7a:c7:47:77: 8f:a0:21:c7:4c:d0:16:65:00:c1:0f:d7:b8:80:e3: d2:75:6b:c1:ea:9e:5c:5c:ea:7d:c1:a1:10:bc:b8: e8:35:1c:9e:27:52:7e:41:8f Exponent: 65537 (0x10001) &lt;span style=&quot;color:#FF0000;&quot;&gt;证书机构对该证书的数字签名&lt;/span&gt; Signature Algorithm: md5WithRSAEncryption 93:5f:8f:5f:c5:af:bf:0a:ab:a5:6d:fb:24:5f:b6:59:5d:9d: 92:2e:4a:1b:8b:ac:7d:99:17:5d:cd:19:f6:ad:ef:63:2f:92: ab:2f:4b:cf:0a:13:90:ee:2c:0e:43:03:be:f6:ea:8e:9c:67: d0:a2:40:03:f7:ef:6a:15:09:79:a9:46:ed:b7:16:1b:41:72: 0d:19:aa:ad:dd:9a:df:ab:97:50:65:f5:5e:85:a6:ef:19:d1: 5a:de:9d:ea:63:cd:cb:cc:6d:5d:01:85:b5:6d:c8:f3:d9:f7: 8f:0e:fc:ba:1f:34:e9:96:6e:6c:cf:f2:ef:9b:bf:de:b5:22: 68:9f 我们先来看一个简单的证书机构签发的流程:这里的认证机构如何是证书申请者本身，将获得自签名证书。当客户端获得服务器下发的数字证书后，即可使用数字证书进行加密交互：数字证书的应用环境是在https安全协议中，使用流程远比上述加密交互流程复杂，但是相关操作封装在传输层，对于应用层透明，在https安全协议中使用非对称加密算法交换密钥，使用对称加密算法对数据进行加密/解密操作，提高加密/解密效率要获得数字证书，我们需要使用数字证书管理工具：KeyTool和OpenSSL构建CSR(数字证书签发申请)，交由CA机构签发，形成最终的数字证书。 SSL/TLS协议分为两层： 记录协议:建议在可靠的传输协议之上，为高层协议提供数据封装、压缩、加密等基本功能的支持 握手协议:建立在SSL记录协议之上，用于在实际的数据传输开始前，通讯双方进行身份认证、协商加密算法、交换加密密钥等经过了SSL/TLS握手协议交互后，数据交互双方确定了本次会话使用的对称加密算法以及密钥，就可以开始进行加密数据交互了，以下是握手协议服务器端和客户端构建加密交互的相关流程图：1、 随机数为后续构建密钥准备2、 其他信息包括服务器证书、甚至包含获取客户端证书的请求3、验证算法如果服务器端回复客户端时带有其他信息，则进入数字证书验证阶段客户端验证服务器端证书：服务器端验证客户端证书:(非金融行业等关键性行业，可选)4、产生密钥当服务器端和客户端经过上述流程后，就开始密钥构建交互了，服务器端和客户端最初需要主密钥为构建会话密钥做准备：5、 会话密钥 (用于对称加密)完成上述主密钥构建操作后，服务器端和客户端将建立会话密钥，完成握手协议：6、加密交互上述服务器端和客户端完成了握手协议以后就进入正式会话阶段，如果上述流程中有任何一端受到外界因素干扰发生异常，则重新进入协商算法阶段，下面流程表现进入会话阶段后，服务器端和客户端将使用会话密钥进行加密交互： 代码解释在JAVA 6 以上版本中提供了完善的数字证书管理的实现，我们不需要关注相关具体算法，仅通过操作密钥库和数字证书就可以完成相应的加密/解密和签名/验证操作，密钥库管理私钥，数字证书管理公钥，私钥和密钥分属消息传递两方，进行加密消息的传递。因此，我们可以将密钥库看做私钥相关操作的入口，数字证书则是公钥相关操作的入口：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234/**** * 获得私钥，获得私钥后，通过RSA算方法实现进行"私钥加密，公钥解密"和"公钥加密，私钥解密"操作 * @param keyStorePath 密钥库路径 * @param alias 别名 * @param password 密码 * @return 私钥 */ private static PrivateKey getPrivateKeyByKeyStore(String keyStorePath,String alias,String password)throws Exception&#123; //获得密钥库 KeyStore ks = getKeyStore(keyStorePath,password); //获得私钥 return (PrivateKey)ks.getKey(alias, password.toCharArray()); &#125; /**** * 由Certificate获得公钥，获得公钥后，通过RSA算方法实现进行"私钥加密，公钥解密"和"公钥加密，私钥解密"操作 * @param certificatePath 证书路径 * @return 公钥 */ private static PublicKey getPublicKeyByCertificate(String certificatePath)throws Exception &#123; //获得证书 Certificate certificate = getCertificate(certificatePath); //获得公钥 return certificate.getPublicKey(); &#125; /**** * 加载数字证书，JAVA 6仅支持x.509的数字证书 * @param certificatePath 证书路径 * @return 证书 * @throws Exception */ private static Certificate getCertificate(String certificatePath) throws Exception&#123; //实例化证书工厂 CertificateFactory certificateFactory = CertificateFactory.getInstance("x.509"); //取得证书文件流 FileInputStream in = new FileInputStream(certificatePath); //生成证书 Certificate certificate = certificateFactory.generateCertificate(in); //关闭证书文件流 in.close(); return certificate; &#125; /**** * 获得Certificate * @param keyStorePath 密钥库路径 * @param alias 别名 * @param password 密码 * @return 证书 * @throws Exception */ private static Certificate getCertificate(String keyStorePath,String alias,String password) throws Exception&#123; //由密钥库获得数字证书构建数字签名对象 //获得密钥库 KeyStore ks = getKeyStore(keyStorePath,password); //获得证书 return ks.getCertificate(alias); &#125; /**** * 加载密钥库，加载了以后，我们就能通过相应的方法获得私钥，也可以获得数字证书 * @param keyStorePath 密钥库路径 * @param password 密码 * @return 密钥库 * @throws Exception */ private static KeyStore getKeyStore(String keyStorePath,String password) throws Exception&#123; //实例化密钥库 KeyStore ks = KeyStore.getInstance(KeyStore.getDefaultType()); //获得密钥库文件流 FileInputStream is = new FileInputStream(keyStorePath); //加载密钥库 ks.load(is,password.toCharArray()); //关闭密钥库文件流 is.close(); return ks; &#125; /**** * 私钥加密 * @param data 待加密的数据 * @param keyStorePath 密钥库路径 * @param alias 别名 * @param password 密码 * @return 加密数据 * @throws Exception */ public static byte[] encryptByPriateKey(byte[] data,String keyStorePath,String alias,String password) throws Exception&#123; //获得私钥 PrivateKey privateKey = getPrivateKeyByKeyStore(keyStorePath,alias,password); //对数据加密 Cipher cipher = Cipher.getInstance(privateKey.getAlgorithm()); return cipher.doFinal(data); &#125; /**** * 私钥解密 * @param data 待解密数据 * @param keyStorePath 密钥库路径 * @param alias 别名 * @param password 密码 * @return 解密数据 * @throws Exception */ public static byte[] decryptByPrivateKey(byte[] data,String keyStorePath,String alias,String password) throws Exception&#123; //取得私钥 PrivateKey privateKey = getPrivateKeyByKeyStore(keyStorePath,alias,password); //对数据解密 Cipher cipher = Cipher.getInstance(privateKey.getAlgorithm()); cipher.init(Cipher.DECRYPT_MODE,privateKey); return cipher.doFinal(data); &#125; /**** * 公钥加密 * @param data 等待加密数据 * @param certificatePath 证书路径 * @return 加密数据 * @throws Exception */ public static byte[] encryptByPublicKey(byte[] data,String certificatePath) throws Exception&#123; //取得公钥 PublicKey publicKey = getPublicKeyByCertificate(certificatePath); //对数据加密 Cipher cipher = Cipher.getInstance(publicKey.getAlgorithm()); cipher.init(Cipher.ENCRYPT_MODE,publicKey); return cipher.doFinal(data); &#125; /**** * 公钥解密 * @param data 等待解密的数据 * @param certificatePath 证书路径 * @return 解密数据 * @throws Exception */ public static byte[] decryptByPublicKey(byte[] data,String certificatePath)throws Exception&#123; //取得公钥 PublicKey publicKey = getPublicKeyByCertificate(certificatePath); //对数据解密 Cipher cipher = Cipher.getInstance(publicKey.getAlgorithm()); cipher.init(Cipher.DECRYPT_MODE, publicKey); return cipher.doFinal(data); &#125; /**** * @param sign 签名 * @param keyStorePath 密钥库路径 * @param alias 别名 * @param password 密码 * @return 签名 * @throws Exception */ public static byte[] sign(byte[] sign,String keyStorePath,String alias,String password)throws Exception&#123; //获得证书 X509Certificate x509Certificate = (X509Certificate) getCertificate(keyStorePath,alias,password); //构建签名,由证书指定签名算法 Signature signature = Signature.getInstance(x509Certificate.getSigAlgName()); //获取私钥 PrivateKey privateKey = getPrivateKeyByKeyStore(keyStorePath,alias,password); //初始化签名，由私钥构建 signature.initSign(privateKey); signature.update(sign); return signature.sign(); &#125; /**** * 验证签名 * @param data 数据 * @param sign 签名 * @param certificatePath 证书路径 * @return 验证通过为真 * @throws Exception */ public static boolean verify(byte[] data,byte[] sign,String certificatePath) throws Exception&#123; //获得证书 X509Certificate x509Certificate = (X509Certificate)getCertificate(certificatePath); //由证书构建签名 Signature signature = Signature.getInstance(x509Certificate.getSigAlgName()); //由证书初始化签名，实际上是使用了证书中的公钥 signature.initVerify(x509Certificate); signature.update(data); return signature.verify(sign); &#125; //我们假定密钥库文件yale.keystore存储在D盘根目录，数字证书文件yale.cer也存储在D盘根目录 /**** * 公钥加密---私钥解密 * @throws Exception */ public static void test1() throws Exception&#123; System.err.println("公钥加密---私钥解密"); String inputStr = "数字证书"; byte[] data = inputStr.getBytes(); //公钥加密 byte[] encrypt = CertificateCoder.encryptByPublicKey(data, certificatePath); //私钥解密 byte[] decrypt = CertificateCoder.decryptByPrivateKey(encrypt, keyStorePath, alias, password); String outputStr = new String(decrypt); System.err.println("加密前：\n" + inputStr); System.err.println("解密后：\n" + outputStr); &#125; /**** * 私钥加密---公钥解密 * @throws Exception */ public static void test2()throws Exception&#123; System.err.println("私钥加密---公钥解密"); String inputStr = "数字签名"; byte[] data = inputStr.getBytes(); //私钥加密 byte[] encodedData = CertificateCoder.encryptByPriateKey(data, keyStorePath, alias, password); //公钥加密 byte[] decodeData = CertificateCoder.decryptByPublicKey(encodedData, certificatePath); String outputStr = new String (decodeData); System.err.println("加密前：\n" + inputStr); System.err.println("解密后：\n" + outputStr); &#125; public static void testSign()throws Exception&#123; String inputStr = "签名"; byte[] data = inputStr.getBytes(); System.err.println("私钥签名---公钥验证"); //产生签名 byte[] sign = CertificateCoder.sign(data, keyStorePath, alias, password); System.err.println("签名:\n" + Hex.encodeHexString(sign)); //验证签名 boolean status = CertificateCoder.verify(data, sign, certificatePath); System.err.println("状态：\n " + status); &#125; RSA非对称加密的2个用途及在HTTPS加密（防窃听） RSA非对称加密会用到一对密钥，分别称为公钥和私钥，公钥加密之后的数据可以通过私钥来进行解密，私钥加密的数据也同样可以用对应的公钥进行解密。在web数据传输过程中，由于客户端和服务器端是多对一的关系，因此可以让所有的客户端持有相同的公钥，服务器持有私钥，这样一来就能方便地实现数据的加密传输。 签名（防篡改） 由于私钥只在某一个体手中，因此可以通过这一点来进行身份识别。比如用户A和B分别有一对密钥中的私钥和公钥，现在A向B发送消息”abc”，可进行如下操作：A用私钥对该文本进行加密之后变成密文”#￥%”，并附加上原文，组合成文本”#￥%：abc”(冒号起分隔作用，并无其他含义，具体实现中可自行处理)一起发送，B接收到该文本之后利用公钥对密文进行解密，将得到的解密后文本与传送过来的文本”abc”之间进行比对，如果一切正常，那么公钥解密之后的文本就是私钥加密之前的文本”abc”，比对结果一致，因此可以说明这段”abc”文本确实是A发送过来的，因为只有A才有对文本进行签名的私钥。能得到这个结论的前提是——A所用的私钥跟B所用的公钥确实是一对。 假如在传送途中别人篡改了”abc”，改成”aaa”，由于中间人没有A所持有的私钥，因此无法对篡改之后的数据生成新的正确签名，那么B在收到数据之后用公钥进行解密，再与传送的文本进行比对的话就不会一致。或者中间人篡改了数据之后用另一私钥对篡改之后的数据进行签名，同样由于B没有中间人的私钥对应的公钥，因此比对也不会一致。记住一点：B的公钥所对应的私钥只在A的手中，因此比对一致就说明该文本来自A。 https如何保证安全？如何保证客户端所持有的公钥就是某合法服务器声明的公钥？ 如果不能保证这一点，那么客户端发送的信息就有可能存在被窃听的危险，因为用此公钥加密的数据可以被其对应的私钥拥有者获取，而该私钥并不在客户端所认为的服务器上。因此可采用一个权威机构进行证书的颁发，所谓证书就是包含了服务器声明的公钥以及组织名称等信息，这里我们只考虑最关键的公钥信息。该权威机构会对申请证书的组织进行审核，确保其身份合法，然后将服务器公钥信息发布给客户端，客户端可利用该公钥与对应的服务器进行通信。整个过程可归纳为以下几步：1、服务器生成一对密钥，私钥自己留着，公钥交给数字证书认证机构（CA）2、CA进行审核，并用CA自己的私钥对服务器提供的公钥进行签名（参照上文RSA签名）3、客户端从CA获取证书（即服务器端公钥），用CA的公钥对签名的证书进行验证，比对一致，说明该服务器公钥确实是CA颁发的（得此结论有一个前提就是：客户端的CA公钥确实是CA的公钥，即该CA的公钥与CA对证书进行签名的私钥确实是一对。参照上文RSA签名中所论述的情况），而CA又作为权威机构保证该公钥的确是服务器端提供的，从而可以确认该证书中的公钥确实是合法服务器端提供的 注：为保证第3步中提到的前提条件，CA的公钥必须要安全地转交给客户端，因此，CA的公钥一般来说由浏览器开发商内置在浏览器的内部。于是，该前提条件在各种信任机制上，基本保证成立。由此可见：所谓的安全的HTTP，其实也是要建立在信任的机制上。 总结：整个过程涉及2对公私密钥对，一对由服务器产生，用于加密，一对由CA产生，用于签名。整个过程还涉及2个信任：客户端信任CA，CA发布的证书中的公钥就是合法服务器的公钥。客户端信任浏览器内置的CA公钥就是与CA私钥对应的公钥。最后要说明的是，非对称加密在https中只是用来对对称加密密钥进行协商的过程才使用，在两端协商完对称加密的密钥之后，数据的加密传输均采用对称加密的方式。 HTTPS在传输数据之前需要客户端（浏览器）与服务端（网站）之间进行一次握手，在握手过程中将确立双方加密传输数据的密码信息。TLS/SSL协议不仅仅是一套加密传输的协议，更是一件经过艺术家精心设计的艺术品，TLS/SSL中使用了非对称加密，对称加密以及HASH算法。握手过程的具体描述如下：1234567891011121. 浏览器将自己支持的一套加密规则发送给网站。 2.网站从中选出一组加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含了网站地址，加密公钥，以及证书的颁发机构等信息。 3.浏览器获得网站证书之后浏览器要做以下工作： a) 验证证书的合法性（颁发证书的机构是否合法，证书中包含的网站地址是否与正在访问的地址一致等），如果证书受信任，则浏览器栏里面会显示一个小锁头，否则会给出证书不受信的提示。 b) 如果证书受信任，或者是用户接受了不受信的证书，浏览器会生成一串随机数的密码，并用证书中提供的公钥加密。 c) 使用约定好的HASH算法计算握手消息，并使用生成的随机数对消息进行加密，最后将之前生成的所有信息发送给网站。 4.网站接收浏览器发来的数据之后要做以下的操作： a) 使用自己的私钥将信息解密取出密码，使用密码解密浏览器发来的握手消息，并验证HASH是否与浏览器发来的一致。 b) 使用密码加密一段握手消息，发送给浏览器。 5.浏览器解密并计算握手消息的HASH，如果与服务端发来的HASH一致，此时握手过程结束，之后所有的通信数据将由之前浏览器生成的随机密码并利用对称加密算法进行加密。 linux下生成https的crt和key证书http://blog.csdn.net/xuplus/article/details/51613883linux下openssl生成 签名的步骤：x509证书一般会用到三类文，key，csr，crt。Key 是私用密钥openssl格，通常是rsa算法。Csr 是证书请求文件，用于申请证书。在制作csr文件的时，必须使用自己的私钥来签署申，还可以设定一个密钥。crt是CA认证后的证书文，（windows下面的，其实是crt），签署人用自己的key给你签署的凭证。 1.key的生成openssl genrsa -des3 -out server.key 2048这样是生成rsa私钥，des3算法，openssl格式，2048位强度。server.key是密钥文件名。为了生成这样的密钥，需要一个至少四位的密码。可以通过以下方法生成没有密码的key:openssl rsa -in server.key -out server.key server.key就是没有密码的版本了。 生成CA的crtopenssl req -new -x509 -key server.key -out ca.crt -days 3650生成的ca.crt文件是用来签署下面的server.csr文件。 csr的生成方法openssl req -new -key server.key -out server.csr需要依次输入国家，地区，组织，email。最重要的是有一个common name，可以写你的名字或者域名。如果为了https申请，这个必须和域名吻合，否则会引发浏览器警报。生成的csr文件交给CA签名后形成服务端自己的证书。 crt生成方法CSR文件必须有CA的签名才可形成证书，可将此文件发送到verisign等地方由它验证，要交一大笔钱，何不自己做CA呢。openssl x509 -req -days 3650 -in server.csr -CA ca.crt -CAkey server.key -CAcreateserial -out server.crt输入key的密钥后，完成证书生成。-CA选项指明用于被签名的csr证书，-CAkey选项指明用于签名的密钥，-CAserial指明序列号文件，而-CAcreateserial指明文件不存在时自动生成。最后生成了私用密钥：server.key和自己认证的SSL证书：server.crt证书合并：cat server.key server.crt &gt; server.pem 什么是数字签名和证书？http://www.jianshu.com/p/9db57e7612551.信息安全三要素 信息安全中有三个需要解决的问题： 保密性(Confidentiality)：信息在传输时不被泄露 完整性（Integrity）：信息在传输时不被篡改 有效性（Availability）：信息的使用者是合法的这三要素统称为CIA Triad。 公钥密码解决保密性问题 数字签名解决完整性问题和有效性问题 2.数字签名 现实生活中，签名有什么作用？在一封信中，文末的签名是为了表示这封信是签名者写的。计算机中，数字签名也是相同的含义：证明消息是某个特定的人，而不是随随便便一个人发送的（有效性）；除此之外，数字签名还能证明消息没有被篡改（完整性）。 简单来说，数字签名（digital signature）是公钥密码的逆应用：用私钥加密消息，用公钥解密消息。 用私钥加密的消息称为签名，只有拥有私钥的用户可以生成签名。 用公钥解密签名这一步称为验证签名，所有用户都可以验证签名(因为公钥是公开的) 一旦签名验证成功，根据公私钥数学上的对应关系，就可以知道该消息是唯一拥有私钥的用户发送的，而不是随便一个用户发送的。 由于私钥是唯一的，因此数字签名可以保证发送者事后不能抵赖对报文的签名。由此，消息的接收者可以通过数字签名，使第三方确信签名人的身份及发出消息的事实。当双方就消息发出与否及其内容出现争论时，数字签名就可成为一个有力的证据。 生成签名 一般来说，不直接对消息进行签名，而是对消息的哈希值进行签名，步骤如下。 对消息进行哈希计算，得到哈希值 利用私钥对哈希值进行加密，生成签名 将签名附加在消息后面，一起发送过去 验证签名 收到消息后，提取消息中的签名 用公钥对签名进行解密，得到哈希值1。 对消息中的正文进行哈希计算，得到哈希值2。 比较哈希值1和哈希值2，如果相同，则验证成功。 3.证书 证书实际上就是对公钥进行数字签名，它是对公钥合法性提供证明的技术。 考虑这样一种场景：我们对签名进行验证时，需要用到公钥。如果公钥也是伪造的，那怎么办？如果公钥是假的，验证数字签名那就无从谈起，根本不可能从数字签名确定对方的合法性。这时候证书就派上用场了。 证书一般包含：公钥（记住证书中是带有公钥的），公钥的数字签名，公钥拥有者的信息若证书验证成功，这表示该公钥是合法，可信的。 接下来又有问题了：验证证书中的数字签名需要另一个公钥，那么这个公钥的合法性又该如何保证？该问题可以无限循环下去，岂不是到不了头了？这已经是个社会学问题了。我们为什么把钱存进银行？因为我们相信银行，它是一个可信的机构（虽然也有破产的风险）。跟银行一样，我们需要一个可信的机构来颁发证书和提供公钥，只要是它提供的公钥，我们就相信是合法的。 这种机构称为认证机构(Certification Authority， CA)。CA就是能够认定”公钥确实属于此人”，并能生成公钥的数字签名的组织或机构。CA有国际性组织和政府设立的组织，也有通过提供认证服务来盈利的组织。 如何生成证书？ 服务器将公钥A给CA（公钥是服务器的） CA用自己的私钥B给公钥A加密，生成数字签名A CA把公钥A，数字签名A，附加一些服务器信息整合在一起，生成证书，发回给服务器。注：私钥B是用于加密公钥A的，私钥B和公钥A并不是配对的。 如何验证证书？ 客户端得到证书 客户端得到证书的公钥B（通过CA或其它途径） 客户端用公钥B对证书中的数字签名解密，得到哈希值 客户端对公钥进行哈希值计算 两个哈希值对比，如果相同，则证书合法。注：公钥B和上述的私钥B是配对的，分别用于对证书的验证（解密）和生成（加密）。 证书作废 当用户私钥丢失、被盗时，认证机构需要对证书进行作废(revoke)。要作废证书，认证机构需要制作一张证书作废清单(Certificate Revocation List)，简称CRL 假设我们有Bob的证书，该证书有合法的认证机构签名，而且在有效期内，但仅凭这些还不能说明该证书一定有效，还需要查询认证机构最新的CRL，并确认该证书是否有效。]]></content>
      <categories>
        <category>读书笔记</category>
        <category>大型网站分布式架构与设计实践</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>msa</tag>
        <tag>架构</tag>
        <tag>soa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[读书笔记]chapter2-分布式基础设施-大型网站分布式架构与设计实践]]></title>
    <url>%2F2017%2F12%2F17%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chapter2-%E5%88%86%E5%B8%83%E5%BC%8F%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%AE%BE%E8%AE%A1%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[分布式系统的支撑系统： 协作及配置管理:Zookeeper 缓存:Redis 消息:ActiveMQ 持久化存储:MySQL,HBase 搜索引擎:Lucene,solr,ElastickSearch CDN 负载均衡:F5,HAProxy,Nginx,SQLProxy 运维自动化 计算:Spark,Spark Streaming,Storm 分布式文件系统:HDFS,FastDfs 日志收集系统:Kafka,ELK 监控系统:Zabbix MySQL高可用方案：(Double)Master-Slave Replication、分库与分表。Master-Slave Replication基于master的binary log进行数据的同步。分库分表的缺点：难以多表关联查询，事务提升到分布式事务，扩容不便会导致数据迁移.考虑点：业务拆分、海量数据带来的单表数据量过大问题、查询效率，高并发访问压力，复杂查询的支持、单点故障。 HBase:分布式列存储关系数据库。集群包含:HMaster和HRegionServer.缺点：支持的查询维度有限，且难以支持复杂查询，如group by,order by,join等。 JMS2种传输模式:Point-to-Point(p2p)模型，Pub/Sub(Publish/Subscribe)模型. 前者称为点对点模型，基于queue，后者称为发布/订阅模型，基于topic。ActiveMQ支持Master-Slave架构，在该架构上，基于恭喜文件系统或共享数据库实现failover(故障转移).ActiveMQ高可用方案:Master-Slave、拆分broker 分库分表、HBase等都会涉及复杂查询的问题，在这个时候，我们可以考虑使用分布式搜索引擎, 如基于Lucene实现的solr和ElasticSearch。]]></content>
      <categories>
        <category>读书笔记</category>
        <category>大型网站分布式架构与设计实践</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>msa</tag>
        <tag>架构</tag>
        <tag>soa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[[读书笔记]chapter1-SOA面向服务的体系架构-大型网站分布式架构与设计实践]]></title>
    <url>%2F2017%2F12%2F17%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-chapter1-SOA%E9%9D%A2%E5%90%91%E6%9C%8D%E5%8A%A1%E7%9A%84%E4%BD%93%E7%B3%BB%E6%9E%B6%E6%9E%84-%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E4%B8%8E%E8%AE%BE%E8%AE%A1%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[分布式应用架构的演变：单一应用-》垂直应用-》分布式应用.垂直应用缺点：通过业务划分，将流量分散到不同的子系统，但子系统见可能存在重叠业务，需要重复造轮子，且容易形成信息孤岛。分布式应用核心：(微)服务化。 协议：基于TCP协议的RPC，基于Http协议的Restful。前者特点是由于处于网络协议的第四层，因此报文较小，网络开销小，性能高，但实现的代价高(不过目前国内有dubbo这个比较成熟的解决方案)，不跨语言；而后者基于网络协议的第7层，可以支持JSON&amp;XML格式数据传输，利于跨语言(目前主流为SpringCloud)。 基于TCP协议的RPCRPC(Remote Process Call):实现又RMI,WebService,Dubbo.涉及服务的提供者和调用方，及参数及结果的序列化。同时伴随着服务的数目增多及服务压力增加：又需要考虑服务分组隔离、服务路由(router)及负载均衡(Load Balance)。 对象的序列化方案：Java本身的,Hessian,Google protobuffer,JSON,xml. 服务的路由和负载均衡服务化(SOA,Service-Oriented Architecture):剥离公共服务，分离不同业务服务。负载均衡方案：硬件负载均衡如F5，软件解决：HAProxy，Ngyinx，LVS，Zookeeper等为了避免服务路由的单点故障，需要一个可以动态注册和获取服务信息的地方，来统一管理服务名称和其对应服务器列表信息，称之为服务配置中心。服务启动时自动将名称、地址注册到服务配置中心，服务消费者通过配置中心来获取机器列表，通过相应的负载均衡算法，选取其中一台服务器进行调用。当服务器宕机或下线时，需要能够动态地从服务配置中心里面移除，并通知相应的服务消费者。(这就是去中心化)。Zookeeper是一个很好的实现。负载均衡算法：轮询(Round Robin)法、随机(Random) 法、源地址哈希法、加权轮询法、加权随机法、最小连接法等.最好可以动态配置负载均衡规则，以满足千变万化的需求。Zookeeper：集群搭建略。客户端推荐zkClient,可以解决ZooKeeper API的一些繁琐问题，如一次性watcher问题，session重建问题等，它将watcher机制转换为一种更加容易理解的订阅模式，并且这种关系可以保持，而非一次性的。 Http服务网关基于网关(gateway)的安全架构:来自外部的请求先经过网关进行权限和安全校验，校验通过后再根据传入的服务名称，去寻找调用服务，最后通过网关返回结果。此时，服务提供者是不直接对外开放的。那么此时需要在网关前面加上一层负载均衡集群，而网关本身也采用集群模式。]]></content>
      <categories>
        <category>读书笔记</category>
        <category>大型网站分布式架构与设计实践</category>
      </categories>
      <tags>
        <tag>MicroService</tag>
        <tag>msa</tag>
        <tag>架构</tag>
        <tag>soa</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用hexo搭建博客]]></title>
    <url>%2F2017%2F12%2F16%2F%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[好处:免费、强大、可迁移、markdown+git组合 创建仓库新建一个名为你的用户名.github.io的仓库，比如说，如果你的github用户名是test，那么你就新建test.github.io的仓库（必须是你的用户名，其它名称无效），将来你的网站访问地址就是 http://test.github.io 了仓库创建成功不会立即生效，需要过一段时间，大概10-30分钟，或者更久。 绑定域名将CNAME指向你的用户名.github.io然后到你的github项目根目录新建一个名为CNAME的文件（无后缀），里面填写你的域名在你绑定了新域名之后，原来的你的用户名.github.io会自动跳转到你的新域名。 配置SSH keyssh-keygen -t rsa -C &quot;邮件地址&quot;找到.ssh\id_rsa.pub文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key。测试是否成功$ ssh -T git@github.com # 注意邮箱地址不用改 此时你还需要配置：12$ git config --global user.name &quot;xbynet&quot;// 你的github用户名，非昵称$ git config --global user.email &quot;xxx@xxx.com&quot;// 填写你的github注册邮箱 使用hexo写博客Hexo是一个简单、快速、强大的基于 Github Pages 的博客发布工具，支持Markdown格式，有众多优秀插件和主题。官网： http://hexo.iogithub: https://github.com/hexojs/hexo 由于github pages存放的都是静态文件，博客存放的不只是文章内容，还有文章列表、分类、标签、翻页等动态内容，假如每次写完一篇文章都要手动更新博文目录和相关链接信息，相信谁都会疯掉，所以hexo所做的就是将这些md文件都放在本地，每次写完文章后调用写好的命令来批量完成相关页面的生成，然后再将有改动的页面提交到github。 安装之前先来说几个注意事项：很多命令既可以用Windows的cmd来完成，也可以使用git bash来完成，但是部分命令会有一些问题，为避免不必要的问题，建议全部使用git bash来执行；hexo不同版本差别比较大，网上很多文章的配置信息都是基于2.x的，所以注意不要被误导；hexo有2种_config.yml文件，一个是根目录下的全局的_config.yml，一个是各个theme下的；12345$ npm install -g hexo$ hexo init$ hexo g # 生成$ hexo s # 启动服务 执行以上命令之后，hexo就会在public文件夹生成相关html文件，这些文件将来都是要提交到github去的：hexo s是开启本地预览服务，打开浏览器访问 http://localhost:4000 即可看到内容，很多人会碰到浏览器一直在转圈但是就是加载不出来的问题，一般情况下是因为端口占用的缘故 修改主题采用hexo-theme-next修改_config.yml中的theme: landscape改为theme: next. 保留CNAME、README.md等文件提交之后网页上一看，发现以前其它代码都没了，此时不要慌，一些非md文件可以把他们放到source文件夹下，这里的所有文件都会原样复制（除了md文件）到public目录的 常用hexo命令12345678910111213141516hexo new &quot;postName&quot; #新建文章hexo new page &quot;pageName&quot; #新建页面hexo generate #生成静态页面至public目录hexo server #开启预览访问端口（默认端口4000，&apos;ctrl + c&apos;关闭server）hexo deploy #部署到GitHubhexo help # 查看帮助hexo version #查看Hexo的版本缩写：hexo n == hexo newhexo g == hexo generatehexo s == hexo serverhexo d == hexo deploy组合命令：hexo s -g #生成并本地预览hexo d -g #生成并上传 _config.yml:这里面都是一些全局配置，每个参数的意思都比较简单明了，所以就不作详细介绍了。需要特别注意的地方是，冒号后面必须有一个空格，否则可能会出问题。 上传到github首先，ssh key肯定要配置好。其次，配置_config.yml中有关deploy的部分：1234567891011正确写法：deploy: type: git repository: git@github.com:liuxianan/liuxianan.github.io.git branch: master 错误写法：deploy: type: github repository: https://github.com/liuxianan/liuxianan.github.io.git branch: master 12npm install hexo-deployer-git --savehexo d -g #生成并上传 遇到的问题：https://stackoverflow.com/questions/17846529/could-not-open-a-connection-to-your-authentication-agenthttps://stackoverflow.com/questions/22575662/filename-too-long-in-git-for-windows如果遇到git not found之类的错误，请使用git bash 而非cmd如果遇到Permission denyed:1、请检查密钥2、可能是没有加载私钥到ssh-agent,到.ssh目录下执行:12eval $(ssh-agent)ssh-add id_rsa 3、git for windows下的Filename too long1git config --global core.longpaths true 写博客定位到我们的hexo根目录，执行命令：hexo new &#39;my-first-blog&#39;hexo会帮我们在source/_posts下生成相关md文件,我们只需要打开这个文件就可以开始写博客了当然你也可以直接自己新建md文件，用这个命令的好处是帮我们自动生成了时间。一般完整格式如下：123456789---title: postName #文章页面上的显示名称，一般是中文date: 2013-12-02 15:30:16 #文章生成时间，一般不改，当然也可以任意修改categories: 默认分类 #分类tags: [tag1,tag2,tag3] #文章标签，可空，多标签请用格式，注意:后面有个空格description: 附加一段文章摘要，字数最好在140字以内，会出现在meta的description里面---以下是正文 那么hexo new page ‘postName’命令和hexo new ‘postName’有什么区别呢？1hexo new page &quot;my-second-blog&quot; 最终部署时生成：hexo\public\my-second-blog\index.html，但是它不会作为文章出现在博文目录。 如何让博文列表不显示全部内容默认情况下，生成的博文目录会显示全部的文章内容，如何设置文章摘要的长度呢？答案是在合适的位置加上即可. 12345678910111213# 前言使用github pages服务搭建博客的好处有：1. 全是静态文件，访问速度快；2. 免费方便，不用花一分钱就可以搭建一个自由的个人博客，不需要服务器不需要后台；3. 可以随意绑定自己的域名，不仔细看的话根本看不出来你的网站是基于github的；&lt;!--more--&gt;4. 数据绝对安全，基于github的版本管理，想恢复到哪个历史版本都行；5. 博客内容可以轻松打包、转移、发布到其它平台；6. 等等； 如何支持目录：为文章添加 toc: true Toc虽然生成了，审查元素看锚也有，但是无法点击:(差了很久，也搜了issues，但是没人遇到和我类似的问题。。。)修改next/layout/_scripts/vendors.swig，添加 12345678910&lt;script&gt; $(function()&#123; $(".nav-item .nav-link").each(function(i)&#123; $(this).text($(this).attr("href").replace('#','')); $(this).parent().contents().filter(function() &#123; return this.nodeType == 3; //Node.TEXT_NODE &#125;).remove(); &#125;); &#125;);&lt;/script&gt; 404页面:在source下新建一个404.html文件即可,建议采用腾讯公益404，如下： 1234567891011121314&lt;!DOCTYPE HTML&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="content-type" content="text/html;charset=utf-8;"/&gt; &lt;meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" /&gt; &lt;meta name="robots" content="all" /&gt; &lt;meta name="robots" content="index,follow"/&gt;&lt;/head&gt;&lt;body&gt; &lt;script type="text/javascript" src="//qzonestyle.gtimg.cn/qzone/hybrid/app/404/search_children.js" charset="utf-8" homePageUrl="/" homePageName="返回主页"&gt;&lt;/script&gt; &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 配置markdown解析器，这个官方文档没有说明，但是由于hexo采用hexo-renderer-marked,故而可以参考hexo-renderer-marked文档:12345678910marked: gfm: true pedantic: false sanitize: false tables: true breaks: true smartLists: true smartypants: true modifyAnchors: &apos;&apos; autolink: true 安装插件：如报ERROR Deployer not found: git,请执行：npm install --save hexo-deployer-git1、sitemap、feed插件$ npm install hexo-generator-sitemap hexo-generator-feed hexo-generator-baidu-sitemap --save启用，修改Hexo_config.yml，增加以下内容 123456789sitemap: path: sitemap.xmlbaidusitemap: path: baidusitemap.xmfeed: type: atom path: atom.xml limit: 100 添加“Fork me on Github” ribbon给blog主页添加一个“Fork me on Github”的绶带（ribbon）那么将下面的代码（注意将you改为你自己的github上的注册名）,添加到next/layout/_layout.swig的body结束标签之前 (注意：这是next主题的地址，其他主题可能不一致):1&lt;a href=&quot;https://github.com/xbynet&quot;&gt;&lt;img style=&quot;position: absolute; top: 0; left: 0; border: 0;&quot; src=&quot;https://camo.githubusercontent.com/567c3a48d796e2fc06ea80409cc9dd82bf714434/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f6c6566745f6461726b626c75655f3132313632312e706e67&quot; alt=&quot;Fork me on GitHub&quot; data-canonical-src=&quot;https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png&quot;&gt;&lt;/a&gt; 插入音乐:两种方式，在线和离线。在线的可以采用网易云音乐的外链播放，不过是一个iframe。离线/在线直播源播放可以采用hexo-tag-aplayer.安装后，修改next/layout/_custom/sidebar.swig:123456789101112131415161718192021222324&lt;div id=&quot;aplayer1&quot; class=&quot;aplayer&quot;&gt;&lt;/div&gt;&lt;script src=&quot;/asserts/js/APlayer.min.js&quot;&gt;&lt;/script&gt;&lt;script&gt; var ap = new APlayer(&#123; element: document.getElementById(&apos;aplayer1&apos;), narrow: false, autoplay: false, showlrc: false, mutex: true, theme: &apos;#e6d0b2&apos;, preload: &apos;metadata&apos;, mode: &apos;circulation&apos;, music: &#123; title: &apos;素雨&apos;, author: &apos;孙雪宁&apos;, url: &apos;http://ws.stream.qqmusic.qq.com/C1000040LV2h3FzIVl.m4a?fromtag=38&apos;, pic: &apos;https://y.gtimg.cn/music/photo_new/T002R300x300M000003mxnKZ0WbTPc.jpg?max_age=2592000&apos; &#125; &#125;);&lt;/script&gt;&lt;!-- &lt;iframe frameborder=&quot;no&quot; border=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; width=330 height=86 src=&quot;//music.163.com/outchain/player?type=2&amp;id=28815250&amp;auto=0&amp;height=66&quot;&gt;&lt;/iframe&gt;--&gt; 如何寻找在线音乐源地址:http://music.liuzhijin.cn/ 参考：https://www.cnblogs.com/liuxianan/p/build-blog-website-by-hexo-github.htmlhttps://segmentfault.com/a/1190000006831597https://www.cnblogs.com/zhcncn/p/4097881.htmlhttps://github.com/litten/BlogBackuphttp://www.jianshu.com/p/f054333ac9e6]]></content>
      <categories>
        <category>杂项</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
</search>
